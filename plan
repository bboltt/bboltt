Below is a **final, formal writeup** of the **Model Implementation** chapter in the **new format** you provided. It incorporates the same implementation details used by the old model (since the new model’s data pipeline and processes remain unchanged), restructured according to the updated headings and requirements.

---

# 8. Model Implementation

This chapter explains how the updated Inclearing Fraud Model (Version 3) is deployed and integrated within the organization’s production environment, including data sources, transformations, and the reporting mechanism for fraud risk scores.

## 8.1. Summary of Model Implementation

The Inclearing Fraud Model V3 is implemented as part of the **Financial Crimes Non‐Card Transaction** fraud detection pipeline. It uses **PySpark** on **[CDSW / Spark cluster / internal big data platform]** to generate daily fraud scores for each inclearing transaction. These scores feed into the **Fraud Operations** rules engine, enabling real‐time or near‐real‐time alerts for potential fraudulent checks.

- **Application / System**:  
  - The model’s output is ingested into **Detica** (the downstream case management system), where Fraud Operations reviews high‐risk items.  
- **Code Platform and Prediction Flow**:  
  - The scoring code is stored in **[Bitbucket / GitHub / internal repository]**.  
  - During each production cycle, the pipeline reads inclearing transactions from the **Data Lake** (same as previous model version), applies the GBT model to compute a fraud likelihood score, and writes back the scored data to a **production table** for consumption by downstream applications.

Because the model architecture (a Gradient Boosted Classifier) and data ingestion pipeline are the same as in the previous version, minimal adjustments were needed beyond retraining on updated historical data.

## 8.2. Production Data Sources, Preparation, and Transformations

### 8.2.1. Data Pipeline

1. **Implementation/Prediction Pipeline**  
   - The system extracts daily inclearing transactions from **`dm_fraud_detection.inc_frd_<...>`** (or equivalent) within the Data Lake.  
   - A PySpark job transforms the raw data, applies feature engineering steps (similar to those used in model development), and feeds the resulting features into the GBT model object.  
   - The model scores each transaction, outputting the predicted probability of fraud (i.e., fraud score).

2. **Reference to Applicable Code**  
   - The Spark job is orchestrated via **[Airflow / internal scheduler]**.  
   - Code is tracked in **Bitbucket** under the repository **`<repo_name>`**, ensuring version control for any changes to data transformations.

### 8.2.2. Data Sources for Model Implementation

- **Inclearing Transaction Table** (`SB_FRAUD_NCA.incI_mth` or similar):  
  - Primary source of deposit items for scoring.  
  - Includes check amount, account details, transaction history, and other features.  
- **Supporting Tables** (internal to Regions):  
  - **SL1_DM, SL1_RF, SL1_DNCCR,** etc., used to enrich the transaction records with relationship, login, or case management attributes.  
- **Vendor Data (if applicable)**:  
  - As with the old model, no external vendor data is directly consumed at scoring time. All enhancements remain internal.

### 8.2.3. Weaknesses and Limitations in Source Data

- **Missing Fields or Records**:  
  - Rarely, certain checks may lack critical fields (e.g., no item serial number). As in the old model, these items receive partial scoring or default values.  
- **Infrequent Data Delays**:  
  - Overnight ingestion delays can occasionally cause a small subset of transactions to miss the day’s scoring window. They are captured and scored retroactively once the data is complete.

### 8.2.4. Data Appropriateness and Accuracy

The production data follows the same schema and cleaning rules as the development dataset, ensuring consistency. Periodic data audits confirm alignment between the development sample and live production feeds. If any outlier patterns emerge (e.g., new transaction codes), updates are made to the transformation logic to maintain model accuracy.

### 8.2.5. Handling Missing or Unknown Values

Any missing numeric fields continue to be imputed with **[–999 or 0, as appropriate]**, consistent with the approach taken during model training. Categoricals that are unrecognized or empty default to “unknown” categories. This approach has been retained from the old model to ensure consistent scoring behavior.

## 8.3. Production Environment

The production environment is hosted on **[CDSW / Hadoop Yarn / Spark cluster name]**, where the PySpark scoring pipeline executes on a daily schedule. The environment automatically pulls together the relevant data tables, runs feature transformations, and applies the GBT model. Key aspects:

1. **Data Integration & Model Execution**  
   - The daily job merges inclearing checks with relevant reference tables.  
   - The GBT scoring step runs after successful data ingestion, generating a **fraud score** per item.

2. **Rules and Overrides**  
   - The final fraud detection process relies on a combination of model output plus any business rules. For instance, certain high‐risk item types might be flagged regardless of model score.  
   - These overrides are the same ones used with the old model; they have been tested with the new GBT scores to confirm compatibility.

3. **Control / Production Checks**  
   - The pipeline logs each step’s status (extraction, transformation, scoring) to a monitoring dashboard.  
   - Reconciliation checks ensure that the number of rows scored matches the expected count of daily transactions.  
   - If errors occur, the pipeline triggers alerts to the Data & Analytics team to investigate and re‐run if necessary.

## 8.4. Model Output and Reporting

### 8.4.1. How Output Is Used and Weighed

The model outputs a numeric probability (fraud likelihood) for every check. Fraud Operations uses these scores to prioritize which items undergo a deeper investigation:

- **Score Thresholds**: If the predicted fraud likelihood exceeds certain cutoffs (e.g., top 5%), an alert is created in **Detica**.  
- **Further Processing**: No additional models consume the GBT output. The risk score is integrated directly into the existing rule engine for advanced decision logic.

### 8.4.2. Transmission and Automation Controls

Once daily scoring is complete, the resulting dataset (with final scores) is written to **[HDFS / a production table / object storage]**. Fraud Operations systems fetch these scores in near‐real time, applying them to:

- **Alerts**: An item with a high fraud score triggers an immediate alert for manual review.  
- **Reporting**: Summaries of daily alerts and captured fraud amounts are compiled in standard management reports.

### 8.4.3. Datasets Storing Model Results

- **Production Scored Table**: **`dm_fraud_detection.inc_frd_scored_v3`** (example naming) stores each check’s final features, the GBT score, and relevant metadata (date/time of scoring, system run ID, etc.).  
- **Historical Audit Dataset**: Archives older daily outputs, ensuring traceability for compliance and monitoring.

### 8.4.4. Disclaimers, Information, and Assumptions

- **Model Uncertainty**: The GBT score is a statistical estimate, not a definitive fraud/no‐fraud determination.  
- **Use of Imputed Values**: Missing data is imputed following the same scheme as training. This is highlighted in the model documentation to warn users that extremely missing data could reduce confidence in the score.  
- **Interpretation**: The final decision (to return or pay a check) remains the responsibility of Fraud Operations, guided by the model’s ranking but supplemented by domain expertise.

### 8.4.5. Controls and Reviews

- **Operational Reviews**: The business regularly reviews performance metrics (e.g., daily fraud capture, false positives) to ensure the model continues meeting goals.  
- **Committee Acceptance**: Any major changes to thresholds or the scoring pipeline are presented to the Model Risk Management committee for approval, maintaining governance standards.

---

**In summary**, the **Model Implementation** for Version 3 follows the same robust pipeline and operational framework as its predecessor. It leverages existing data sources, transformations, and rule sets—requiring only minimal adjustments to accommodate the newly trained GBT model. This consistent design ensures minimal disruption while introducing improved fraud detection capability.
