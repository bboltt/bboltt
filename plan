**Possible 2-Minute Spoken Script**

> “Here’s a quick overview of our latest progress:
> 
> First, I added a **feature importance** step to automatically select the top predictive features. This uses a **Spark ML RandomForest** that ranks all candidate features for distinguishing PWM from Consumer. We then save the top features in a data-lake table, so each new run of the pipeline only relies on the best columns.
> 
> Next, we updated the **prospecting workflow** to cluster clients on these selected features—no more manual feature list. With each run, it’s dynamically updated based on whichever columns are currently most relevant.
> 
> Finally, I introduced an **insight generation** post-processing stage. For each cluster, we train a small “cluster vs. rest” model to find the key features that matter most for that cluster, compute mean and standard-deviation stats for the existing PWM in it, and then generate a textual insight for each prospect comparing their feature values to the cluster’s norm. This ends up in one final table showing exactly why a prospect was placed in a particular cluster and how they differ. 
> 
> All told, we now have an **end-to-end** automated flow—from feature selection to clustering and detailed insights—that should help both data teams and business stakeholders quickly see what drives each prospect’s assignment.”
