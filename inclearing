from pyspark.sql import functions as F

# Define your bucket splits and labels
bucket_splits = [0, 100, 1000, 10000, 100000, 1000000, float('inf')]
bucket_labels = ["0-100", "100-1000", "1000-10000", "10000-100000", "100000-1000000", "1000000+"]

# Bucketizer from PySpark ML to create buckets
from pyspark.ml.feature import Bucketizer

bucketizer = Bucketizer(
    splits=bucket_splits,
    inputCol="amount",
    outputCol="amount_bucket_idx"
)

# Apply bucketizer
train_bucketed = bucketizer.setHandleInvalid("keep").transform(train)

# Map bucket indices to labels
bucket_label_expr = F.create_map([F.lit(i) for pair in enumerate(bucket_labels) for i in pair])

train_bucketed = train_bucketed.withColumn(
    "amount_bucket",
    bucket_label_expr[F.col("amount_bucket_idx").cast("integer")]
)

# Extract month from ods_business_dt
train_bucketed = train_bucketed.withColumn(
    "month",
    F.date_format("ods_business_dt", "yyyy-MM")
)

# Aggregate to calculate fraud rate
fraud_rate_df = train_bucketed.groupBy("month", "amount_bucket").agg(
    (F.sum("label") / F.count("*")).alias("fraud_rate")
)

# Pivot the table to get desired format
final_df = fraud_rate_df.groupBy("month").pivot("amount_bucket", bucket_labels).agg(F.first("fraud_rate"))

# Sort by month
final_df = final_df.orderBy("month")

# Display final result
final_df.show(truncate=False)

