#!/usr/bin/env python
# run_feature_importance.py

import sys
import pyspark.sql.functions as F
from pyspark.sql import SparkSession
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np

from optimal_spark_config.create_spark_instance import generate_spark_instance
from HiveIO.config import get_config, get_schema

def main(config_name="config.yaml", top_n=20):
    """
    1) Loads config & schema.
    2) Reads the 'features' table (df_features).
    3) Trains a tree-based model to classify Consumer vs. PWM.
    4) Prints top feature importances.
    5) Optionally updates config or returns them for usage in clustering.
    """

    # 1) Create Spark session
    spark = generate_spark_instance(total_memory=600, total_vcpu=300)

    # 2) Load config & schema
    #    If you want to pass a different config file, do: python run_feature_importance.py my_config.yaml
    cfg = get_config(None, config_name)  # 'None' or some module reference for the first argument
    schema = get_schema(None, "schema.yml")  # or use config if you have a separate schema name

    # 3) Identify the table & features from config
    features_tabl = cfg["tables"]["prospecting_features"]
    db_name = schema.databases[0]["name"]
    feature_names = list(cfg["features"]["feature_names"])

    # 4) Decide which columns to load
    #    We'll load the label column "consumer_pwm" plus the feature columns.
    #    If there's a "business_date", filter or choose the latest, etc.
    business_date = cfg["dates"]["business_date"]

    # 5) Read the table
    df_features = spark.sql(f"""
        SELECT *
        FROM {db_name}.{features_tabl}
        WHERE business_date = '{business_date}'
    """)

    # 6) We'll classify PWM (label=1) vs Consumer (label=0).
    #    Filter out any rows that aren't "pwm" or "consumer" if necessary.
    df_features = df_features.filter(F.col("consumer_pwm").isin(["pwm", "consumer"]))

    # 7) Collect to Pandas. Only load the columns we need:
    #    label column: consumer_pwm
    #    feature columns: feature_names
    selected_cols = ["consumer_pwm"] + feature_names
    pdf = df_features.select(*selected_cols).toPandas()

    # 8) Convert label to numeric: 1 = pwm, 0 = consumer
    pdf["label"] = np.where(pdf["consumer_pwm"] == "pwm", 1, 0)

    # 9) Fill NaN with 0 or a suitable default
    pdf = pdf.fillna(0)

    # 10) Prepare X, y for training
    X = pdf[feature_names].values
    y = pdf["label"].values

    # 11) Split for a quick train/test (optional)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # 12) Train a RandomForest (tree-based) to get feature importances
    rf = RandomForestClassifier(
        n_estimators=100,
        max_depth=8,
        random_state=42
    )
    rf.fit(X_train, y_train)

    # 13) Compute feature importances
    importances = rf.feature_importances_
    feature_importance_pairs = list(zip(feature_names, importances))
    # Sort by importance desc
    feature_importance_pairs.sort(key=lambda x: x[1], reverse=True)

    # 14) Print top-N features
    print(f"\nTop {top_n} Most Important Features for PWM vs Consumer:\n{'-'*60}")
    for i, (fname, imp) in enumerate(feature_importance_pairs[:top_n], start=1):
        print(f"{i}. {fname} => {imp:.4f}")

    # 15) If you want to store them back into config or a file, do so here
    #     For example, let's store top_n features in the config under "features" -> "selected"
    selected_top_features = [f[0] for f in feature_importance_pairs[:top_n]]
    print(f"\nSelected top {top_n} features:\n{selected_top_features}\n")

    # You could optionally write them to config. Example pseudo-code:
    # cfg["features"]["selected"] = selected_top_features
    # save_config(cfg, "new_config.yaml")  # you'd need a function to write config back

    # 16) Stop Spark (optional if you'd like)
    spark.stop()


if __name__ == "__main__":
    # You can parse arguments from sys.argv if you want:
    # e.g. python run_feature_importance.py config.yaml 15
    # or just rely on defaults
    import sys

    if len(sys.argv) >= 2:
        cfg_file = sys.argv[1]
    else:
        cfg_file = "config.yaml"

    if len(sys.argv) >= 3:
        top_n = int(sys.argv[2])
    else:
        top_n = 20

    main(cfg_file, top_n)







# model_pipeline.py

import pandas as pd
import numpy as np

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from pyspark.sql import Row, DataFrame, SparkSession


def perform_clustering(spark: SparkSession, df: DataFrame, features: list, k: int):
    """
    1) Convert PWM Spark DF -> Pandas
    2) Scale features
    3) K-Means (scikit-learn)
    4) Assign cluster_id to each row
    5) Compute each PWM's distance to cluster center
    6) Store the max distance per cluster

    Returns:
        df_with_clusters: Spark DF with original columns + 'cluster_id' + 'distance_to_center'
        centers_df     : Spark DF of cluster centers => each row has [cluster_id, features (list)]
        scaler         : Fitted StandardScaler
        features_used  : The list of features we scaled
        max_dist_df    : Spark DF with columns [cluster_id, max_distance]
    """
    # Columns needed in the Pandas PDF
    columns_needed = [
        "hh_id_in_wh",
        "bal_amt_sum",
        "transaction_sum_1_month",
        "recent_opened_counts_30d",
    ] + features

    pdf = df.select(*columns_needed).toPandas().fillna(0.0)

    # 1) Scale
    scaler = StandardScaler()
    scaled = scaler.fit_transform(pdf[features].values)

    # 2) K-Means
    kmeans = KMeans(n_clusters=k, random_state=42)
    cluster_ids = kmeans.fit_predict(scaled)
    pdf["cluster_id"] = cluster_ids

    # 3) Compute distance to cluster center
    cluster_centers = kmeans.cluster_centers_  # shape: (k, len(features))
    distances = []
    for i, c_id in enumerate(cluster_ids):
        dist = np.linalg.norm(scaled[i] - cluster_centers[c_id])
        distances.append(dist)
    pdf["distance_to_center"] = distances

    # 4) Build Spark DF from labeled Pandas DF
    df_with_clusters = spark.createDataFrame(pdf)

    # 5) Also build a Spark DF for cluster centers
    center_rows = []
    for i, center in enumerate(cluster_centers):
        center_rows.append(
            Row(cluster_id=int(i), features=center.tolist())
        )
    centers_df = spark.createDataFrame(center_rows)

    # 6) Build a Spark DF for max distance per cluster
    #    (the cluster's "radius")
    #    We can do this in Pandas or Spark. Here, let's just do it in Pandas quickly:
    gpdf = pdf.groupby("cluster_id")["distance_to_center"].max().reset_index()
    max_dist_rows = []
    for row in gpdf.itertuples(index=False):
        max_dist_rows.append(
            Row(cluster_id=int(row.cluster_id), max_distance=float(row.distance_to_center))
        )
    max_dist_df = spark.createDataFrame(max_dist_rows)

    return df_with_clusters, centers_df, scaler, features, max_dist_df


def calculate_similarity_and_filter(
    spark,
    df,           # large consumer Spark DataFrame
    centers_df,   # Spark DF with columns: [cluster_id, features (array<double>)]
    scaler,       # scikit-learn StandardScaler from perform_clustering
    features,     # list of feature columns used in clustering
    max_dist_df   # Spark DF with columns: [cluster_id, max_distance]
):
    """
    Assigns each consumer to its best cluster based on cosine similarity,
    computes the distance to that cluster center, and filters out consumers
    whose distance exceeds the cluster's farthest PWM distance.

    Returns a Spark DataFrame with these columns (at minimum):
      - hh_id_in_wh
      - cluster_id
      - max_similarity
      - distance_to_center
      - plus any columns that were in `df` except for replaced ones
      - columns from max_dist_df if needed

    The output is already filtered so that distance_to_center <= max_distance.
    """

    import pyspark.sql.functions as F
    import math
    from pyspark.sql.types import DoubleType
    from pyspark.sql.window import Window

    # 1) Extract scikit-learn scaling info
    #    We'll replicate mean_/scale_ in Spark for each feature
    mean_values = list(scaler.mean_)
    scale_values = list(scaler.scale_)

    # 2) Build columns to scale each feature in Spark
    #    For each feature f_i, we create a column: (f_i - mean_i)/scale_i
    scaled_exprs = []
    for i, fcol in enumerate(features):
        scaled_exprs.append(
            ((F.col(fcol) - mean_values[i]) / scale_values[i]).alias(f"{fcol}_scaled")
        )

    # Keep other columns from df (e.g. "hh_id_in_wh") so we can join later
    # but do *not* drop them yet. We'll re-select everything we need.
    all_cols_except_features = [c for c in df.columns if c not in features]

    # 3) Create a new DataFrame with scaled feature columns + original non-feature columns
    df_scaled = df.select(
        *all_cols_except_features,
        *scaled_exprs
    )

    # 4) Combine the scaled columns into a single array column, e.g. "scaledFeatures"
    df_scaled = df_scaled.withColumn(
        "scaledFeatures",
        F.array([F.col(f"{fcol}_scaled") for fcol in features])
    )

    # 5) Define UDFs for cosine similarity and Euclidean distance
    @F.udf(DoubleType())
    def cos_sim(v1, v2):
        # v1, v2 are Python lists or tuples of floats
        dot = 0.0
        norm1 = 0.0
        norm2 = 0.0
        for x, y in zip(v1, v2):
            dot += x * y
            norm1 += x * x
            norm2 += y * y
        if norm1 == 0.0 or norm2 == 0.0:
            return 0.0
        return float(dot / math.sqrt(norm1 * norm2))

    @F.udf(DoubleType())
    def euc_dist(v1, v2):
        s = 0.0
        for x, y in zip(v1, v2):
            diff = x - y
            s += diff * diff
        return float(math.sqrt(s))

    # 6) Cross join each consumer row with each cluster center => compute similarity, distance
    #    Make sure "features" in centers_df is an array<double> (scaled center).
    df_join = df_scaled.crossJoin(centers_df)

    df_join = df_join \
        .withColumn("cos_similarity", cos_sim("scaledFeatures", "features")) \
        .withColumn("distance_to_center", euc_dist("scaledFeatures", "features"))

    # 7) For each consumer (identified by something like hh_id_in_wh),
    #    pick the single cluster with the maximum similarity
    w = Window.partitionBy("hh_id_in_wh").orderBy(F.col("cos_similarity").desc())
    df_best = df_join \
        .withColumn("rn", F.row_number().over(w)) \
        .filter(F.col("rn") == 1) \
        .drop("rn") \
        .withColumnRenamed("cos_similarity", "max_similarity")

    # 8) Join with the cluster's max_distance and filter out consumers beyond that radius
    df_filtered = df_best.join(max_dist_df, on="cluster_id", how="left") \
                         .filter(F.col("distance_to_center") <= F.col("max_distance"))

    return df_filtered





# prospecting.py

from pyspark.sql import DataFrame, SparkSession
import pyspark.sql.functions as F
from pyspark.sql.functions import col, avg

# import your scikit-learn K-Means pipeline
from private_wealth_retention.consumer_prospecting.model.model_pipeline import (
    perform_clustering,
    calculate_similarity_and_filter,
)

def get_prospects(spark, pwm_df, consumer_df, k, n, feature_cols):
    """
    1) Cluster the PWM data
    2) Compute farthest distance for each cluster
    3) For the consumer data, compute cluster assignment & distance
    4) Filter out consumers that are beyond that cluster's farthest PWM distance
    5) Return top N by similarity
    """

    # 1) Perform K-Means on the PWM
    #    df_with_clusters => Spark DF with cluster_id, distance_to_center, etc.
    #    max_dist_df => Spark DF with columns [cluster_id, max_distance]
    df_with_clusters, centers_df, scaler, used_feats, max_dist_df = perform_clustering(
        spark, pwm_df, feature_cols, k
    )

    # 2) Calculate similarity for consumers & filter by distance
    consumer_with_cluster = calculate_similarity_and_filter(
        spark, consumer_df, centers_df, scaler, used_feats, max_dist_df
    )
    # consumer_with_cluster is already filtered to only those with distance <= cluster's max_distance

    # 3) Optional: gather cluster-level stats from PWM
    cluster_stats = (
        df_with_clusters
        .groupBy("cluster_id")
        .agg(
            avg("bal_amt_sum").alias("cluster_avg_curr_bal_amt"),
            avg("transaction_sum_1_month").alias("cluster_avg_transaction_1_month"),
            avg("recent_opened_counts_30d").alias("cluster_avg_opened_ar_30d"),
        )
    )

    # 4) Attach consumer columns
    consumer_view = consumer_df.select(
        col("hh_id_in_wh"),
        col("bal_amt_sum").alias("consumer_curr_bal_amt"),
        col("transaction_sum_1_month").alias("consumer_transaction_1_month"),
        col("recent_opened_counts_30d").alias("consumer_opened_ar_30d"),
    )

    final_df = (
        consumer_with_cluster
        .join(consumer_view, "hh_id_in_wh", "left")
        .join(cluster_stats, "cluster_id", "left")
        .select(
            "hh_id_in_wh",
            F.col("max_similarity").alias("similarity_score"),
            "distance_to_center",  # if you want to see how far the consumer is
            "cluster_avg_curr_bal_amt",
            "cluster_avg_transaction_1_month",
            "cluster_avg_opened_ar_30d",
            F.col("consumer_curr_bal_amt").alias("curr_bal_amt"),
            F.col("consumer_transaction_1_month").alias("transaction_1_month"),
            F.col("consumer_opened_ar_30d").alias("opened_ar_30d"),
        )
    )

    # 5) Sort descending by similarity, limit to N
    final_df = final_df.orderBy(F.col("similarity_score").desc()).limit(n)

    return final_df




# run_prospect.py

if __name__ == "__main__":
    spark = ...
    # load df_features, filter pwm, consumer, etc.
    # pick feature_cols, k, n

    # Affluent
    affluent_prospects = get_prospects(
        spark, affluent_pwm, consumer, k, n, features
    ).withColumn("prospect_segmet", F.lit("Affluent"))

    # High Net Worth
    high_net_worth_prospects = get_prospects(
        spark, high_net_worth_pwm, consumer, k, n, features
    ).withColumn("prospect_segmet", F.lit("High Net Worth"))

    # Union and write
    prospects = affluent_prospects.union(high_net_worth_prospects)
    ...


