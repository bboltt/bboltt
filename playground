from pyspark.sql.functions import col, countDistinct, when, to_date, datediff

def count_new_users(spark, cfg, prev_dt, latest_dt):
    # Get master data
    df = get_master_data(spark, cfg)

    # Filter for the date range [prev_dt, latest_dt]
    df = df.filter(
        (col("business_date") >= prev_dt) & 
        (col("business_date") <= latest_dt)
    )

    # Calculate days_diff
    df = df.withColumn("days_diff", datediff(to_date(col("business_date")), to_date(col("open_date"))))

    # Add intermediate columns for each interval
    df = df.withColumn("ip_in_1m", when((col("days_diff") <= 30) & (col("days_diff") >= 0), col("ip_id")))
    df = df.withColumn("ip_in_2m", when((col("days_diff") <= 60) & (col("days_diff") >= 0), col("ip_id")))
    df = df.withColumn("ip_in_6m", when((col("days_diff") <= 180) & (col("days_diff") >= 0), col("ip_id")))
    df = df.withColumn("ip_in_12m", when((col("days_diff") <= 365) & (col("days_diff") >= 0), col("ip_id")))

    # Perform groupBy and aggregations for all dates at once
    result_df = (
        df.groupBy("segmt_prod_type", "business_date")
        .agg(
            countDistinct("ip_in_1m").alias("users_past_1_month"),
            countDistinct("ip_in_2m").alias("users_past_2_months"),
            countDistinct("ip_in_6m").alias("users_past_6_months"),
            countDistinct("ip_in_12m").alias("users_past_12_months")
        )
    )

    # Return the final result
    return result_df



from pyspark.sql.functions import col, mean, max, min, when, to_date, datediff

def product_level_balance(spark, cfg, prev_dt, latest_dt):
    # Get master data
    df = get_master_data(spark, cfg)

    # Filter for the date range [prev_dt, latest_dt]
    df = df.filter(
        (col("business_date") >= prev_dt) &
        (col("business_date") <= latest_dt)
    )

    # Calculate days_diff
    df = df.withColumn("days_diff", datediff(to_date(col("business_date")), to_date(col("open_date"))))

    # Perform aggregations for different time ranges
    result_df = (
        df.groupBy("segmt_prod_type", "business_date")
        .agg(
            # Mean, Max, Min for 1 month
            mean(when((col("days_diff") <= 30) & (col("days_diff") >= 0), col("curr_bal_amt"))).alias("mean_balance_past_1_month"),
            max(when((col("days_diff") <= 30) & (col("days_diff") >= 0), col("curr_bal_amt"))).alias("max_balance_past_1_month"),
            min(when((col("days_diff") <= 30) & (col("days_diff") >= 0), col("curr_bal_amt"))).alias("min_balance_past_1_month"),

            # Mean, Max, Min for 2 months
            mean(when((col("days_diff") <= 60) & (col("days_diff") >= 0), col("curr_bal_amt"))).alias("mean_balance_past_2_months"),
            max(when((col("days_diff") <= 60) & (col("days_diff") >= 0), col("curr_bal_amt"))).alias("max_balance_past_2_months"),
            min(when((col("days_diff") <= 60) & (col("days_diff") >= 0), col("curr_bal_amt"))).alias("min_balance_past_2_months"),

            # Mean, Max, Min for 6 months
            mean(when((col("days_diff") <= 180) & (col("days_diff") >= 0), col("curr_bal_amt"))).alias("mean_balance_past_6_months"),
            max(when((col("days_diff") <= 180) & (col("days_diff") >= 0), col("curr_bal_amt"))).alias("max_balance_past_6_months"),
            min(when((col("days_diff") <= 180) & (col("days_diff") >= 0), col("curr_bal_amt"))).alias("min_balance_past_6_months"),

            # Mean, Max, Min for 12 months
            mean(when((col("days_diff") <= 365) & (col("days_diff") >= 0), col("curr_bal_amt"))).alias("mean_balance_past_12_months"),
            max(when((col("days_diff") <= 365) & (col("days_diff") >= 0), col("curr_bal_amt"))).alias("max_balance_past_12_months"),
            min(when((col("days_diff") <= 365) & (col("days_diff") >= 0), col("curr_bal_amt"))).alias("min_balance_past_12_months")
        )
    )

    # Return the final DataFrame
    return result_df




def count_users_per_product(spark, cfg, open_notopened_df, prev_dt, latest_dt):
    # Repartition DataFrames for efficient joins
    open_notopened_df = open_notopened_df.repartition("segmt_prod_type", "business_date").cache()
    user_count = count_new_users(spark, cfg, prev_dt, latest_dt).repartition("segmt_prod_type", "business_date").cache()
    product_level_balance_features = product_level_balance(spark, cfg, prev_dt, latest_dt).repartition("segmt_prod_type", "business_date").cache()

    # Join DataFrames in Spark directly
    final_df = (
        open_notopened_df
        .join(user_count, on=["segmt_prod_type", "business_date"], how="left")
        .join(product_level_balance_features, on=["segmt_prod_type", "business_date"], how="left")
    )

    # Convert float columns if needed
    float_cols = [
        "cumulative_user_count",
        "users_past_1_month",
        "users_past_2_months",
        "users_past_6_months",
        "users_past_12_months",
        "mean_balance_past_1_month",
        "max_balance_past_1_month",
        "min_balance_past_1_month",
        "mean_balance_past_2_months",
        "max_balance_past_2_months",
        "min_balance_past_2_months",
        "mean_balance_past_6_months",
        "max_balance_past_6_months",
        "min_balance_past_6_months",
        "mean_balance_past_12_months",
        "max_balance_past_12_months",
        "min_balance_past_12_months",
    ]

    for col_name in float_cols:
        if col_name in final_df.columns:
            final_df = final_df.withColumn(col_name, final_df[col_name].cast("float"))

    return final_df





