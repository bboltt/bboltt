from pyspark.sql.functions import col, countDistinct
from datetime import datetime, timedelta

def count_users_per_product_fn(spark, cfg, prev_dt, latest_dt):
    # Get master data
    df = get_master_data(spark, cfg)
    
    # Convert prev_dt and latest_dt to Python datetime objects
    start_date = datetime.strptime(prev_dt, "%Y-%m-%d")
    end_date = datetime.strptime(latest_dt, "%Y-%m-%d")
    
    # Initialize an empty DataFrame to store results
    final_df = None
    
    # Loop through each date in the range
    current_date = start_date
    while current_date <= end_date:
        current_date_str = current_date.strftime("%Y-%m-%d")  # Convert date to string
        
        # Filter the DataFrame for the current date
        filtered_df = df.filter(col("business_date") == current_date_str) \
                        .filter(col("close_date").isNull()) \
                        .groupBy("segmt_prod_type", "business_date") \
                        .agg(countDistinct("ip_id").alias("cumulative_user_count"))
        
        # If the filtered DataFrame is not empty, union it with the final DataFrame
        if final_df is None:
            final_df = filtered_df
        else:
            final_df = final_df.union(filtered_df)
        
        # Move to the next date
        current_date += timedelta(days=1)
    
    # Select the required columns
    if final_df is not None:
        final_df = final_df.select("segmt_prod_type", "business_date", "cumulative_user_count")
    
    return final_df





