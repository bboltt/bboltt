import optuna

# Objective function for Bayesian Optimization
def objective(trial):
    # Suggest hyperparameters
    max_depth = trial.suggest_int("maxDepth", 3, 10)
    min_instances_per_node = trial.suggest_int("minInstancesPerNode", 10, 200)
    step_size = trial.suggest_loguniform("stepSize", 0.01, 0.2)
    max_bins = trial.suggest_categorical("maxBins", [64, 128, 256])
    subsampling_rate = trial.suggest_float("subsamplingRate", 0.5, 1.0)

    # Define the model with suggested hyperparameters
    rf = GBTClassifier(
        labelCol="label",
        featuresCol="features",
        predictionCol="prediction",
        maxDepth=max_depth,
        minInstancesPerNode=min_instances_per_node,
        stepSize=step_size,
        maxBins=max_bins,
        subsamplingRate=subsampling_rate,
        seed=22,
    )

    # Train the model
    rf_model = rf.fit(balance)

    # Evaluate on test data
    rf_pred_test = rf_model.transform(test_1)
    evaluator = BinaryClassificationEvaluator(labelCol=target_column)
    auc_roc_test = evaluator.evaluate(rf_pred_test, {evaluator.metricName: "areaUnderROC"})

    return auc_roc_test  # Maximize AUC_ROC on test data

# Perform Bayesian Optimization
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=50)

# Get the best hyperparameters
best_params = study.best_params
print("Best Parameters:", best_params)

# Train final model with the best hyperparameters
final_rf = GBTClassifier(
    labelCol="label",
    featuresCol="features",
    predictionCol="prediction",
    maxDepth=best_params["maxDepth"],
    minInstancesPerNode=best_params["minInstancesPerNode"],
    stepSize=best_params["stepSize"],
    maxBins=best_params["maxBins"],
    subsamplingRate=best_params["subsamplingRate"],
    seed=22,
)

rf_model = final_rf.fit(balance)

# Evaluate final model on training and test sets
rf_pred_train = rf_model.transform(train_1)
rf_pred_test = rf_model.transform(test_1)
evaluator = BinaryClassificationEvaluator(labelCol=target_column)

# Append metrics
agg_scor_dict["AUC_PR_Train"].append(evaluator.evaluate(rf_pred_train, {evaluator.metricName: "areaUnderPR"}))
agg_scor_dict["AUC_PR_Test"].append(evaluator.evaluate(rf_pred_test, {evaluator.metricName: "areaUnderPR"}))
agg_scor_dict["AUC_ROC_Train"].append(evaluator.evaluate(rf_pred_train, {evaluator.metricName: "areaUnderROC"}))
agg_scor_dict["AUC_ROC_Test"].append(evaluator.evaluate(rf_pred_test, {evaluator.metricName: "areaUnderROC"}))

# Print metrics
for c in agg_scor_dict:
    print(c, agg_scor_dict[c][-1])

# Save metrics to CSV
rf_pred.unpersist()
df_scores = pd.DataFrame(agg_scor_dict).T
df_scores.to_csv("metrics_output.csv", index=False)
print("Metrics saved to metrics_output.csv")

