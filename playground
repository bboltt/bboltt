#!/usr/bin/env python
# run_feature_importance.py

import sys
import pyspark.sql.functions as F
from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml import Pipeline
from datetime import datetime, timedelta

# Example: import your project modules
import private_wealth_retention.consumer_prospecting as cp
from optimal_spark_config.create_spark_instance import generate_spark_instance
from HiveIO.config import get_config, get_schema
from HiveIO.io import set_data_config


def train_rf_and_get_importances(df_labeled, feature_list, label_col="label", n_trees=100, max_depth=5, seed=42):
    """
    A pure function that:
      1) Builds a VectorAssembler and RandomForest pipeline.
      2) Trains the model on df_labeled (Spark DataFrame).
      3) Returns a sorted list of (feature_name, importance) in descending order.

    This can be unit tested by passing in a small Spark DataFrame.
    """
    # Build a VectorAssembler
    assembler = VectorAssembler(
        inputCols=feature_list,
        outputCol="features",
        handleInvalid="skip"   # skip rows with invalid (non-numeric) values
    )

    # Build a RandomForest
    rf = RandomForestClassifier(
        featuresCol="features",
        labelCol=label_col,
        numTrees=n_trees,
        maxDepth=max_depth,
        seed=seed
    )

    # Build a Pipeline
    pipeline = Pipeline(stages=[assembler, rf])
    model = pipeline.fit(df_labeled)

    # The last stage is our trained RandomForestClassificationModel
    rf_model = model.stages[-1]

    # Extract feature importances (Spark ML vector) => convert to python list
    importances = rf_model.featureImportances
    importances_list = importances.toArray()

    # Pair each feature with its importance
    feature_importance_pairs = list(zip(feature_list, importances_list))
    # Sort descending by importance
    feature_importance_pairs.sort(key=lambda x: x[1], reverse=True)

    return feature_importance_pairs


def save_top_features_to_datalake(spark, table_name, business_date, top_features_with_importance, top_n=30, mode="append"):
    """
    Saves the top-N features (and their importances) to a table in the data lake.
    Table schema example:
      business_date: string/date
      feature_name:  string
      rank:          integer
      importance:    double

    You can read from this table later (instead of config) to get the best features.
    """
    # Slice top N
    sliced = top_features_with_importance[:top_n]

    # Build list of rows: (business_date, feature_name, rank, importance)
    rows = []
    for rank, (feat, imp) in enumerate(sliced, start=1):
        rows.append((business_date, feat, rank, float(imp)))

    # Create Spark DataFrame
    columns = ["business_date", "feature_name", "rank", "importance"]
    df_save = spark.createDataFrame(rows, columns)

    # Write to data lake table (Hive table) with desired mode (append/overwrite)
    df_save.write.saveAsTable(table_name, mode=mode)
    # or df_save.write.insertInto(table_name, overwrite=...)
    # or use df_save.write.format("parquet").saveAsTable(...)
    # depending on your environment


def main(top_n=30):
    """
    Main driver function:
      - Creates Spark session
      - Loads config and data
      - Filters data to PWM/Consumer
      - Builds label col
      - Calls train_rf_and_get_importances
      - Prints top-N, saves them to data lake table
    """

    spark = generate_spark_instance(total_memory=600, total_vcpu=30)

    # 1) Load config & schema
    cfg = get_config(cp, "config.yaml")
    schema = get_schema(cp, "schema.yml")

    business_date = cfg["dates"]["business_date"]
    feature_names = list(cfg["feature_names"])
    features_table = cfg["tables"]["prospecting_features"]
    db_name = schema["databases"]["name"]

    # 2) (Optional) If you have a standard function to load data:
    load_data_from, get_table_partitions, _, _ = set_data_config(spark, cfg)
    df_features = load_data_from(features_table)
    # Otherwise, you could do:
    # df_features = spark.sql(f"SELECT * FROM {db_name}.{features_table} ...")

    # 3) Filter to consumer or pwm
    df_features = df_features.filter(F.col("consumer_pwm").isin(["pwm", "consumer"]))

    # 4) Create numeric label
    df_labeled = df_features.withColumn(
        "label",
        F.when(F.col("consumer_pwm") == "pwm", F.lit(1.0)).otherwise(F.lit(0.0))
    )

    # 5) Clean up feature list
    #    Example: remove columns we know are not numeric
    drop_cols = ["hh_id_in_wh", "business_date", "consumer_pwm"]
    feature_list = [f.lower() for f in feature_names if f not in drop_cols]

    # 6) Train the RF model and get all features' importances
    feature_importance_pairs = train_rf_and_get_importances(
        df_labeled,
        feature_list,
        label_col="label",
        n_trees=100,
        max_depth=5,
        seed=42
    )

    # Print top N
    print(f"\nTop {top_n} Most Important Features (Spark ML RandomForest):\n")
    for i, (feature, importance) in enumerate(feature_importance_pairs[:top_n], start=1):
        print(f"{i}. {feature}: {importance:.4f}")

    # 7) Save them to the data lake so you can load them later
    #    Suppose we store in a new table in the same DB, e.g. 'top_feature_importances'
    top_features_table = f"{db_name}.top_feature_importances"
    save_top_features_to_datalake(
        spark,
        table_name=top_features_table,
        business_date=business_date,
        top_features_with_importance=feature_importance_pairs,
        top_n=top_n,
        mode="append"  # or "overwrite"
    )

    # 8) Stop Spark
    spark.stop()


if __name__ == "__main__":
    # Possibly parse top_n from command line
    if len(sys.argv) >= 2:
        top_n = int(sys.argv[1])
    else:
        top_n = 30

    main(top_n)









# model_pipeline.py

import pandas as pd
import numpy as np

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from pyspark.sql import Row, DataFrame, SparkSession


def perform_clustering(spark: SparkSession, df: DataFrame, features: list, k: int):
    """
    1) Convert PWM Spark DF -> Pandas
    2) Scale features
    3) K-Means (scikit-learn)
    4) Assign cluster_id to each row
    5) Compute each PWM's distance to cluster center
    6) Store the max distance per cluster

    Returns:
        df_with_clusters: Spark DF with original columns + 'cluster_id' + 'distance_to_center'
        centers_df     : Spark DF of cluster centers => each row has [cluster_id, features (list)]
        scaler         : Fitted StandardScaler
        features_used  : The list of features we scaled
        max_dist_df    : Spark DF with columns [cluster_id, max_distance]
    """
    # Columns needed in the Pandas PDF
    columns_needed = [
        "hh_id_in_wh",
        "bal_amt_sum",
        "transaction_sum_1_month",
        "recent_opened_counts_30d",
    ] + features

    pdf = df.select(*columns_needed).toPandas().fillna(0.0)

    # 1) Scale
    scaler = StandardScaler()
    scaled = scaler.fit_transform(pdf[features].values)

    # 2) K-Means
    kmeans = KMeans(n_clusters=k, random_state=42)
    cluster_ids = kmeans.fit_predict(scaled)
    pdf["cluster_id"] = cluster_ids

    # 3) Compute distance to cluster center
    cluster_centers = kmeans.cluster_centers_  # shape: (k, len(features))
    distances = []
    for i, c_id in enumerate(cluster_ids):
        dist = np.linalg.norm(scaled[i] - cluster_centers[c_id])
        distances.append(dist)
    pdf["distance_to_center"] = distances

    # 4) Build Spark DF from labeled Pandas DF
    df_with_clusters = spark.createDataFrame(pdf)

    # 5) Also build a Spark DF for cluster centers
    center_rows = []
    for i, center in enumerate(cluster_centers):
        center_rows.append(
            Row(cluster_id=int(i), features=center.tolist())
        )
    centers_df = spark.createDataFrame(center_rows)

    # 6) Build a Spark DF for max distance per cluster
    #    (the cluster's "radius")
    #    We can do this in Pandas or Spark. Here, let's just do it in Pandas quickly:
    gpdf = pdf.groupby("cluster_id")["distance_to_center"].max().reset_index()
    max_dist_rows = []
    for row in gpdf.itertuples(index=False):
        max_dist_rows.append(
            Row(cluster_id=int(row.cluster_id), max_distance=float(row.distance_to_center))
        )
    max_dist_df = spark.createDataFrame(max_dist_rows)

    return df_with_clusters, centers_df, scaler, features, max_dist_df


def calculate_similarity_and_filter(
    spark,
    df,           # large consumer Spark DataFrame
    centers_df,   # Spark DF with columns: [cluster_id, features (array<double>)]
    scaler,       # scikit-learn StandardScaler from perform_clustering
    features,     # list of feature columns used in clustering
    max_dist_df   # Spark DF with columns: [cluster_id, max_distance]
):
    """
    Assigns each consumer to its best cluster based on cosine similarity,
    computes the distance to that cluster center, and filters out consumers
    whose distance exceeds the cluster's farthest PWM distance.

    Returns a Spark DataFrame with these columns (at minimum):
      - hh_id_in_wh
      - cluster_id
      - max_similarity
      - distance_to_center
      - plus any columns that were in `df` except for replaced ones
      - columns from max_dist_df if needed

    The output is already filtered so that distance_to_center <= max_distance.
    """

    import pyspark.sql.functions as F
    import math
    from pyspark.sql.types import DoubleType
    from pyspark.sql.window import Window

    # 1) Extract scikit-learn scaling info
    #    We'll replicate mean_/scale_ in Spark for each feature
    mean_values = list(scaler.mean_)
    scale_values = list(scaler.scale_)

    # 2) Build columns to scale each feature in Spark
    #    For each feature f_i, we create a column: (f_i - mean_i)/scale_i
    scaled_exprs = []
    for i, fcol in enumerate(features):
        scaled_exprs.append(
            ((F.col(fcol) - mean_values[i]) / scale_values[i]).alias(f"{fcol}_scaled")
        )

    # Keep other columns from df (e.g. "hh_id_in_wh") so we can join later
    # but do *not* drop them yet. We'll re-select everything we need.
    all_cols_except_features = [c for c in df.columns if c not in features]

    # 3) Create a new DataFrame with scaled feature columns + original non-feature columns
    df_scaled = df.select(
        *all_cols_except_features,
        *scaled_exprs
    )

    # 4) Combine the scaled columns into a single array column, e.g. "scaledFeatures"
    df_scaled = df_scaled.withColumn(
        "scaledFeatures",
        F.array([F.col(f"{fcol}_scaled") for fcol in features])
    )

    # 5) Define UDFs for cosine similarity and Euclidean distance
    @F.udf(DoubleType())
    def cos_sim(v1, v2):
        # v1, v2 are Python lists or tuples of floats
        dot = 0.0
        norm1 = 0.0
        norm2 = 0.0
        for x, y in zip(v1, v2):
            dot += x * y
            norm1 += x * x
            norm2 += y * y
        if norm1 == 0.0 or norm2 == 0.0:
            return 0.0
        return float(dot / math.sqrt(norm1 * norm2))

    @F.udf(DoubleType())
    def euc_dist(v1, v2):
        s = 0.0
        for x, y in zip(v1, v2):
            diff = x - y
            s += diff * diff
        return float(math.sqrt(s))

    # 6) Cross join each consumer row with each cluster center => compute similarity, distance
    #    Make sure "features" in centers_df is an array<double> (scaled center).
    df_join = df_scaled.crossJoin(centers_df)

    df_join = df_join \
        .withColumn("cos_similarity", cos_sim("scaledFeatures", "features")) \
        .withColumn("distance_to_center", euc_dist("scaledFeatures", "features"))

    # 7) For each consumer (identified by something like hh_id_in_wh),
    #    pick the single cluster with the maximum similarity
    w = Window.partitionBy("hh_id_in_wh").orderBy(F.col("cos_similarity").desc())
    df_best = df_join \
        .withColumn("rn", F.row_number().over(w)) \
        .filter(F.col("rn") == 1) \
        .drop("rn") \
        .withColumnRenamed("cos_similarity", "max_similarity")

    # 8) Join with the cluster's max_distance and filter out consumers beyond that radius
    df_filtered = df_best.join(max_dist_df, on="cluster_id", how="left") \
                         .filter(F.col("distance_to_center") <= F.col("max_distance"))

    return df_filtered





# prospecting.py

# prospecting.py

import pyspark.sql.functions as F
from pyspark.sql import DataFrame, SparkSession
from private_wealth_retention.consumer_prospecting.model.model_pipeline import (
    perform_clustering,
    calculate_similarity_and_filter  # or your final approach to consumer assignment
)

def get_prospects(spark: SparkSession,
                  pwm_df: DataFrame,
                  consumer_df: DataFrame,
                  k: int,
                  n: int,
                  feature_cols: list,
                  prospect_segment: str,
                  business_date: str):
    """
    1) Perform K-Means on PWM data (affluent or high_net_worth).
    2) Assign each PWM client a cluster, store cluster_id + distance_to_center, etc.
    3) For consumers, compute similarity/distance, pick the best cluster, 
       and limit to top N prospects by similarity.
    4) Return two DataFrames:
       - df_pwm_clusters: minimal columns for existing PWM
       - df_prospects: minimal columns for identified prospects
    """

    # 1) Cluster the PWM data using your 'perform_clustering' function.
    #    Make sure that function returns distance_to_center for each row
    #    (via e.g. an added column or storing in a new DF).
    df_pwm_clustered, centers_df, scaler, used_features, max_dist_df = perform_clustering(
        spark, pwm_df, feature_cols, k
    )
    # df_pwm_clustered is your PWM data with columns like: hh_id_in_wh, cluster_id, distance_to_center, etc.

    # 2) Build a minimal DataFrame for PWM cluster assignments
    df_pwm_clusters = df_pwm_clustered.select(
        F.col("hh_id_in_wh"),
        F.col("cluster_id"),
        F.col("distance_to_center"),
        F.lit(business_date).alias("business_date")
    )

    # 3) Assign clusters to consumers, compute distance, filter if desired.
    df_assigned = calculate_similarity_and_filter(
        spark, consumer_df, centers_df, scaler, used_features, max_dist_df
    )
    # Suppose the returned DF has columns: hh_id_in_wh, max_similarity, distance_to_center, cluster_id, etc.

    # 4) Pick top N by similarity
    df_top = df_assigned.orderBy(F.col("max_similarity").desc()).limit(n)

    # 5) Build a minimal DataFrame for prospects
    df_prospects = df_top.select(
        F.col("hh_id_in_wh"),
        F.col("max_similarity").alias("similarity_score"),
        F.col("distance_to_center"),
        F.lit(prospect_segment).alias("prospect_segment"),
        F.col("cluster_id"),
        F.lit(business_date).alias("business_date")
    )

    # Return two DataFrames:
    # - df_pwm_clusters (PWM assignments)
    # - df_prospects (consumer prospects)
    return df_pwm_clusters, df_prospects





# run_prospect.py

if __name__ == "__main__":
    spark = ...
    # load df_features, filter pwm, consumer, etc.
    # pick feature_cols, k, n

    # Affluent
    affluent_prospects = get_prospects(
        spark, affluent_pwm, consumer, k, n, features
    ).withColumn("prospect_segmet", F.lit("Affluent"))

    # High Net Worth
    high_net_worth_prospects = get_prospects(
        spark, high_net_worth_pwm, consumer, k, n, features
    ).withColumn("prospect_segmet", F.lit("High Net Worth"))

    # Union and write
    prospects = affluent_prospects.union(high_net_worth_prospects)
    ...


