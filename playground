import pandas as pd
from pyspark.sql.functions import col, min, max, avg, stddev, count, when, expr, percentile_approx

# List of feature columns
feature_columns = ["feature1", "feature2", "feature3"]  # Replace with actual feature names

# Compute statistics for each column
stats_list = []
for feature in feature_columns:
    stats = df.agg(
        min(col(feature)).alias("min"),
        max(col(feature)).alias("max"),
        avg(col(feature)).alias("avg"),
        stddev(col(feature)).alias("stddev"),
        percentile_approx(col(feature), 0.5).alias("median"),  # Approximate median
        (100 * (count(when(col(feature) == 0, 1)) / count(col(feature)))).alias("percent_zeros")
    ).withColumn("feature", expr(f"'{feature}'"))  # Add feature name as a column

    stats_list.append(stats)

# Union all computed stats
final_stats_df = stats_list[0]
for stats in stats_list[1:]:
    final_stats_df = final_stats_df.union(stats)

# Reorder columns
final_stats_df = final_stats_df.select("feature", "min", "max", "median", "avg", "stddev", "percent_zeros")

# Convert to pandas DataFrame
pandas_df = final_stats_df.toPandas()

# Display in Jupyter Notebook
import ace_tools as tools
tools.display_dataframe_to_user(name="Feature Statistics", dataframe=pandas_df)

# Save to CSV if needed
pandas_df.to_csv("feature_statistics.csv", index=False)




# run_insight.py

import sys
import pyspark.sql.functions as F

from optimal_spark_config.create_spark_instance import generate_spark_instance
from HiveIO.config import get_config, get_schema
from HiveIO.io import set_data_config
from private_wealth_retention.consumer_prospecting.model.insight_pipeline import (
    train_cluster_insight_model,
    compute_cluster_mean_std,
    generate_insight_string
)

def main():
    spark = generate_spark_instance(total_memory=600, total_vcpu=300)

    # 1) Load config, schema, etc.
    #    or skip if you only rely on tables from data lake
    #    We'll assume we have a config with table names
    cfg = get_config(None, "config.yaml")  
    schema = get_schema(None, "schema.yml")
    db_name = schema["databases"][0]["name"]
    business_date = cfg["dates"]["business_date"]

    # 2) We assume we have two tables (or data files):
    #    a) df_prospects_all => minimal columns + cluster_id
    #    b) df_pwm_clusters_all => minimal columns + cluster_id
    #    c) We also need the underlying feature columns for those prospects/pwm
    #       so we can read them or join them from the original data.

    #    Let's say we have:
    prospects_table = "DM_WM_DEV.prospects_all"
    pwm_table = "DM_WM_DEV.pwm_clusters_all"
    features_table = cfg["tables"]["prospecting_features"]  # full feature data?

    # Or load from data lake config if you prefer:
    load_data_from, get_table_partitions, _, _ = set_data_config(spark, cfg)

    df_prospects_base = load_data_from(prospects_table)
    df_pwm_base = load_data_from(pwm_table)
    # For actual feature values, we can load from features_table
    df_features = load_data_from(features_table).filter(F.col("business_date") == business_date)

    # 3) We need to join the features to each row so we know the numeric columns
    #    for each hh_id_in_wh. We'll assume "hh_id_in_wh" is the join key.
    #    We'll join the prospects with the features, similarly PWM with the features.

    # pick which feature columns to keep (or keep all numeric)
    # here we assume your feature columns are stored in config or we discover them dynamically
    feature_cols = cfg["features"]["all_numeric_cols"]

    df_prospects = df_prospects_base.join(df_features.select("hh_id_in_wh", *feature_cols),
                                          on="hh_id_in_wh",
                                          how="left")

    df_pwm = df_pwm_base.join(df_features.select("hh_id_in_wh", *feature_cols),
                              on="hh_id_in_wh",
                              how="left")

    # 4) We want to gather cluster IDs. We'll do distinct to avoid duplicates:
    cluster_ids = [row["cluster_id"] for row in df_prospects.select("cluster_id").distinct().collect()]

    # 5) Build a final DataFrame of prospects with an "insight" column
    #    We'll do it cluster by cluster, then union results.

    union_df = None


    for c_id in cluster_ids:
        # a) Train a mini “cluster vs. rest” RF to get top 5 features
        feature_importances = train_cluster_insight_model(
            df=df_prospects,
            cluster_col="cluster_id",
            cluster_id=c_id,
            feature_cols=feature_cols,
            label_col="label"
        )
        top_5_features = [fp[0] for fp in feature_importances[:5]]

        # b) Compute mean/std for PWM in this cluster
        stats_dict = compute_cluster_mean_std(
            df=df_pwm,
            cluster_id=c_id,
            pwm_cluster_col="cluster_id",
            feature_cols=top_5_features
        )

        # c) Filter prospects in cluster c_id
        df_cluster = df_prospects.filter(F.col("cluster_id") == c_id)

        # d) Collect them to the driver
        local_rows = df_cluster.collect()  # <--- Avoiding UDF, but watch memory usage

        # e) Build new rows with an "insight" column
        new_data = []
        for row in local_rows:
            row_dict = row.asDict()  # convert Row to dict
            # Build row_values for top features
            row_values = {}
            for feat in top_5_features:
                row_values[feat] = row_dict.get(feat, 0.0) or 0.0

            # Generate the insight string
            # (We don't need the actual feature importances for the string, just names)
            top_feats_stub = [(f, 0.0) for f in top_5_features]
            insight_str = generate_insight_string(
                top_features=top_feats_stub,
                row_values=row_values,
                cluster_stats=stats_dict
            )

            # Add the new column
            row_dict["insight"] = insight_str
            new_data.append(row_dict)

        # f) Build a new Spark DF with the “insight” column
        #    We'll just adapt df_cluster's schema + add "insight".
        #    Note: if "insight" doesn't exist in the old schema, we must add it.
        new_schema = df_cluster.schema.add(
            StructField("insight", StringType(), True)
        )
        df_cluster_with_insight = spark.createDataFrame(new_data, schema=new_schema)

        # g) Union into union_df
        if union_df is None:
            union_df = df_cluster_with_insight
        else:
            union_df = union_df.union(df_cluster_with_insight)

    # 6) Now union_df has all prospects, with an "insight" column for each cluster
    #    We can finalize columns. E.g. we only want:
    #    [hh_id_in_wh, similarity_score, distance_to_center, prospect_segment, cluster_id, insight, business_date]

    final_cols = [
        "hh_id_in_wh",
        "similarity_score",
        "distance_to_center",
        "prospect_segment",
        "cluster_id",
        "insight",
        "business_date"
    ]
    df_insight = union_df.select(*[F.col(c) for c in final_cols])

    # 7) Write final result to a table or do whatever is needed
    # e.g. df_insight.write.saveAsTable("DM_WM_DEV.prospect_insights", mode="overwrite")

    spark.stop()


if __name__ == "__main__":
    main()




# insight_pipeline.py
# Placed in private_wealth_retention/consumer_prospecting/model/

import pyspark.sql.functions as F
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml import Pipeline

def train_cluster_insight_model(df: "DataFrame",
                                cluster_col: str,
                                cluster_id: int,
                                feature_cols: list,
                                label_col: str = "label",
                                seed: int = 42):
    """
    Trains a RandomForest (Spark ML) to distinguish "prospects in cluster_id"
    vs. "prospects in other clusters" by setting label=1 for the cluster, else 0.
    Returns a list of (feature, importance) sorted descending.
    """
    # 1) Build label: 1 if cluster_col == cluster_id, else 0
    df_labeled = df.withColumn(
        label_col,
        F.when(F.col(cluster_col) == cluster_id, 1.0).otherwise(0.0)
    )

    # 2) VectorAssembler
    assembler = VectorAssembler(
        inputCols=feature_cols,
        outputCol="features",
        handleInvalid="skip"
    )

    # 3) RandomForest
    rf = RandomForestClassifier(
        featuresCol="features",
        labelCol=label_col,
        numTrees=50,      # can adjust
        maxDepth=5,       # can adjust
        seed=seed
    )

    pipeline = Pipeline(stages=[assembler, rf])
    model = pipeline.fit(df_labeled)

    # 4) Extract feature importances
    rf_model = model.stages[-1]
    importances = rf_model.featureImportances.toArray()
    feature_importance_pairs = list(zip(feature_cols, importances))
    feature_importance_pairs.sort(key=lambda x: x[1], reverse=True)
    return feature_importance_pairs


def compute_cluster_mean_std(df, cluster_id, pwm_cluster_col, feature_cols):
    """
    Given a DataFrame of PWM data (or you can filter it before calling),
    compute mean and std for each feature in feature_cols among
    the PWM who belong to cluster_id.
    Returns a dict: { feature: (mean, std), ... }
    """
    # Filter PWM for cluster_id
    df_filtered = df.filter(F.col(pwm_cluster_col) == cluster_id)

    # compute mean/std for each feature
    agg_exprs = []
    for fcol in feature_cols:
        agg_exprs.append(F.mean(fcol).alias(f"{fcol}_mean"))
        agg_exprs.append(F.stddev_pop(fcol).alias(f"{fcol}_std"))  # or stddev_samp

    row_stats = df_filtered.agg(*agg_exprs).collect()[0]
    # Build a dict: feature -> (mean, std)
    stats_dict = {}
    for fcol in feature_cols:
        mean_val = row_stats[f"{fcol}_mean"] if row_stats[f"{fcol}_mean"] is not None else 0.0
        std_val = row_stats[f"{fcol}_std"] if row_stats[f"{fcol}_std"] is not None else 0.0
        stats_dict[fcol] = (float(mean_val), float(std_val))
    return stats_dict


def generate_insight_string(top_features, row_values, cluster_stats):
    """
    Build a single string describing top 5 features (or fewer).
    `top_features`: [(featureName, importance), ...] sorted descending
    `row_values`:   dict with { featureName -> this prospect's value }
    `cluster_stats`: dict with { featureName -> (mean, std) }

    Returns a single string containing each feature's value, cluster's mean,
    and [mean ± 2*std] range.
    """
    # Only take top 5
    top_5 = top_features[:5]
    lines = []
    for i, (feat, imp) in enumerate(top_5, start=1):
        val = row_values.get(feat, 0.0)
        mean_val, std_val = cluster_stats.get(feat, (0.0, 0.0))
        lower = mean_val - 2 * std_val
        upper = mean_val + 2 * std_val
        line = (f"{i}) Feature: {feat}, value={val}, "
                f"cluster mean={mean_val:.2f}, range=[{lower:.2f}, {upper:.2f}]")
        lines.append(line)

    insight_str = " | ".join(lines)
    return insight_str






#!/usr/bin/env python
# run_feature_importance.py

import sys
import pyspark.sql.functions as F
from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml import Pipeline
from datetime import datetime, timedelta

# Example: import your project modules
import private_wealth_retention.consumer_prospecting as cp
from optimal_spark_config.create_spark_instance import generate_spark_instance
from HiveIO.config import get_config, get_schema
from HiveIO.io import set_data_config


def train_rf_and_get_importances(df_labeled, feature_list, label_col="label", n_trees=100, max_depth=5, seed=42):
    """
    A pure function that:
      1) Builds a VectorAssembler and RandomForest pipeline.
      2) Trains the model on df_labeled (Spark DataFrame).
      3) Returns a sorted list of (feature_name, importance) in descending order.

    This can be unit tested by passing in a small Spark DataFrame.
    """
    # Build a VectorAssembler
    assembler = VectorAssembler(
        inputCols=feature_list,
        outputCol="features",
        handleInvalid="skip"   # skip rows with invalid (non-numeric) values
    )

    # Build a RandomForest
    rf = RandomForestClassifier(
        featuresCol="features",
        labelCol=label_col,
        numTrees=n_trees,
        maxDepth=max_depth,
        seed=seed
    )

    # Build a Pipeline
    pipeline = Pipeline(stages=[assembler, rf])
    model = pipeline.fit(df_labeled)

    # The last stage is our trained RandomForestClassificationModel
    rf_model = model.stages[-1]

    # Extract feature importances (Spark ML vector) => convert to python list
    importances = rf_model.featureImportances
    importances_list = importances.toArray()

    # Pair each feature with its importance
    feature_importance_pairs = list(zip(feature_list, importances_list))
    # Sort descending by importance
    feature_importance_pairs.sort(key=lambda x: x[1], reverse=True)

    return feature_importance_pairs


def save_top_features_to_datalake(spark, table_name, business_date, top_features_with_importance, top_n=30, mode="append"):
    """
    Saves the top-N features (and their importances) to a table in the data lake.
    Table schema example:
      business_date: string/date
      feature_name:  string
      rank:          integer
      importance:    double

    You can read from this table later (instead of config) to get the best features.
    """
    # Slice top N
    sliced = top_features_with_importance[:top_n]

    # Build list of rows: (business_date, feature_name, rank, importance)
    rows = []
    for rank, (feat, imp) in enumerate(sliced, start=1):
        rows.append((business_date, feat, rank, float(imp)))

    # Create Spark DataFrame
    columns = ["business_date", "feature_name", "rank", "importance"]
    df_save = spark.createDataFrame(rows, columns)

    # Write to data lake table (Hive table) with desired mode (append/overwrite)
    df_save.write.saveAsTable(table_name, mode=mode)
    # or df_save.write.insertInto(table_name, overwrite=...)
    # or use df_save.write.format("parquet").saveAsTable(...)
    # depending on your environment


def main(top_n=30):
    """
    Main driver function:
      - Creates Spark session
      - Loads config and data
      - Filters data to PWM/Consumer
      - Builds label col
      - Calls train_rf_and_get_importances
      - Prints top-N, saves them to data lake table
    """

    spark = generate_spark_instance(total_memory=600, total_vcpu=30)

    # 1) Load config & schema
    cfg = get_config(cp, "config.yaml")
    schema = get_schema(cp, "schema.yml")

    business_date = cfg["dates"]["business_date"]
    feature_names = list(cfg["feature_names"])
    features_table = cfg["tables"]["prospecting_features"]
    db_name = schema["databases"]["name"]

    # 2) (Optional) If you have a standard function to load data:
    load_data_from, get_table_partitions, _, _ = set_data_config(spark, cfg)
    df_features = load_data_from(features_table)
    # Otherwise, you could do:
    # df_features = spark.sql(f"SELECT * FROM {db_name}.{features_table} ...")

    # 3) Filter to consumer or pwm
    df_features = df_features.filter(F.col("consumer_pwm").isin(["pwm", "consumer"]))

    # 4) Create numeric label
    df_labeled = df_features.withColumn(
        "label",
        F.when(F.col("consumer_pwm") == "pwm", F.lit(1.0)).otherwise(F.lit(0.0))
    )

    # 5) Clean up feature list
    #    Example: remove columns we know are not numeric
    drop_cols = ["hh_id_in_wh", "business_date", "consumer_pwm"]
    feature_list = [f.lower() for f in feature_names if f not in drop_cols]

    # 6) Train the RF model and get all features' importances
    feature_importance_pairs = train_rf_and_get_importances(
        df_labeled,
        feature_list,
        label_col="label",
        n_trees=100,
        max_depth=5,
        seed=42
    )

    # Print top N
    print(f"\nTop {top_n} Most Important Features (Spark ML RandomForest):\n")
    for i, (feature, importance) in enumerate(feature_importance_pairs[:top_n], start=1):
        print(f"{i}. {feature}: {importance:.4f}")

    # 7) Save them to the data lake so you can load them later
    #    Suppose we store in a new table in the same DB, e.g. 'top_feature_importances'
    top_features_table = f"{db_name}.top_feature_importances"
    save_top_features_to_datalake(
        spark,
        table_name=top_features_table,
        business_date=business_date,
        top_features_with_importance=feature_importance_pairs,
        top_n=top_n,
        mode="append"  # or "overwrite"
    )

    # 8) Stop Spark
    spark.stop()


if __name__ == "__main__":
    # Possibly parse top_n from command line
    if len(sys.argv) >= 2:
        top_n = int(sys.argv[1])
    else:
        top_n = 30

    main(top_n)









# model_pipeline.py

import pandas as pd
import numpy as np

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from pyspark.sql import Row, DataFrame, SparkSession


def perform_clustering(spark: SparkSession, df: DataFrame, features: list, k: int):
    """
    1) Convert PWM Spark DF -> Pandas
    2) Scale features
    3) K-Means (scikit-learn)
    4) Assign cluster_id to each row
    5) Compute each PWM's distance to cluster center
    6) Store the max distance per cluster

    Returns:
        df_with_clusters: Spark DF with original columns + 'cluster_id' + 'distance_to_center'
        centers_df     : Spark DF of cluster centers => each row has [cluster_id, features (list)]
        scaler         : Fitted StandardScaler
        features_used  : The list of features we scaled
        max_dist_df    : Spark DF with columns [cluster_id, max_distance]
    """
    # Columns needed in the Pandas PDF
    columns_needed = [
        "hh_id_in_wh",
        "bal_amt_sum",
        "transaction_sum_1_month",
        "recent_opened_counts_30d",
    ] + features

    pdf = df.select(*columns_needed).toPandas().fillna(0.0)

    # 1) Scale
    scaler = StandardScaler()
    scaled = scaler.fit_transform(pdf[features].values)

    # 2) K-Means
    kmeans = KMeans(n_clusters=k, random_state=42)
    cluster_ids = kmeans.fit_predict(scaled)
    pdf["cluster_id"] = cluster_ids

    # 3) Compute distance to cluster center
    cluster_centers = kmeans.cluster_centers_  # shape: (k, len(features))
    distances = []
    for i, c_id in enumerate(cluster_ids):
        dist = np.linalg.norm(scaled[i] - cluster_centers[c_id])
        distances.append(dist)
    pdf["distance_to_center"] = distances

    # 4) Build Spark DF from labeled Pandas DF
    df_with_clusters = spark.createDataFrame(pdf)

    # 5) Also build a Spark DF for cluster centers
    center_rows = []
    for i, center in enumerate(cluster_centers):
        center_rows.append(
            Row(cluster_id=int(i), features=center.tolist())
        )
    centers_df = spark.createDataFrame(center_rows)

    # 6) Build a Spark DF for max distance per cluster
    #    (the cluster's "radius")
    #    We can do this in Pandas or Spark. Here, let's just do it in Pandas quickly:
    gpdf = pdf.groupby("cluster_id")["distance_to_center"].max().reset_index()
    max_dist_rows = []
    for row in gpdf.itertuples(index=False):
        max_dist_rows.append(
            Row(cluster_id=int(row.cluster_id), max_distance=float(row.distance_to_center))
        )
    max_dist_df = spark.createDataFrame(max_dist_rows)

    return df_with_clusters, centers_df, scaler, features, max_dist_df


def calculate_similarity_and_filter(
    spark,
    df,           # large consumer Spark DataFrame
    centers_df,   # Spark DF with columns: [cluster_id, features (array<double>)]
    scaler,       # scikit-learn StandardScaler from perform_clustering
    features,     # list of feature columns used in clustering
    max_dist_df   # Spark DF with columns: [cluster_id, max_distance]
):
    """
    Assigns each consumer to its best cluster based on cosine similarity,
    computes the distance to that cluster center, and filters out consumers
    whose distance exceeds the cluster's farthest PWM distance.

    Returns a Spark DataFrame with these columns (at minimum):
      - hh_id_in_wh
      - cluster_id
      - max_similarity
      - distance_to_center
      - plus any columns that were in `df` except for replaced ones
      - columns from max_dist_df if needed

    The output is already filtered so that distance_to_center <= max_distance.
    """

    import pyspark.sql.functions as F
    import math
    from pyspark.sql.types import DoubleType
    from pyspark.sql.window import Window

    # 1) Extract scikit-learn scaling info
    #    We'll replicate mean_/scale_ in Spark for each feature
    mean_values = list(scaler.mean_)
    scale_values = list(scaler.scale_)

    # 2) Build columns to scale each feature in Spark
    #    For each feature f_i, we create a column: (f_i - mean_i)/scale_i
    scaled_exprs = []
    for i, fcol in enumerate(features):
        scaled_exprs.append(
            ((F.col(fcol) - mean_values[i]) / scale_values[i]).alias(f"{fcol}_scaled")
        )

    # Keep other columns from df (e.g. "hh_id_in_wh") so we can join later
    # but do *not* drop them yet. We'll re-select everything we need.
    all_cols_except_features = [c for c in df.columns if c not in features]

    # 3) Create a new DataFrame with scaled feature columns + original non-feature columns
    df_scaled = df.select(
        *all_cols_except_features,
        *scaled_exprs
    )

    # 4) Combine the scaled columns into a single array column, e.g. "scaledFeatures"
    df_scaled = df_scaled.withColumn(
        "scaledFeatures",
        F.array([F.col(f"{fcol}_scaled") for fcol in features])
    )

    # 5) Define UDFs for cosine similarity and Euclidean distance
    @F.udf(DoubleType())
    def cos_sim(v1, v2):
        # v1, v2 are Python lists or tuples of floats
        dot = 0.0
        norm1 = 0.0
        norm2 = 0.0
        for x, y in zip(v1, v2):
            dot += x * y
            norm1 += x * x
            norm2 += y * y
        if norm1 == 0.0 or norm2 == 0.0:
            return 0.0
        return float(dot / math.sqrt(norm1 * norm2))

    @F.udf(DoubleType())
    def euc_dist(v1, v2):
        s = 0.0
        for x, y in zip(v1, v2):
            diff = x - y
            s += diff * diff
        return float(math.sqrt(s))

    # 6) Cross join each consumer row with each cluster center => compute similarity, distance
    #    Make sure "features" in centers_df is an array<double> (scaled center).
    df_join = df_scaled.crossJoin(centers_df)

    df_join = df_join \
        .withColumn("cos_similarity", cos_sim("scaledFeatures", "features")) \
        .withColumn("distance_to_center", euc_dist("scaledFeatures", "features"))

    # 7) For each consumer (identified by something like hh_id_in_wh),
    #    pick the single cluster with the maximum similarity
    w = Window.partitionBy("hh_id_in_wh").orderBy(F.col("cos_similarity").desc())
    df_best = df_join \
        .withColumn("rn", F.row_number().over(w)) \
        .filter(F.col("rn") == 1) \
        .drop("rn") \
        .withColumnRenamed("cos_similarity", "max_similarity")

    # 8) Join with the cluster's max_distance and filter out consumers beyond that radius
    df_filtered = df_best.join(max_dist_df, on="cluster_id", how="left") \
                         .filter(F.col("distance_to_center") <= F.col("max_distance"))

    return df_filtered





# prospecting.py

# prospecting.py

import pyspark.sql.functions as F
from pyspark.sql import DataFrame, SparkSession
from private_wealth_retention.consumer_prospecting.model.model_pipeline import (
    perform_clustering,
    calculate_similarity_and_filter  # or your final approach to consumer assignment
)

def get_prospects(spark: SparkSession,
                  pwm_df: DataFrame,
                  consumer_df: DataFrame,
                  k: int,
                  n: int,
                  feature_cols: list,
                  prospect_segment: str,
                  business_date: str):
    """
    1) Perform K-Means on PWM data (affluent or high_net_worth).
    2) Assign each PWM client a cluster, store cluster_id + distance_to_center, etc.
    3) For consumers, compute similarity/distance, pick the best cluster, 
       and limit to top N prospects by similarity.
    4) Return two DataFrames:
       - df_pwm_clusters: minimal columns for existing PWM
       - df_prospects: minimal columns for identified prospects
    """

    # 1) Cluster the PWM data using your 'perform_clustering' function.
    #    Make sure that function returns distance_to_center for each row
    #    (via e.g. an added column or storing in a new DF).
    df_pwm_clustered, centers_df, scaler, used_features, max_dist_df = perform_clustering(
        spark, pwm_df, feature_cols, k
    )
    # df_pwm_clustered is your PWM data with columns like: hh_id_in_wh, cluster_id, distance_to_center, etc.

    # 2) Build a minimal DataFrame for PWM cluster assignments
    df_pwm_clusters = df_pwm_clustered.select(
        F.col("hh_id_in_wh"),
        F.col("cluster_id"),
        F.col("distance_to_center"),
        F.lit(business_date).alias("business_date")
    )

    # 3) Assign clusters to consumers, compute distance, filter if desired.
    df_assigned = calculate_similarity_and_filter(
        spark, consumer_df, centers_df, scaler, used_features, max_dist_df
    )
    # Suppose the returned DF has columns: hh_id_in_wh, max_similarity, distance_to_center, cluster_id, etc.

    # 4) Pick top N by similarity
    df_top = df_assigned.orderBy(F.col("max_similarity").desc()).limit(n)

    # 5) Build a minimal DataFrame for prospects
    df_prospects = df_top.select(
        F.col("hh_id_in_wh"),
        F.col("max_similarity").alias("similarity_score"),
        F.col("distance_to_center"),
        F.lit(prospect_segment).alias("prospect_segment"),
        F.col("cluster_id"),
        F.lit(business_date).alias("business_date")
    )

    # Return two DataFrames:
    # - df_pwm_clusters (PWM assignments)
    # - df_prospects (consumer prospects)
    return df_pwm_clusters, df_prospects





# run_prospect.py

# run_prospect.py

def main():
    spark = generate_spark_instance(total_memory=600, total_vcpu=300)
    cfg = get_config(cp, "config.yaml")
    schema = get_schema(cp, "schema.yml")

    # load features
    # read selected features from the "selected_features_table" in data lake, etc.
    # gather PWM subsets & consumer subset...

    # for "Affluent" PWM
    df_pwm_clusters_affluent, df_prospects_affluent = get_prospects(
        spark,
        pwm_affluent,
        consumer,
        k,
        n,
        selected_features,
        prospect_segment="Affluent",
        business_date=business_date
    )

    # for "High Net Worth" PWM
    df_pwm_clusters_hnw, df_prospects_hnw = get_prospects(
        spark,
        pwm_high_net_worth,
        consumer,
        k,
        n,
        selected_features,
        prospect_segment="High Net Worth",
        business_date=business_date
    )

    # union the clusters if you want a single table, or save them separately
    df_clusters_all = df_pwm_clusters_affluent.union(df_pwm_clusters_hnw)
    df_prospects_all = df_prospects_affluent.union(df_prospects_hnw)

    # store them as needed
    # e.g. df_clusters_all.write.saveAsTable(...)
    # e.g. df_prospects_all.write.saveAsTable(...)

    spark.stop()

