from pyspark.sql.functions import col, countDistinct, to_date, datediff, when

# Add a column for the difference in days between open_date and business_date
df = df.withColumn("days_diff", datediff(to_date(col("business_date")), to_date(col("open_date"))))

# Add intermediate columns for each time window
df = df.withColumn("ip_in_1m", when((col("days_diff") <= 30) & (col("days_diff") >= 0), col("ip_id")))
df = df.withColumn("ip_in_2m", when((col("days_diff") <= 60) & (col("days_diff") >= 0), col("ip_id")))
df = df.withColumn("ip_in_6m", when((col("days_diff") <= 180) & (col("days_diff") >= 0), col("ip_id")))
df = df.withColumn("ip_in_12m", when((col("days_diff") <= 365) & (col("days_diff") >= 0), col("ip_id")))

# Group by segmt_prod_type and count distinct ip_id for each time window
result_df = (
    df.groupBy("segmt_prod_type")
    .agg(
        countDistinct("ip_in_1m").alias("distinct_ip_count_1m"),
        countDistinct("ip_in_2m").alias("distinct_ip_count_2m"),
        countDistinct("ip_in_6m").alias("distinct_ip_count_6m"),
        countDistinct("ip_in_12m").alias("distinct_ip_count_12m")
    )
)

# Show the result
result_df.show()





import os
import numpy as np
import pandas as pd
import optuna

from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.functions import col, explode, udf
from pyspark.sql.types import FloatType, ArrayType

from pyspark.ml import Pipeline
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import GBTClassifier
from pyspark.ml.evaluation import BinaryClassificationEvaluator

from inclearing.dev_data.feature import (
    Identifier,
    Feat_NUM_50_RETRAIN,
    Target
)
from optimal_spark_config.create_spark_instance import generate_spark_instance

#----------------------------------------------------------------------------------
# 1. Create Spark session
#----------------------------------------------------------------------------------
spark = generate_spark_instance(total_memory=400, total_vcpu=200)

#----------------------------------------------------------------------------------
# 2. Load / prepare data
#----------------------------------------------------------------------------------
DEV_trans = (
    spark.table("dm_fraud_detection.inc_frd_train_mtkpop_v5_imputed")
    .filter(col('ods_business_dt').between('2024-04-22', '2024-10-04'))
    .fillna(-999, subset=Feat_NUM_50_RETRAIN)
    .select(*Identifier, *Feat_NUM_50_RETRAIN, *Target, 'train_set')
)

target_column = "label"

assemblerNum = VectorAssembler(inputCols=Feat_NUM_50_RETRAIN, outputCol="features")
pipe_assem = Pipeline(stages=[assemblerNum])
DEV_1 = pipe_assem.fit(DEV_trans)
DEV_vec = DEV_1.transform(DEV_trans)

train_1 = DEV_vec.filter(col('train_set') == 1)
test_1 = DEV_vec.filter(col('train_set') == 0)

#----------------------------------------------------------------------------------
# 3. Handle class imbalance
#----------------------------------------------------------------------------------
def handle_imbalanced_data(df, factor, target_column, imbalanced_value):
    # "factor" = how many times to replicate minority class
    num_rep = int(
        np.round(
            df.count() / df.filter(df[target_column] == imbalanced_value).count() * factor
        )
    )
    # UDF to create an array of 'num_rep' placeholders for label=1
    n_to_array = udf(
        lambda x: [0.0] * num_rep if x == imbalanced_value else [0.0],
        ArrayType(FloatType())
    )
    df = df.withColumn("n", n_to_array(df[target_column]))
    df = df.withColumn("n", explode(df.n))
    return df.drop("n")

balance = handle_imbalanced_data(train_1, factor=0.5, target_column=target_column, imbalanced_value=1)

# Cache everything to speed up repeated operations
balance = balance.cache()
balance.count()  # materialize the cache
train_1 = train_1.cache()
train_1.count()
test_1 = test_1.cache()
test_1.count()

#----------------------------------------------------------------------------------
# 4. Objective function (one training run per trial)
#----------------------------------------------------------------------------------
def objective(trial):
    # Choose hyperparameters
    max_depth = trial.suggest_int("maxDepth", 3, 8)
    min_instances_per_node = trial.suggest_int("minInstancesPerNode", 10, 30)
    step_size = trial.suggest_loguniform("stepSize", 0.2, 0.4)
    max_bins = trial.suggest_categorical("maxBins", [64])
    subsampling_rate = trial.suggest_float("subsamplingRate", 0.8, 1.0)
    max_iter = trial.suggest_int("maxIter", 50, 250, step=50)

    # Define model
    gbt = GBTClassifier(
        labelCol=target_column,
        featuresCol="features",
        predictionCol="prediction",
        maxDepth=max_depth,
        minInstancesPerNode=min_instances_per_node,
        stepSize=step_size,
        maxBins=max_bins,
        subsamplingRate=subsampling_rate,
        seed=22,
        maxIter=max_iter
    )

    # Fit model once
    model = gbt.fit(balance)

    # Predict
    pred_train = model.transform(train_1).cache()
    pred_train.count()  # materialize
    pred_test = model.transform(test_1).cache()
    pred_test.count()

    # Evaluate
    evaluator = BinaryClassificationEvaluator(labelCol=target_column)
    auc_pr_train = evaluator.evaluate(pred_train, {evaluator.metricName: "areaUnderPR"})
    auc_roc_train = evaluator.evaluate(pred_train, {evaluator.metricName: "areaUnderROC"})
    auc_pr_test = evaluator.evaluate(pred_test, {evaluator.metricName: "areaUnderPR"})
    auc_roc_test = evaluator.evaluate(pred_test, {evaluator.metricName: "areaUnderROC"})

    # Single-pass confusion matrix for train
    train_counts = (
        pred_train.groupBy("label", "prediction")
        .agg(F.count("*").alias("cnt"))
        .collect()
    )
    train_count_dict = {(r["label"], r["prediction"]): r["cnt"] for r in train_counts}
    # Single-pass confusion matrix for test
    test_counts = (
        pred_test.groupBy("label", "prediction")
        .agg(F.count("*").alias("cnt"))
        .collect()
    )
    test_count_dict = {(r["label"], r["prediction"]): r["cnt"] for r in test_counts}

    # Summaries
    train_positive_true_tp = train_count_dict.get((1.0, 1.0), 0)
    train_positive_pred_tp = sum(v for (lbl, pred), v in train_count_dict.items() if pred == 1.0)
    train_neg_true_fp = train_count_dict.get((0.0, 1.0), 0)
    train_positive_pred_fp = train_count_dict.get((1.0, 0.0), 0)

    test_positive_true_tp = test_count_dict.get((1.0, 1.0), 0)
    test_positive_pred_tp = sum(v for (lbl, pred), v in test_count_dict.items() if pred == 1.0)
    test_neg_true_fp = test_count_dict.get((0.0, 1.0), 0)
    test_positive_pred_fp = test_count_dict.get((1.0, 0.0), 0)

    #----------------------------------------------------------------------------------
    # 5. Append this trial’s metrics as one row to a single CSV
    #----------------------------------------------------------------------------------
    # Build a small DataFrame with one row = current trial's metrics
    df_current_trial = pd.DataFrame([{
        "trial_number": trial.number,
        "maxDepth": max_depth,
        "minInstancesPerNode": min_instances_per_node,
        "stepSize": step_size,
        "maxBins": max_bins,
        "subsamplingRate": subsampling_rate,
        "maxIter": max_iter,
        "AUC_PR_Train": auc_pr_train,
        "AUC_ROC_Train": auc_roc_train,
        "AUC_PR_Test": auc_pr_test,
        "AUC_ROC_Test": auc_roc_test,
        "train_positive_true_tp": train_positive_true_tp,
        "train_positive_pred_tp": train_positive_pred_tp,
        "train_neg_true_fp": train_neg_true_fp,
        "train_positive_pred_fp": train_positive_pred_fp,
        "test_positive_true_tp": test_positive_true_tp,
        "test_positive_pred_tp": test_positive_pred_tp,
        "test_neg_true_fp": test_neg_true_fp,
        "test_positive_pred_fp": test_positive_pred_fp
    }])

    csv_path = "optuna_results.csv"
    # Check if CSV already exists
    file_exists = os.path.isfile(csv_path)
    # Write in append mode, and only write header if file doesn’t exist
    df_current_trial.to_csv(csv_path, mode="a", header=not file_exists, index=False)

    print(f"** Trial {trial.number} finished. AUC_ROC_Test={auc_roc_test} appended to {csv_path}. **")

    # Return the metric to maximize
    return auc_roc_test

#----------------------------------------------------------------------------------
# 6. Run Optuna search
#----------------------------------------------------------------------------------
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=5)

print("Best Parameter:")
print(study.best_params)

#----------------------------------------------------------------------------------
# 7. Train final model on best parameters (optional)
#----------------------------------------------------------------------------------
best_params = study.best_params
gbt_best = GBTClassifier(
    labelCol=target_column,
    featuresCol="features",
    predictionCol="prediction",
    maxDepth=best_params["maxDepth"],
    minInstancesPerNode=best_params["minInstancesPerNode"],
    stepSize=best_params["stepSize"],
    maxBins=best_params["maxBins"],
    subsamplingRate=best_params["subsamplingRate"],
    seed=22,
    maxIter=best_params["maxIter"]
)

best_model = gbt_best.fit(balance)
pred_train_best = best_model.transform(train_1)
pred_test_best = best_model.transform(test_1)

evaluator = BinaryClassificationEvaluator(labelCol=target_column)
best_model_metrics = {
    "AUC_PR_Train": evaluator.evaluate(pred_train_best, {evaluator.metricName: "areaUnderPR"}),
    "AUC_ROC_Train": evaluator.evaluate(pred_train_best, {evaluator.metricName: "areaUnderROC"}),
    "AUC_PR_Test": evaluator.evaluate(pred_test_best, {evaluator.metricName: "areaUnderPR"}),
    "AUC_ROC_Test": evaluator.evaluate(pred_test_best, {evaluator.metricName: "areaUnderROC"}),
}

print("Best Model Metrics:")
for k, v in best_model_metrics.items():
    print(f"{k} = {v}")

best_model.write().overwrite().save(
    f"hdfs://testdatalake-ns/tables/DM_FRAUD_DETECTION/INC_FRD/gbm_final_{best_params['maxDepth']}_{best_params['minInstancesPerNode']}_{best_params['stepSize']}_{best_params['maxBins']}_{best_params['subsamplingRate']}_{best_params['maxIter']}"
)





import numpy as np
import pandas as pd
from collections import defaultdict
from apply_hive_table import apply_hive
from modelconftranslator.Config import get_config
from modelconftranslator.DataIO import (set_data_config, write_hive_table_external)
from itertools import chain
from pyspark.ml import Pipeline
from pyspark.ml import PipelineModel
from pyspark.ml.tuning import (
    ParamGridBuilder,
    TrainValidationSplit,
)
from pyspark.ml.feature import (
    ChiSqSelector,
    Tokenizer,
    QuantileDiscretizer,
)
from pyspark.ml.classification import (
    RandomForestClassifier,
    RandomForestClassificationModel,
    GBTClassifier,
)
from pyspark.ml.linalg import (
    DenseVector,
    SparseVector,
    Vector,
    VectorUDT,
)
from pyspark.ml.feature import (
    VectorAssembler,
    StringIndexer,
    OneHotEncoder,
)
import inclearing as inclearing
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.mllib.stat import Statistics
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.functions import isnan, when, count, col, explode, udf
from pyspark.sql.types import *
from typing import Dict, Any
from inclearing.dev_data.feature import Features_NUM, Target, Identifier, Feat_NUM_50, Feat_NUM_50_RETRAIN

from optimal_spark_config.create_spark_instance import (
    generate_spark_instance,
)

import optuna

#def main():
spark = generate_spark_instance(total_memory=400, total_vcpu=200)

#=======================================#
DEV_trans = (
    spark.table("dm_fraud_detection.inc_frd_train_mtkpop_v5_imputed")
    .filter(col('ods_business_dt').between('2024-04-22', '2024-10-04'))
    .fillna(-999, subset=Feat_NUM_50_RETRAIN)
    .select(
        *Identifier, 
        *Feat_NUM_50_RETRAIN,
        *Target, 'train_set'
    )
)

target_column = "label"

assemblerNum = VectorAssembler(inputCols=Feat_NUM_50_RETRAIN, outputCol="features")
pipe_assem = Pipeline(stages=[assemblerNum])
DEV_1 = pipe_assem.fit(DEV_trans)
DEV_vec = DEV_1.transform(DEV_trans)

train_1 = DEV_vec.filter(col('train_set')==1)
test_1 = DEV_vec.filter(col('train_set')==0)

def handle_imbalanced_data(train, factor, target_column, imbalanced_value):
    import numpy as np
    from pyspark.sql.functions import udf, explode
    from pyspark.sql.types import FloatType, ArrayType

    num_rep = int(
        np.round(
            train.count()
            / train.filter(train[target_column] == imbalanced_value).count() * factor
        )
    )
    n_to_array = udf(
        lambda x: [0.0] * num_rep if x == 1.0 else [0.0], 
        ArrayType(FloatType())
    )
    train = train.withColumn("n", n_to_array(train[target_column]))
    train = train.withColumn("n", explode(train.n))
    return train.drop("n")

# Balancing the event-nonevent records in the training sample
balance = handle_imbalanced_data(train_1, 0.5, target_column, imbalanced_value=1)

# -- CHANGED: Cache data so Spark doesn't re-read or re-transform repeatedly.
balance = balance.cache()
balance.count()  # materialize the cache

train_1 = train_1.cache()
train_1.count()
test_1 = test_1.cache()
test_1.count()

# Storage dict for trial metrics
agg_scor_dict = {
    "AUC_PR_Train": [0.0],
    "AUC_PR_Test": [0.0],
    "AUC_ROC_Train": [0.0],
    "AUC_ROC_Test": [0.0],
    "maxDepth": [0.0],
    "min_instance": [0.0],
    "stepSize": [0.0],
    "maxBins": [0.0],
    "subSamplingRate":[0.0],
    # -- CHANGED: Add maxIter so we can store it each trial
    "maxIter": [0.0],  
    "train_positive_true_tp": [0.0],
    "train_positive_pred_tp": [0.0],
    "train_neg_true_fp": [0.0],
    "train_positive_pred_fp": [0.0],
    "test_positive_true_tp": [0.0],
    "test_positive_pred_tp": [0.0],
    "test_neg_true_fp": [0.0],
    "test_positive_pred_fp": [0.0]
}

param_cols = ['maxDepth', 'minInstancesPerNode', 'stepSize', 'maxBins', 'subsamplingRate', 'maxIter']
score_cols = ['AUC_PR_Train', 'AUC_PR_Test', 'AUC_ROC_Train', 'AUC_ROC_Test']

#=========================================================================================
# Objective function for Optuna
#=========================================================================================
def objective(trial):
    # Hyperparameter search space
    max_depth = trial.suggest_int("maxDepth", 3, 8)
    min_instances_per_node = trial.suggest_int("minInstancesPerNode", 10, 30)
    step_size = trial.suggest_loguniform("stepSize", 0.2, 0.4)
    max_bins = trial.suggest_categorical("maxBins", [64])
    subsampling_rate = trial.suggest_float("subsamplingRate", 0.8, 1.0)
    # -- CHANGED: Instead of an incremental loop, we treat maxIter as a single hyperparam
    max_iter = trial.suggest_int("maxIter", 50, 250, step=50)

    # -- CHANGED: Train exactly once with the chosen maxIter
    rf = GBTClassifier(
        labelCol="label",
        featuresCol="features",
        predictionCol="prediction",
        maxDepth=max_depth,
        minInstancesPerNode=min_instances_per_node,
        stepSize=step_size,
        maxBins=max_bins,
        subsamplingRate=subsampling_rate,
        seed=22,
        maxIter=max_iter
    )

    # Single training run
    rf_model = rf.fit(balance)

    # Predictions
    rf_pred_train = rf_model.transform(train_1).cache()
    rf_pred_train.count()  # materialize
    rf_pred_test = rf_model.transform(test_1).cache()
    rf_pred_test.count()   # materialize

    # Evaluators
    evaluator = BinaryClassificationEvaluator(labelCol=target_column)
    auc_pr_train = evaluator.evaluate(rf_pred_train, {evaluator.metricName: "areaUnderPR"})
    auc_roc_train = evaluator.evaluate(rf_pred_train, {evaluator.metricName: "areaUnderROC"})
    auc_pr_test = evaluator.evaluate(rf_pred_test, {evaluator.metricName: "areaUnderPR"})
    auc_roc_test = evaluator.evaluate(rf_pred_test, {evaluator.metricName: "areaUnderROC"})

    # -- CHANGED: Single-pass confusion matrix
    train_counts = (
        rf_pred_train.groupBy("label", "prediction")
        .agg(F.count("*").alias("cnt"))
        .collect()
    )
    train_count_dict = {(row["label"], row["prediction"]): row["cnt"] for row in train_counts}

    test_counts = (
        rf_pred_test.groupBy("label", "prediction")
        .agg(F.count("*").alias("cnt"))
        .collect()
    )
    test_count_dict = {(row["label"], row["prediction"]): row["cnt"] for row in test_counts}

    # Extract confusion matrix metrics
    # Train
    train_positive_true_tp = train_count_dict.get((1.0, 1.0), 0)
    train_positive_pred_tp = sum(v for (l, p), v in train_count_dict.items() if p == 1.0)
    train_neg_true_fp = train_count_dict.get((0.0, 1.0), 0)
    train_positive_pred_fp = train_count_dict.get((1.0, 0.0), 0)

    # Test
    test_positive_true_tp = test_count_dict.get((1.0, 1.0), 0)
    test_positive_pred_tp = sum(v for (l, p), v in test_count_dict.items() if p == 1.0)
    test_neg_true_fp = test_count_dict.get((0.0, 1.0), 0)
    test_positive_pred_fp = test_count_dict.get((1.0, 0.0), 0)

    # Store metrics in global dict
    agg_scor_dict["AUC_PR_Train"].append(auc_pr_train)
    agg_scor_dict["AUC_PR_Test"].append(auc_pr_test)
    agg_scor_dict["AUC_ROC_Train"].append(auc_roc_train)
    agg_scor_dict["AUC_ROC_Test"].append(auc_roc_test)
    agg_scor_dict["maxDepth"].append(max_depth)
    agg_scor_dict["min_instance"].append(min_instances_per_node)
    agg_scor_dict["stepSize"].append(step_size)
    agg_scor_dict["maxBins"].append(max_bins)
    agg_scor_dict["subSamplingRate"].append(subsampling_rate)
    agg_scor_dict["maxIter"].append(max_iter)  # -- CHANGED: store it
    agg_scor_dict["train_positive_true_tp"].append(train_positive_true_tp)
    agg_scor_dict["train_positive_pred_tp"].append(train_positive_pred_tp)
    agg_scor_dict["train_neg_true_fp"].append(train_neg_true_fp)
    agg_scor_dict["train_positive_pred_fp"].append(train_positive_pred_fp)
    agg_scor_dict["test_positive_true_tp"].append(test_positive_true_tp)
    agg_scor_dict["test_positive_pred_tp"].append(test_positive_pred_tp)
    agg_scor_dict["test_neg_true_fp"].append(test_neg_true_fp)
    agg_scor_dict["test_positive_pred_fp"].append(test_positive_pred_fp)

    # Print metrics
    print(f"Trial {trial.number} completed:")
    for key in agg_scor_dict.keys():
        print(f"{key}: {agg_scor_dict[key][-1]}")

    # Save partial CSV right after each trial
    df_scores = pd.DataFrame(agg_scor_dict)
    df_scores.to_csv(f"optuna_metrics_after_trial_{trial.number}.csv", index=False)
    print(f"Trial {trial.number} metrics saved to optuna_metrics_after_trial_{trial.number}.csv")

    return auc_roc_test  # We'll optimize for AUC_ROC on test set

#=========================================================================================
# Perform Bayesian Optimization
#=========================================================================================
study = optuna.create_study(direction="maximize")
n_trials=5
study.optimize(objective, n_trials=n_trials)

# Save all trial results to CSV
df_scores = pd.DataFrame(agg_scor_dict)
df_scores.to_csv(f"optuna_{n_trials}_trials_metrics.csv", index=False)
print("All trial metrics saved to optuna_{n_trials}_trials_metrics.csv")

print("Best Parameter:")
print(study.best_params)

# Train final model using best parameters
best_params = study.best_params
best_model = GBTClassifier(
    labelCol="label",
    featuresCol="features",
    predictionCol="prediction",
    maxDepth=best_params["maxDepth"],
    minInstancesPerNode=best_params["minInstancesPerNode"],
    stepSize=best_params["stepSize"],
    maxBins=best_params["maxBins"],
    subsamplingRate=best_params["subsamplingRate"],
    seed=22,
    maxIter=best_params["maxIter"]  # -- CHANGED: use best maxIter
)

rf_best_model = best_model.fit(balance)

# Evaluate best model
rf_pred_train = rf_best_model.transform(train_1)
rf_pred_test = rf_best_model.transform(test_1)

# Initialize evaluator
evaluator = BinaryClassificationEvaluator(labelCol=target_column)
best_model_metrics = {
    "AUC_PR_Train": evaluator.evaluate(rf_pred_train, {evaluator.metricName: "areaUnderPR"}),
    "AUC_ROC_Train": evaluator.evaluate(rf_pred_train, {evaluator.metricName: "areaUnderROC"}),
    "AUC_PR_Test": evaluator.evaluate(rf_pred_test, {evaluator.metricName: "areaUnderPR"}),
    "AUC_ROC_Test": evaluator.evaluate(rf_pred_test, {evaluator.metricName: "areaUnderROC"}),
}

print("Best Model Metrics:")
for key, value in best_model_metrics.items():
    print(f"{key}: {value}")

pd.DataFrame([best_model_metrics]).to_csv(f"{n_trials}_trials_best_model_metrics.csv", index=False)
print("Best model metrics saved to best_model_metrics.csv")

# Save model
rf_best_model.write().overwrite().save(
    f"""hdfs://testdatalake-ns/tables/DM_FRAUD_DETECTION/INC_FRD/gbm_retrain_v5_{
        best_params["maxDepth"]
    }_{
        best_params["minInstancesPerNode"]
    }_{
        best_params["stepSize"]
    }_{
        best_params["maxBins"]
    }_{
        best_params["subsamplingRate"]
    }_{
        best_params["maxIter"]
    }"""
)





