feature_categories:
  # 1) Exclude columns
  hh_id_in_wh: "exclude"
  business_date: "exclude"
  consumer_pwm: "exclude"

  # 2) Segment features
  f__emerging: "segment"
  b__affluent: "segment"
  c__high_net_worth: "segment"
  d__very_high_net_worth: "segment"

  # 3) AUM features
  aum_dollars: "aum"
  b__500k_to__2_mm: "aum"
  c__2mm_to__5mm: "aum"
  d__5mm_to__10mm: "aum"
  e__10mm_to__25mm: "aum"

  # 4) Product features (1/0 for has/doesn't have)
  checking: "product"
  commercial_line_of_credit: "product"
  commercial_term_loan: "product"
  credit_card: "product"
  heloan: "product"
  heloc: "product"
  money_market: "product"
  other: "product"
  prime_time_checking: "product"
  private_wealth_cd: "product"
  regions_wealth_account: "product"
  regular_cd: "product"
  sbl: "product"
  small_business_cd: "product"
  unsecured_line: "product"

  # 5) Numeric features (using 5%/95% percentiles for insight)
  bal_amt_sum: "numeric"
  bal_amt_mean: "numeric"
  bal_amt_max: "numeric"
  bal_amt_min: "numeric"
  bal_amt_std: "numeric"

  ledger_bal_amt_sum: "numeric"
  ledger_bal_amt_mean: "numeric"
  ledger_bal_amt_std: "numeric"
  ledger_bal_amt_max: "numeric"
  ledger_bal_amt_min: "numeric"

  transaction_in_sum_1_month: "numeric"
  transaction_in_max_1_month: "numeric"
  transaction_in_count: "numeric"
  transaction_out_sum_1_month: "numeric"
  transaction_out_max_1_month: "numeric"
  transaction_out_count: "numeric"

  arrangement_count: "numeric"
  product_diversity: "numeric"
  state_count: "numeric"
  balance_trend: "numeric"

  recent_opened_counts_30d: "numeric"
  recent_opened_counts_90d: "numeric"
  recent_opened_counts_180d: "numeric"
  recent_opened_counts_365d: "numeric"
  recent_opened_counts_730d: "numeric"

  recent_closed_counts_30d: "numeric"
  recent_closed_counts_90d: "numeric"
  recent_closed_counts_180d: "numeric"
  recent_closed_counts_365d: "numeric"
  recent_closed_counts_730d: "numeric"

  hh_longevity: "numeric"



# insight_pipeline.py
# Place this in: private_wealth_retention/consumer_prospecting/model/

import pyspark.sql.functions as F
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml import Pipeline
from pyspark.sql.types import Row


def train_cluster_insight_model(
    df,
    cluster_col,
    cluster_id,
    feature_cols,
    label_col="label",
    seed=42,
    num_trees=50,
    max_depth=5
):
    """
    For the given cluster_col == cluster_id, label that cluster's prospects as 1, others as 0,
    then train a "cluster vs rest" RandomForest to see which features matter most.

    Returns a list of (feature, importance) sorted descending.
    """
    # 1) Build the label: 1 if cluster_col == cluster_id, else 0
    df_labeled = df.withColumn(
        label_col,
        F.when(F.col(cluster_col) == cluster_id, 1.0).otherwise(0.0)
    )

    # 2) VectorAssembler
    assembler = VectorAssembler(
        inputCols=feature_cols,
        outputCol="features",
        handleInvalid="skip"
    )

    # 3) RandomForest
    rf = RandomForestClassifier(
        labelCol=label_col,
        featuresCol="features",
        numTrees=num_trees,
        maxDepth=max_depth,
        seed=seed
    )

    pipeline = Pipeline(stages=[assembler, rf])
    model = pipeline.fit(df_labeled)

    # 4) Extract feature importances
    rf_model = model.stages[-1]  # The fitted RandomForestClassificationModel
    importances_array = rf_model.featureImportances.toArray()
    feature_importance_pairs = list(zip(feature_cols, importances_array))
    feature_importance_pairs.sort(key=lambda x: x[1], reverse=True)

    return feature_importance_pairs


def compute_cluster_percentiles(df, cluster_id, cluster_col, feature_cols, p_low=0.05, p_high=0.95):
    """
    Computes the p_low and p_high percentiles for each feature in feature_cols,
    among rows with cluster_col == cluster_id.
    Returns a dict: { feature_name -> (p5_value, p95_value) }.
    """
    df_filtered = df.filter(F.col(cluster_col) == cluster_id)

    # Build exprs for each numeric feature
    agg_exprs = []
    for fcol in feature_cols:
        agg_exprs.append(
            F.expr(f"percentile_approx({fcol}, {p_low})").alias(f"{fcol}_p5")
        )
        agg_exprs.append(
            F.expr(f"percentile_approx({fcol}, {p_high})").alias(f"{fcol}_p95")
        )

    row_stats = df_filtered.agg(*agg_exprs).collect()[0]

    stats_dict = {}
    for fcol in feature_cols:
        p5_val = row_stats[f"{fcol}_p5"]
        p95_val = row_stats[f"{fcol}_p95"]
        if p5_val is None:
            p5_val = 0.0
        if p95_val is None:
            p95_val = 0.0
        stats_dict[fcol] = (float(p5_val), float(p95_val))

    return stats_dict


def build_feature_insight(feature_name, feature_value, category, p5=None, p95=None):
    """
    Returns a text snippet about a single feature, depending on its category.

    category can be: "exclude", "segment", "product", "aum", "numeric", etc.
    p5/p95 are the 5%/95% percentile if needed for numeric or aum features.
    """

    # 1) If "exclude", skip
    if category == "exclude":
        return ""

    # 2) segment
    if category == "segment":
        # If feature_value > 0.5 => "This household is b__affluent" or custom logic
        if feature_value > 0.5:
            if feature_name == "f__emerging":
                return "This household is emerging."
            elif feature_name == "b__affluent":
                return "This household is affluent."
            elif feature_name == "c__high_net_worth":
                return "This household is high net worth."
            elif feature_name == "d__very_high_net_worth":
                return "This household is very high net worth."
        # If 0 => skip
        return ""

    # 3) product
    if category == "product":
        if feature_value > 0.5:
            return f"This household has opened {feature_name} product."
        else:
            return f"This household has not opened {feature_name} product."

    # 4) aum
    if category == "aum":
        if p5 is not None and p95 is not None:
            return (f"The household has AUM of {feature_value:.2f}. "
                    f"Typical AUM in this cluster is between {p5:.2f} and {p95:.2f}.")
        else:
            return f"The household has AUM of {feature_value:.2f}."

    # 5) numeric
    if category == "numeric":
        if p5 is not None and p95 is not None:
            return (f"The household's {feature_name} is {feature_value:.2f}. "
                    f"Similar PWM households in this cluster have {feature_name} "
                    f"between {p5:.2f} and {p95:.2f}.")
        else:
            return f"The household's {feature_name} is {feature_value:.2f}."

    # 6) fallback
    return f"{feature_name} => {feature_value:.2f}"


def generate_insight_string(top_features, row_values, cluster_percentiles, feature_categories):
    """
    Builds a single insight string for one prospect row,
    based on top_features, a row_values dict, and the cluster's p5/p95 stats.

    top_features: list of (feature_name, importance) sorted descending
    row_values  : dict { feature -> prospect's numeric value }
    cluster_percentiles : dict { feature -> (p5_val, p95_val) }
    feature_categories : dict from config ( feature_name -> "segment"/"aum"/"product"/"numeric"/"exclude" )
    """

    lines = []
    for (feat, _imp) in top_features:
        cat = feature_categories.get(feat, "numeric")

        # if "exclude", skip
        if cat == "exclude":
            continue

        val = row_values.get(feat, 0.0) or 0.0
        p5, p95 = cluster_percentiles.get(feat, (None, None))

        snippet = build_feature_insight(feat, val, cat, p5, p95)
        snippet = snippet.strip()
        if snippet:
            lines.append(snippet)

    # combine them
    return " | ".join(lines)




#!/usr/bin/env python
# run_insight.py

import sys
import pyspark.sql.functions as F
from pyspark.sql.types import StringType, StructType
from pyspark.sql import SparkSession

from optimal_spark_config.create_spark_instance import generate_spark_instance
from HiveIO.config import get_config, get_schema
from HiveIO.io import set_data_config

# Import from your new insight_pipeline.py
from private_wealth_retention.consumer_prospecting.model.insight_pipeline import (
    train_cluster_insight_model,
    compute_cluster_percentiles,
    generate_insight_string
)

def main():
    spark = generate_spark_instance(total_memory=600, total_vcpu=300)

    # 1) Load config & schema
    # adjust references if needed
    cfg = get_config(None, "config.yaml")
    schema = get_schema(None, "schema.yml")
    db_name = schema["databases"][0]["name"]
    business_date = cfg["dates"]["business_date"]

    # For example, we assume the config has "feature_categories" nested like:
    # cfg["feature_categories"] = { ... } 
    feature_categories = cfg["feature_categories"]

    # 2) Table references
    prospects_table = f"{db_name}.prospects_all"
    pwm_table = f"{db_name}.pwm_clusters_all"
    features_table = cfg["tables"]["prospecting_features"]

    # 3) Load the data
    load_data_from, get_table_partitions, _, _ = set_data_config(spark, cfg)

    df_prospects_base = load_data_from(prospects_table)
    df_pwm_base = load_data_from(pwm_table)
    # big features table
    df_features = load_data_from(features_table).filter(F.col("business_date") == business_date)

    # 4) Identify numeric feature columns
    # For example, you can gather them from config or discover them in the feature table
    # Here, let's assume you have a list:
    all_features = list(cfg["features"]["all_numeric_cols"])  # or something similar

    # Join prospects & PWM minimal data with the actual feature data
    df_prospects = df_prospects_base.join(
        df_features.select("hh_id_in_wh", *all_features),
        on="hh_id_in_wh",
        how="left"
    )
    df_pwm = df_pwm_base.join(
        df_features.select("hh_id_in_wh", *all_features),
        on="hh_id_in_wh",
        how="left"
    )

    # 5) Gather distinct cluster_ids
    cluster_ids = [
        row["cluster_id"] for row in df_prospects.select("cluster_id").distinct().collect()
    ]

    union_df = None

    # 6) For each cluster, train "cluster vs. rest," compute p5/p95 among PWM, build insights
    for c_id in cluster_ids:
        # a) train cluster-insight model
        feature_importances = train_cluster_insight_model(
            df=df_prospects,
            cluster_col="cluster_id",
            cluster_id=c_id,
            feature_cols=all_features,
            label_col="label",
            seed=42,
            num_trees=50,
            max_depth=5
        )
        # pick top 5
        top_5_features = feature_importances[:5]

        # b) compute 5% / 95% among the PWM for cluster c_id
        cluster_percentiles = compute_cluster_percentiles(
            df=df_pwm,
            cluster_id=c_id,
            cluster_col="cluster_id",
            feature_cols=[feat for (feat, imp) in top_5_features]
        )

        # c) filter this cluster's prospects
        df_cluster = df_prospects.filter(F.col("cluster_id") == c_id)

        # d) Collect them to driver, build "insight" string locally
        local_rows = df_cluster.collect()
        new_data = []
        for row in local_rows:
            row_dict = row.asDict()
            # build a "row_values" dict: {feature -> value}
            row_values = {}
            for feat in all_features:
                row_values[feat] = row_dict.get(feat, 0.0) or 0.0

            # generate the insight
            # generate_insight_string needs top_features=(feat, imp) and cluster_percentiles
            insight_text = generate_insight_string(
                top_features=top_5_features,
                row_values=row_values,
                cluster_percentiles=cluster_percentiles,
                feature_categories=feature_categories
            )

            row_dict["insight"] = insight_text
            new_data.append(row_dict)

        # e) Build a new Spark DF with the new "insight" column
        new_schema = df_cluster.schema.add("insight", StringType())
        df_cluster_with_insight = spark.createDataFrame(new_data, schema=new_schema)

        # f) Union
        if union_df is None:
            union_df = df_cluster_with_insight
        else:
            union_df = union_df.union(df_cluster_with_insight)

    # 7) final columns
    final_cols = [
        "hh_id_in_wh",
        "similarity_score",
        "distance_to_center",
        "prospect_segment",
        "cluster_id",
        "insight",
        "business_date"
    ]
    # Adjust if your actual columns differ
    df_insight = union_df.select(*[F.col(c) for c in final_cols if c in union_df.columns])

    # 8) Write final or do next steps
    # e.g.
    # df_insight.write.saveAsTable(f"{db_name}.prospect_insights", mode="overwrite")

    spark.stop()

if __name__ == "__main__":
    main()
