feature_categories:
  # 1) Exclude columns
  hh_id_in_wh: "exclude"
  business_date: "exclude"
  consumer_pwm: "exclude"

  # 2) Segment features
  f__emerging: "segment"
  b__affluent: "segment"
  c__high_net_worth: "segment"
  d__very_high_net_worth: "segment"

  # 3) AUM features
  aum_dollars: "aum"
  b__500k_to__2_mm: "aum"
  c__2mm_to__5mm: "aum"
  d__5mm_to__10mm: "aum"
  e__10mm_to__25mm: "aum"

  # 4) Product features (1/0 for has/doesn't have)
  checking: "product"
  commercial_line_of_credit: "product"
  commercial_term_loan: "product"
  credit_card: "product"
  heloan: "product"
  heloc: "product"
  money_market: "product"
  other: "product"
  prime_time_checking: "product"
  private_wealth_cd: "product"
  regions_wealth_account: "product"
  regular_cd: "product"
  sbl: "product"
  small_business_cd: "product"
  unsecured_line: "product"

  # 5) Numeric features (using 5%/95% percentiles for insight)
  bal_amt_sum: "numeric"
  bal_amt_mean: "numeric"
  bal_amt_max: "numeric"
  bal_amt_min: "numeric"
  bal_amt_std: "numeric"

  ledger_bal_amt_sum: "numeric"
  ledger_bal_amt_mean: "numeric"
  ledger_bal_amt_std: "numeric"
  ledger_bal_amt_max: "numeric"
  ledger_bal_amt_min: "numeric"

  transaction_in_sum_1_month: "numeric"
  transaction_in_max_1_month: "numeric"
  transaction_in_count: "numeric"
  transaction_out_sum_1_month: "numeric"
  transaction_out_max_1_month: "numeric"
  transaction_out_count: "numeric"

  arrangement_count: "numeric"
  product_diversity: "numeric"
  state_count: "numeric"
  balance_trend: "numeric"

  recent_opened_counts_30d: "numeric"
  recent_opened_counts_90d: "numeric"
  recent_opened_counts_180d: "numeric"
  recent_opened_counts_365d: "numeric"
  recent_opened_counts_730d: "numeric"

  recent_closed_counts_30d: "numeric"
  recent_closed_counts_90d: "numeric"
  recent_closed_counts_180d: "numeric"
  recent_closed_counts_365d: "numeric"
  recent_closed_counts_730d: "numeric"

  hh_longevity: "numeric"



# insight_pipeline.py
# Place in: private_wealth_retention/consumer_prospecting/model/

import pyspark.sql.functions as F
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml import Pipeline


def train_cluster_insight_model(
    df,
    cluster_col,
    cluster_id,
    feature_cols,
    label_col="label",
    seed=42,
    num_trees=50,
    max_depth=5
):
    """
    For cluster_col == cluster_id, label those rows as 1, others as 0,
    then train a 'cluster vs rest' RandomForest.
    Returns a sorted list of (feature, importance).
    """
    df_labeled = df.withColumn(
        label_col,
        F.when(F.col(cluster_col) == cluster_id, 1.0).otherwise(0.0)
    )

    assembler = VectorAssembler(
        inputCols=feature_cols,
        outputCol="features",
        handleInvalid="skip"
    )
    rf = RandomForestClassifier(
        labelCol=label_col,
        featuresCol="features",
        numTrees=num_trees,
        maxDepth=max_depth,
        seed=seed
    )

    pipeline = Pipeline(stages=[assembler, rf])
    model = pipeline.fit(df_labeled)

    rf_model = model.stages[-1]  # RandomForestClassificationModel
    importances = rf_model.featureImportances.toArray()
    feature_importance_pairs = list(zip(feature_cols, importances))
    feature_importance_pairs.sort(key=lambda x: x[1], reverse=True)

    return feature_importance_pairs


def compute_cluster_percentiles(
    df,
    cluster_id,
    cluster_col,
    feature_cols,
    p_low=0.05,
    p_high=0.95
):
    """
    Among rows with cluster_col == cluster_id, compute p_low and p_high percentiles
    for each feature in feature_cols. Returns { feature -> (p5_val, p95_val) }.
    """
    df_filtered = df.filter(F.col(cluster_col) == cluster_id)

    agg_exprs = []
    for fcol in feature_cols:
        agg_exprs.append(
            F.expr(f"percentile_approx({fcol}, {p_low})").alias(f"{fcol}_p5")
        )
        agg_exprs.append(
            F.expr(f"percentile_approx({fcol}, {p_high})").alias(f"{fcol}_p95")
        )

    row_stats = df_filtered.agg(*agg_exprs).collect()[0]

    stats_dict = {}
    for fcol in feature_cols:
        p5_val = row_stats[f"{fcol}_p5"]
        p95_val = row_stats[f"{fcol}_p95"]
        if p5_val is None:
            p5_val = 0.0
        if p95_val is None:
            p95_val = 0.0
        stats_dict[fcol] = (float(p5_val), float(p95_val))

    return stats_dict


def build_feature_insight(
    feature_name,
    feature_value,
    category,
    row_values,
    cluster_percentiles,
    aum_snippet_done=False
):
    """
    Returns a text snippet about a single feature, depending on its category.
    If category == 'aum', ALWAYS reference 'aum_dollars' from row_values 
    and p5/p95 from cluster_percentiles['aum_dollars']. 
    Also adds dollar sign with thousands separators.

    'aum_snippet_done': pass True if we already created an AUM snippet, so we skip further AUM text.
    """

    # 1) If exclude, skip
    if category == "exclude":
        return ""

    # 2) Segment
    if category == "segment":
        if feature_value > 0.5:
            if feature_name == "f__emerging":
                return "This household is emerging."
            elif feature_name == "b__affluent":
                return "This household is affluent."
            elif feature_name == "c__high_net_worth":
                return "This household is high net worth."
            elif feature_name == "d__very_high_net_worth":
                return "This household is very high net worth."
        return ""

    # 3) Product
    if category == "product":
        if feature_value > 0.5:
            return f"This household has opened {feature_name} product."
        else:
            return f"This household has not opened {feature_name} product."

    # 4) AUM => always reference aum_dollars, ignoring current feature_name
    if category == "aum":
        # If we already created an AUM snippet, we skip or produce empty
        if aum_snippet_done:
            return ""  # skip duplicates
        aum_val = row_values.get("aum_dollars", 0.0)
        p5, p95 = cluster_percentiles.get("aum_dollars", (None, None))
        if p5 is not None and p95 is not None:
            return (
                f"The household has AUM of ${aum_val:,.2f}. "
                f"Typical AUM in this cluster is between ${p5:,.2f} and ${p95:,.2f}."
            )
        else:
            return f"The household has AUM of ${aum_val:,.2f}."

    # 5) numeric
    if category == "numeric":
        p5, p95 = cluster_percentiles.get(feature_name, (None, None))
        if p5 is not None and p95 is not None:
            return (
                f"The household's {feature_name} is ${feature_value:,.2f}. "
                f"Similar PWM households in this cluster have {feature_name} "
                f"between ${p5:,.2f} and ${p95:,.2f}."
            )
        else:
            return f"The household's {feature_name} is ${feature_value:,.2f}."

    # fallback
    return f"{feature_name} => ${feature_value:,.2f}"


def generate_insight_string(
    top_features,
    row_values,
    cluster_percentiles,
    feature_categories
):
    """
    Builds a single insight string for one prospect row,
    referencing 'aum_dollars' if the category is 'aum'. 
    We skip repeated AUM snippets if multiple aum features appear in top_features.
    """
    lines = []
    aum_snippet_done = False

    for (feat, _imp) in top_features:
        cat = feature_categories.get(feat, "numeric")
        val = row_values.get(feat, 0.0) or 0.0

        snippet = build_feature_insight(
            feature_name=feat,
            feature_value=val,
            category=cat,
            row_values=row_values,
            cluster_percentiles=cluster_percentiles,
            aum_snippet_done=aum_snippet_done
        )
        snippet = snippet.strip()
        if snippet:
            lines.append(snippet)

        # If we just produced an aum snippet, mark aum_snippet_done=True
        if cat == "aum" and snippet != "":
            aum_snippet_done = True

    return " | ".join(lines)





#!/usr/bin/env python
# run_insight.py

import sys
import pyspark.sql.functions as F
from pyspark.sql.types import StringType
from pyspark.sql import SparkSession

from optimal_spark_config.create_spark_instance import generate_spark_instance
from HiveIO.config import get_config, get_schema
from HiveIO.io import set_data_config

# Import from your new insight_pipeline
from private_wealth_retention.consumer_prospecting.model.insight_pipeline import (
    train_cluster_insight_model,
    compute_cluster_percentiles,
    generate_insight_string
)

def main():
    spark = generate_spark_instance(total_memory=600, total_vcpu=300)

    # 1) Load config, schema, etc.
    cfg = get_config(None, "config.yaml")
    schema = get_schema(None, "schema.yml")
    db_name = schema["databases"][0]["name"]
    business_date = cfg["dates"]["business_date"]

    feature_categories = cfg["feature_categories"]  # the big mapping from your config
    all_features = cfg["features"]["all_numeric_cols"]  # e.g. or however you list your features

    # 2) Table references
    prospects_table = f"{db_name}.prospects_all"
    pwm_table = f"{db_name}.pwm_clusters_all"
    features_table = cfg["tables"]["prospecting_features"]

    # 3) Load data
    load_data_from, get_table_partitions, _, _ = set_data_config(spark, cfg)
    df_prospects_base = load_data_from(prospects_table)
    df_pwm_base = load_data_from(pwm_table)
    df_features = load_data_from(features_table).filter(F.col("business_date") == business_date)

    # 4) Join with actual numeric columns
    df_prospects = df_prospects_base.join(
        df_features.select("hh_id_in_wh", *all_features),
        on="hh_id_in_wh",
        how="left"
    )
    df_pwm = df_pwm_base.join(
        df_features.select("hh_id_in_wh", *all_features),
        on="hh_id_in_wh",
        how="left"
    )

    # 5) Distinct cluster_ids
    cluster_ids = [
        row["cluster_id"]
        for row in df_prospects.select("cluster_id").distinct().collect()
    ]

    union_df = None

    # 6) For each cluster
    for c_id in cluster_ids:
        # a) train cluster-insight model => pick top features
        feature_importances = train_cluster_insight_model(
            df=df_prospects,
            cluster_col="cluster_id",
            cluster_id=c_id,
            feature_cols=all_features,
            label_col="label",
            seed=42,
            num_trees=50,
            max_depth=5
        )
        top_5_features = feature_importances[:5]

        # b) compute 5–95% among PWM
        #    We only need p5/p95 for the features in top_5, *plus* "aum_dollars" if it's not there
        #    because we might always need cluster_percentiles["aum_dollars"] if any AUM feature appears
        needed_features = set([f for (f, imp) in top_5_features])
        needed_features.add("aum_dollars")  # ensure we have it for AUM references
        needed_features = list(needed_features)

        cluster_percentiles = compute_cluster_percentiles(
            df=df_pwm,
            cluster_id=c_id,
            cluster_col="cluster_id",
            feature_cols=needed_features
        )

        # c) filter prospects in this cluster
        df_cluster = df_prospects.filter(F.col("cluster_id") == c_id)

        # d) Collect them & build insight
        local_rows = df_cluster.collect()
        new_data = []
        for row in local_rows:
            row_dict = row.asDict()
            # build row_values
            row_values = {}
            for feat in all_features:
                row_values[feat] = row_dict.get(feat, 0.0) or 0.0

            insight_str = generate_insight_string(
                top_features=top_5_features,
                row_values=row_values,
                cluster_percentiles=cluster_percentiles,
                feature_categories=feature_categories
            )
            row_dict["insight"] = insight_str
            new_data.append(row_dict)

        # e) Convert back to Spark DF
        new_schema = df_cluster.schema.add("insight", StringType())
        df_cluster_with_insight = spark.createDataFrame(new_data, schema=new_schema)

        # f) Union
        if union_df is None:
            union_df = df_cluster_with_insight
        else:
            union_df = union_df.union(df_cluster_with_insight)

    # 7) final columns
    final_cols = [
        "hh_id_in_wh",
        "similarity_score",
        "distance_to_center",
        "prospect_segment",
        "cluster_id",
        "insight",
        "business_date"
    ]
    # only select columns that exist
    existing_cols = [c for c in final_cols if c in union_df.columns]
    df_insight = union_df.select(*[F.col(c) for c in existing_cols])

    # 8) write final
    # df_insight.write.saveAsTable(f"{db_name}.prospect_insights", mode="overwrite")

    spark.stop()

if __name__ == "__main__":
    main()
