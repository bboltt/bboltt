feature_human_labels = {
    # --- Excluded columns (no human label needed if you skip them) ---
    # "hh_id_in_wh": "exclude",
    # "business_date": "exclude",
    # "consumer_pwm": "exclude",

    # --- Segment Features ---
    "f__emerging": "the emerging wealth segment",
    "b__affluent": "the affluent segment",
    "c__high_net_worth": "the high net worth segment",
    "d__very_high_net_worth": "the very high net worth segment",

    # --- AUM / Wealth Tier Features ---
    "aum_dollars": "AUM dollars",
    "b__500k_to__2_mm": "the $500k–$2mm wealth tier",
    "c__2mm_to__5mm": "the $2mm–$5mm wealth tier",
    "d__5mm_to__10mm": "the $5mm–$10mm wealth tier",
    "e__10mm_to__25mm": "the $10mm–$25mm wealth tier",
    "f__emerging": "the emerging wealth tier",
    "b__affluent": "the affluent tier",
    "c__high_net_worth": "the high net worth tier",
    "d__very_high_net_worth": "the very high net worth tier",

    # --- Product Features ---
    "checking": "a checking product",
    "commercial_line_of_credit": "a commercial line of credit",
    "commercial_term_loan": "a commercial term loan",
    "credit_card": "a credit card",
    "heloan": "a home equity loan",
    "heloc": "a home equity line of credit",
    "money_market": "a money market account",
    "other": "another product type",
    "prime_time_checking": "a prime time checking product",
    "private_wealth_cd": "a private wealth CD product",
    "regions_wealth_account": "a Regions wealth account",
    "regular_cd": "a regular CD product",
    "sbl": "a small business loan (SBL)",
    "small_business_cd": "a small business CD",
    "unsecured_line": "an unsecured line of credit",

    # --- Numeric Balance / Ledger Features ---
    "bal_amt_sum": "the total balance across all accounts",
    "bal_amt_mean": "the average balance across all accounts",
    "bal_amt_max": "the maximum balance across all accounts",
    "bal_amt_min": "the minimum balance across all accounts",
    "bal_amt_std": "the standard deviation of balances",

    "ledger_bal_amt_sum": "the total ledger balance across all accounts",
    "ledger_bal_amt_mean": "the average ledger balance across all accounts",
    "ledger_bal_amt_std": "the standard deviation of ledger balances",
    "ledger_bal_amt_max": "the maximum ledger balance across all accounts",
    "ledger_bal_amt_min": "the minimum ledger balance across all accounts",

    # --- Transaction Features ---
    "transaction_in_sum_1_month": "the total inbound transactions in the past month",
    "transaction_in_max_1_month": "the maximum inbound transaction in the past month",
    "transaction_in_count": "the count of inbound transactions",
    "transaction_out_sum_1_month": "the total outbound transactions in the past month",
    "transaction_out_max_1_month": "the maximum outbound transaction in the past month",
    "transaction_out_count": "the count of outbound transactions",

    # --- Arrangement / Diversity / Count Features ---
    "arrangement_count": "the arrangement count",
    "product_diversity": "the product diversity score",
    "state_count": "the number of states associated with this household",
    "balance_trend": "the balance change over the previous month",

    # --- Recent Opened / Closed Features ---
    "recent_opened_counts_30d": "the number of accounts opened in the last 30 days",
    "recent_opened_counts_90d": "the number of accounts opened in the last 90 days",
    "recent_opened_counts_180d": "the number of accounts opened in the last 180 days",
    "recent_opened_counts_365d": "the number of accounts opened in the last 365 days",
    "recent_opened_counts_730d": "the number of accounts opened in the last 730 days",

    "recent_closed_counts_30d": "the number of accounts closed in the last 30 days",
    "recent_closed_counts_90d": "the number of accounts closed in the last 90 days",
    "recent_closed_counts_180d": "the number of accounts closed in the last 180 days",
    "recent_closed_counts_365d": "the number of accounts closed in the last 365 days",
    "recent_closed_counts_730d": "the number of accounts closed in the last 730 days",

    # --- Other / Longevity ---
    "hh_longevity": "the household longevity in days"
}


# insight_pipeline.py

import pyspark.sql.functions as F
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml import Pipeline

def train_cluster_insight_model(
    df,
    cluster_col,
    cluster_id,
    feature_cols,
    label_col="label",
    seed=42,
    num_trees=50,
    max_depth=5
):
    """
    Train a 'cluster vs. rest' RandomForest to find which features matter 
    for cluster_id vs. other clusters.
    Returns a sorted list of (feature, importance).
    """
    df_labeled = df.withColumn(
        label_col,
        F.when(F.col(cluster_col) == cluster_id, 1.0).otherwise(0.0)
    )

    assembler = VectorAssembler(
        inputCols=feature_cols,
        outputCol="features",
        handleInvalid="skip"
    )
    rf = RandomForestClassifier(
        labelCol=label_col,
        featuresCol="features",
        numTrees=num_trees,
        maxDepth=max_depth,
        seed=seed
    )

    pipeline = Pipeline(stages=[assembler, rf])
    model = pipeline.fit(df_labeled)

    rf_model = model.stages[-1]  # random forest model
    importances = rf_model.featureImportances.toArray()
    feature_importance_pairs = list(zip(feature_cols, importances))
    feature_importance_pairs.sort(key=lambda x: x[1], reverse=True)

    return feature_importance_pairs

def generate_insight_string_simple(top_features, feature_human_labels):
    """
    Instead of numeric values or ranges, produce a short sentence 
    listing the 'human-friendly' labels for each top feature.

    e.g. "The household is selected because of AUM dollars, 
          the balance change over the previous month, 
          and the sum of balances in all accounts."
    """
    # Only keep the feature names that actually have > 0 importance
    # (some might be zero if random forest doesn't use them)
    relevant_features = [f for (f, imp) in top_features if imp > 0]

    # Convert each feature name to a human-friendly phrase
    labels = []
    for feat in relevant_features:
        if feat in feature_human_labels:
            labels.append(feature_human_labels[feat])
        else:
            # fallback to raw name
            labels.append(feat.replace("_", " "))

    if not labels:
        return "The household is selected, but no significant features were found."

    # Build a single sentence with commas + "and"
    # e.g. "A, B, and C" 
    if len(labels) == 1:
        label_str = labels[0]
    elif len(labels) == 2:
        label_str = f"{labels[0]} and {labels[1]}"
    else:
        # multiple => separate by commas, last item with "and"
        label_str = ", ".join(labels[:-1]) + ", and " + labels[-1]

    return f"The household is selected because of {label_str}."

#!/usr/bin/env python
# run_insight.py

import sys
import pyspark.sql.functions as F
from pyspark.sql.types import StringType
from pyspark.sql import SparkSession

from optimal_spark_config.create_spark_instance import generate_spark_instance
from HiveIO.config import get_config, get_schema
from HiveIO.io import set_data_config

# Import your new pipeline
from private_wealth_retention.consumer_prospecting.model.insight_pipeline import (
    train_cluster_insight_model,
    generate_insight_string_simple
)

def main():
    spark = generate_spark_instance(total_memory=600, total_vcpu=300)

    cfg = get_config(None, "config.yaml")
    schema = get_schema(None, "schema.yml")
    db_name = schema["databases"][0]["name"]
    business_date = cfg["dates"]["business_date"]

    # Suppose we have a dictionary of human-friendly labels.
    # You can store this in config.yaml or define it here:
    feature_human_labels = {
        "aum_dollars": "AUM dollars",
        "bal_amt_sum": "the sum of balance in all accounts",
        "bal_amt_mean": "the average balance of all accounts",
        "bal_amt_max": "the maximum balance across all accounts",
        "balance_trend": "the balance change over the previous month",
        "b__affluent": "its affluent segment",
        "c__high_net_worth": "its high net worth segment",
        # ...
    }

    # If config has them, do e.g. feature_human_labels = cfg["feature_human_labels"]

    all_features = cfg["features"]["all_numeric_cols"]  # or wherever you store them

    # 2) Table references
    prospects_table = f"{db_name}.prospects_all"
    pwm_table = f"{db_name}.pwm_clusters_all"  # might not even need PWM now if we skip stats
    features_table = cfg["tables"]["prospecting_features"]

    # 3) Load data
    load_data_from, get_table_partitions, _, _ = set_data_config(spark, cfg)
    df_prospects_base = load_data_from(prospects_table)
    # If you no longer need PWM data for stats, skip it or keep if you do something else.
    # df_pwm_base = load_data_from(pwm_table)
    df_features = load_data_from(features_table).filter(F.col("business_date") == business_date)

    # 4) Join with numeric columns
    df_prospects = df_prospects_base.join(
        df_features.select("hh_id_in_wh", *all_features),
        on="hh_id_in_wh",
        how="left"
    )

    # 5) distinct cluster_ids
    cluster_ids = [
        row["cluster_id"]
        for row in df_prospects.select("cluster_id").distinct().collect()
    ]

    union_df = None

    for c_id in cluster_ids:
        # a) train cluster-insight model => pick top features
        feature_importances = train_cluster_insight_model(
            df=df_prospects,
            cluster_col="cluster_id",
            cluster_id=c_id,
            feature_cols=all_features,
            label_col="label",
            seed=42,
            num_trees=50,
            max_depth=5
        )
        top_5_features = feature_importances[:5]

        # b) filter prospects in this cluster
        df_cluster = df_prospects.filter(F.col("cluster_id") == c_id)

        # c) collect => build insight
        local_rows = df_cluster.collect()
        new_data = []
        for row in local_rows:
            row_dict = row.asDict()

            # no need for row_values or stats => we only produce a sentence referencing top features
            insight_str = generate_insight_string_simple(
                top_5_features,
                feature_human_labels
            )
            row_dict["insight"] = insight_str
            new_data.append(row_dict)

        # d) create new DF with insight col
        new_schema = df_cluster.schema.add("insight", StringType())
        df_cluster_with_insight = spark.createDataFrame(new_data, schema=new_schema)

        if union_df is None:
            union_df = df_cluster_with_insight
        else:
            union_df = union_df.union(df_cluster_with_insight)

    # final columns
    final_cols = [
        "hh_id_in_wh",
        "similarity_score",
        "distance_to_center",
        "prospect_segment",
        "cluster_id",
        "insight",
        "business_date"
    ]
    existing_cols = [c for c in final_cols if c in union_df.columns]
    df_insight = union_df.select(*existing_cols)

    # write final
    # df_insight.write.saveAsTable(f"{db_name}.prospect_insights", mode="overwrite")

    spark.stop()

if __name__ == "__main__":
    main()


# insight_pipeline.py
# Place in: private_wealth_retention/consumer_prospecting/model/

import pyspark.sql.functions as F
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml import Pipeline


def train_cluster_insight_model(
    df,
    cluster_col,
    cluster_id,
    feature_cols,
    label_col="label",
    seed=42,
    num_trees=50,
    max_depth=5
):
    """
    For cluster_col == cluster_id, label those rows as 1, others as 0,
    then train a 'cluster vs rest' RandomForest.
    Returns a sorted list of (feature, importance).
    """
    df_labeled = df.withColumn(
        label_col,
        F.when(F.col(cluster_col) == cluster_id, 1.0).otherwise(0.0)
    )

    assembler = VectorAssembler(
        inputCols=feature_cols,
        outputCol="features",
        handleInvalid="skip"
    )
    rf = RandomForestClassifier(
        labelCol=label_col,
        featuresCol="features",
        numTrees=num_trees,
        maxDepth=max_depth,
        seed=seed
    )

    pipeline = Pipeline(stages=[assembler, rf])
    model = pipeline.fit(df_labeled)

    rf_model = model.stages[-1]  # RandomForestClassificationModel
    importances = rf_model.featureImportances.toArray()
    feature_importance_pairs = list(zip(feature_cols, importances))
    feature_importance_pairs.sort(key=lambda x: x[1], reverse=True)

    return feature_importance_pairs


def compute_cluster_percentiles(
    df,
    cluster_id,
    cluster_col,
    feature_cols,
    p_low=0.05,
    p_high=0.95
):
    """
    Among rows with cluster_col == cluster_id, compute p_low and p_high percentiles
    for each feature in feature_cols. Returns { feature -> (p5_val, p95_val) }.
    """
    df_filtered = df.filter(F.col(cluster_col) == cluster_id)

    agg_exprs = []
    for fcol in feature_cols:
        agg_exprs.append(
            F.expr(f"percentile_approx({fcol}, {p_low})").alias(f"{fcol}_p5")
        )
        agg_exprs.append(
            F.expr(f"percentile_approx({fcol}, {p_high})").alias(f"{fcol}_p95")
        )

    row_stats = df_filtered.agg(*agg_exprs).collect()[0]

    stats_dict = {}
    for fcol in feature_cols:
        p5_val = row_stats[f"{fcol}_p5"]
        p95_val = row_stats[f"{fcol}_p95"]
        if p5_val is None:
            p5_val = 0.0
        if p95_val is None:
            p95_val = 0.0
        stats_dict[fcol] = (float(p5_val), float(p95_val))

    return stats_dict


def build_feature_insight(
    feature_name,
    feature_value,
    category,
    row_values,
    cluster_percentiles,
    aum_snippet_done=False
):
    """
    Returns a text snippet about a single feature, depending on its category.
    If category == 'aum', ALWAYS reference 'aum_dollars' from row_values 
    and p5/p95 from cluster_percentiles['aum_dollars']. 
    Also adds dollar sign with thousands separators.

    'aum_snippet_done': pass True if we already created an AUM snippet, so we skip further AUM text.
    """

    # 1) If exclude, skip
    if category == "exclude":
        return ""

    # 2) Segment
    if category == "segment":
        if feature_value > 0.5:
            if feature_name == "f__emerging":
                return "This household is emerging."
            elif feature_name == "b__affluent":
                return "This household is affluent."
            elif feature_name == "c__high_net_worth":
                return "This household is high net worth."
            elif feature_name == "d__very_high_net_worth":
                return "This household is very high net worth."
        return ""

    # 3) Product
    if category == "product":
        if feature_value > 0.5:
            return f"This household has opened {feature_name} product."
        else:
            return f"This household has not opened {feature_name} product."

    # 4) AUM => always reference aum_dollars, ignoring current feature_name
    if category == "aum":
        # If we already created an AUM snippet, we skip or produce empty
        if aum_snippet_done:
            return ""  # skip duplicates
        aum_val = row_values.get("aum_dollars", 0.0)
        p5, p95 = cluster_percentiles.get("aum_dollars", (None, None))
        if p5 is not None and p95 is not None:
            return (
                f"The household has AUM of ${aum_val:,.2f}. "
                f"Typical AUM in this cluster is between ${p5:,.2f} and ${p95:,.2f}."
            )
        else:
            return f"The household has AUM of ${aum_val:,.2f}."

    # 5) numeric
    if category == "numeric":
        p5, p95 = cluster_percentiles.get(feature_name, (None, None))
        if p5 is not None and p95 is not None:
            return (
                f"The household's {feature_name} is ${feature_value:,.2f}. "
                f"Similar PWM households in this cluster have {feature_name} "
                f"between ${p5:,.2f} and ${p95:,.2f}."
            )
        else:
            return f"The household's {feature_name} is ${feature_value:,.2f}."

    # fallback
    return f"{feature_name} => ${feature_value:,.2f}"


def generate_insight_string(
    top_features,
    row_values,
    cluster_percentiles,
    feature_categories
):
    """
    Builds a single insight string for one prospect row,
    referencing 'aum_dollars' if the category is 'aum'. 
    We skip repeated AUM snippets if multiple aum features appear in top_features.
    """
    lines = []
    aum_snippet_done = False

    for (feat, _imp) in top_features:
        cat = feature_categories.get(feat, "numeric")
        val = row_values.get(feat, 0.0) or 0.0

        snippet = build_feature_insight(
            feature_name=feat,
            feature_value=val,
            category=cat,
            row_values=row_values,
            cluster_percentiles=cluster_percentiles,
            aum_snippet_done=aum_snippet_done
        )
        snippet = snippet.strip()
        if snippet:
            lines.append(snippet)

        # If we just produced an aum snippet, mark aum_snippet_done=True
        if cat == "aum" and snippet != "":
            aum_snippet_done = True

    return " | ".join(lines)





#!/usr/bin/env python
# run_insight.py

import sys
import pyspark.sql.functions as F
from pyspark.sql.types import StringType
from pyspark.sql import SparkSession

from optimal_spark_config.create_spark_instance import generate_spark_instance
from HiveIO.config import get_config, get_schema
from HiveIO.io import set_data_config

# Import from your new insight_pipeline
from private_wealth_retention.consumer_prospecting.model.insight_pipeline import (
    train_cluster_insight_model,
    compute_cluster_percentiles,
    generate_insight_string
)

def main():
    spark = generate_spark_instance(total_memory=600, total_vcpu=300)

    # 1) Load config, schema, etc.
    cfg = get_config(None, "config.yaml")
    schema = get_schema(None, "schema.yml")
    db_name = schema["databases"][0]["name"]
    business_date = cfg["dates"]["business_date"]

    feature_categories = cfg["feature_categories"]  # the big mapping from your config
    all_features = cfg["features"]["all_numeric_cols"]  # e.g. or however you list your features

    # 2) Table references
    prospects_table = f"{db_name}.prospects_all"
    pwm_table = f"{db_name}.pwm_clusters_all"
    features_table = cfg["tables"]["prospecting_features"]

    # 3) Load data
    load_data_from, get_table_partitions, _, _ = set_data_config(spark, cfg)
    df_prospects_base = load_data_from(prospects_table)
    df_pwm_base = load_data_from(pwm_table)
    df_features = load_data_from(features_table).filter(F.col("business_date") == business_date)

    # 4) Join with actual numeric columns
    df_prospects = df_prospects_base.join(
        df_features.select("hh_id_in_wh", *all_features),
        on="hh_id_in_wh",
        how="left"
    )
    df_pwm = df_pwm_base.join(
        df_features.select("hh_id_in_wh", *all_features),
        on="hh_id_in_wh",
        how="left"
    )

    # 5) Distinct cluster_ids
    cluster_ids = [
        row["cluster_id"]
        for row in df_prospects.select("cluster_id").distinct().collect()
    ]

    union_df = None

    # 6) For each cluster
    for c_id in cluster_ids:
        # a) train cluster-insight model => pick top features
        feature_importances = train_cluster_insight_model(
            df=df_prospects,
            cluster_col="cluster_id",
            cluster_id=c_id,
            feature_cols=all_features,
            label_col="label",
            seed=42,
            num_trees=50,
            max_depth=5
        )
        top_5_features = feature_importances[:5]

        # b) compute 5–95% among PWM
        #    We only need p5/p95 for the features in top_5, *plus* "aum_dollars" if it's not there
        #    because we might always need cluster_percentiles["aum_dollars"] if any AUM feature appears
        needed_features = set([f for (f, imp) in top_5_features])
        needed_features.add("aum_dollars")  # ensure we have it for AUM references
        needed_features = list(needed_features)

        cluster_percentiles = compute_cluster_percentiles(
            df=df_pwm,
            cluster_id=c_id,
            cluster_col="cluster_id",
            feature_cols=needed_features
        )

        # c) filter prospects in this cluster
        df_cluster = df_prospects.filter(F.col("cluster_id") == c_id)

        # d) Collect them & build insight
        local_rows = df_cluster.collect()
        new_data = []
        for row in local_rows:
            row_dict = row.asDict()
            # build row_values
            row_values = {}
            for feat in all_features:
                row_values[feat] = row_dict.get(feat, 0.0) or 0.0

            insight_str = generate_insight_string(
                top_features=top_5_features,
                row_values=row_values,
                cluster_percentiles=cluster_percentiles,
                feature_categories=feature_categories
            )
            row_dict["insight"] = insight_str
            new_data.append(row_dict)

        # e) Convert back to Spark DF
        new_schema = df_cluster.schema.add("insight", StringType())
        df_cluster_with_insight = spark.createDataFrame(new_data, schema=new_schema)

        # f) Union
        if union_df is None:
            union_df = df_cluster_with_insight
        else:
            union_df = union_df.union(df_cluster_with_insight)

    # 7) final columns
    final_cols = [
        "hh_id_in_wh",
        "similarity_score",
        "distance_to_center",
        "prospect_segment",
        "cluster_id",
        "insight",
        "business_date"
    ]
    # only select columns that exist
    existing_cols = [c for c in final_cols if c in union_df.columns]
    df_insight = union_df.select(*[F.col(c) for c in existing_cols])

    # 8) write final
    # df_insight.write.saveAsTable(f"{db_name}.prospect_insights", mode="overwrite")

    spark.stop()

if __name__ == "__main__":
    main()
