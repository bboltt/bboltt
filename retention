import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report

# ------------------------------
# 1. Data Preparation
# ------------------------------
# Assuming your dataframe is loaded as df_master_transaction
df = df_master_transaction.copy()

# Convert date columns to datetime
df['month_end_date'] = pd.to_datetime(df['month_end_date'], errors='coerce')
df['attrition_date'] = pd.to_datetime(df['attrition_date'], errors='coerce')

# Sort the dataframe by user and month
df.sort_values(['ip_id', 'month_end_date'], inplace=True)
df.reset_index(drop=True, inplace=True)

# ------------------------------
# 2. Filter Data Prior to Attrition Date
# ------------------------------
# For each record, if attrition_date is known, only include data with month_end_date < attrition_date.
# For non-attrited users (where attrition_date might be NaT), keep all records.
df = df[df.apply(lambda row: (pd.isna(row['attrition_date']) or (row['month_end_date'] < row['attrition_date'])), axis=1)]

# ------------------------------
# 3. Identify Count and Amount Columns
# ------------------------------
# Exclude columns that are not part of the analysis
exclude_cols = ['hh_id_in_wh', 'segment']

# For count columns, we assume they start with "num_" (per your screenshot)
count_cols = [col for col in df.columns 
              if col.startswith("num_") and col not in exclude_cols and col not in ['ip_id', 'attrition_label', 'month_end_date', 'attrition_date']]

# For amount columns, we assume they contain "amt" somewhere in the name
amount_cols = [col for col in df.columns 
               if ("amt" in col) and col not in exclude_cols and col not in ['attrition_label', 'attrition_date']]

# Create overall totals for broader signal analysis
df['total_count'] = df[count_cols].sum(axis=1)
df['total_amount'] = df[amount_cols].sum(axis=1)

# ------------------------------
# 4. Helper Functions for Feature Engineering
# ------------------------------
def add_lag_and_pct_change(df, group_col, col, lag=1):
    """
    For each user (group_col), create a lag feature of 'col' and calculate the percentage change.
    """
    df[f'lag_{col}'] = df.groupby(group_col)[col].shift(lag)
    df[f'{col}_pct_change'] = (df[col] - df[f'lag_{col}']) / df[f'lag_{col}']
    # Replace infinities and handle division by zero
    df[f'{col}_pct_change'].replace([np.inf, -np.inf], np.nan, inplace=True)
    return df

def add_rolling_recovery(df, group_col, col, window=3):
    """
    For each user, compute the maximum value in the next 'window' months.
    'Recovery' is defined as reaching or exceeding the previous month's (lag) value.
    """
    def rolling_max_next_n(series):
        return series.shift(-1).rolling(window=window, min_periods=1).max()
    df[f'{col}_max_next_{window}'] = df.groupby(group_col)[col].transform(rolling_max_next_n)
    df[f'{col}_recovered'] = df[f'{col}_max_next_{window}'] >= df[f'lag_{col}']
    return df

# ------------------------------
# 5. Implementing Candidate Attrition Rules
# ------------------------------

# --- Rule 1: ACH Incoming Drop ---
# Based on your screenshot, assume the ACH incoming count column is named "num_ach_incoming"
col_ach_count = "num_ach_incoming"  # adjust if necessary
if col_ach_count in df.columns:
    df = add_lag_and_pct_change(df, group_col='ip_id', col=col_ach_count)
    df = add_rolling_recovery(df, group_col='ip_id', col=col_ach_count, window=3)
    # Flag when ACH incoming drops by 50% or more and does not recover within 3 months
    df['rule_ach_drop'] = ((df[f'{col_ach_count}_pct_change'] <= -0.50) &
                           (df[f'{col_ach_count}_recovered'] == False)).astype(int)
else:
    print(f"Column {col_ach_count} not found in dataframe.")

# --- Rule 2: Total Transaction Count Drop ---
col_total_count = "total_count"
df = add_lag_and_pct_change(df, group_col='ip_id', col=col_total_count)
df = add_rolling_recovery(df, group_col='ip_id', col=col_total_count, window=3)
# Flag when total transaction count drops by 40% or more with no recovery
df['rule_total_count_drop'] = ((df[f'{col_total_count}_pct_change'] <= -0.40) &
                               (df[f'{col_total_count}_recovered'] == False)).astype(int)

# --- Rule 3: Zelle Credit Drop ---
# Assume the Zelle credit count column is named "num_zelle_credit" based on your screenshot
col_zelle_credit = "num_zelle_credit"  # adjust if needed
if col_zelle_credit in df.columns:
    df = add_lag_and_pct_change(df, group_col='ip_id', col=col_zelle_credit)
    df = add_rolling_recovery(df, group_col='ip_id', col=col_zelle_credit, window=3)
    # Flag when Zelle credit drops by 40% or more with no recovery
    df['rule_zelle_drop'] = ((df[f'{col_zelle_credit}_pct_change'] <= -0.40) &
                             (df[f'{col_zelle_credit}_recovered'] == False)).astype(int)
else:
    print(f"Column {col_zelle_credit} not found in dataframe.")

# ------------------------------
# 6. Combine the Rules into a Final Attrition Alert
# ------------------------------
# For this example, if any one of the rules is triggered in a month for a user, we flag an alert.
df['attrition_alert'] = ((df.get('rule_ach_drop', 0) == 1) | 
                         (df['rule_total_count_drop'] == 1) |
                         (df.get('rule_zelle_drop', 0) == 1)).astype(int)

# ------------------------------
# 7. Evaluation & Exploratory Analysis
# ------------------------------
# Evaluate rule performance against the actual attrition label
cm = confusion_matrix(df['attrition_label'], df['attrition_alert'])
print("Confusion Matrix:\n", cm)
report = classification_report(df['attrition_label'], df['attrition_alert'])
print("\nClassification Report:\n", report)

# Compare average percentage changes for key metrics by attrition label
cols_to_check = []
if col_ach_count in df.columns:
    cols_to_check.append(f'{col_ach_count}_pct_change')
cols_to_check.append(f'{col_total_count}_pct_change')
if col_zelle_credit in df.columns:
    cols_to_check.append(f'{col_zelle_credit}_pct_change')

print("Average Percentage Changes by Attrition Label:")
print(df.groupby('attrition_label')[cols_to_check].mean())

# ------------------------------
# 8. Visualization Example: Sample User Trend
# ------------------------------
if col_ach_count in df.columns:
    sample_user = df['ip_id'].iloc[0]  # pick the first user
    user_df = df[df['ip_id'] == sample_user]
    plt.figure(figsize=(10,5))
    plt.plot(user_df['month_end_date'], user_df[col_ach_count], marker='o', label='ACH Incoming')
    plt.title(f'ACH Incoming Transactions Over Time for User {sample_user}')
    plt.xlabel('Month End Date')
    plt.ylabel('ACH Incoming Transactions')
    plt.xticks(rotation=45)
    plt.legend()
    plt.tight_layout()
    plt.show()




import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report

# ------------------------------
# 1. Data Preparation
# ------------------------------
# Assuming your dataframe is loaded as df_master_transaction
df = df_master_transaction.copy()

# Convert 'month_end_date' to datetime
df['month_end_date'] = pd.to_datetime(df['month_end_date'], errors='coerce')

# Sort the dataframe by user and month
df.sort_values(['ip_id', 'month_end_date'], inplace=True)
df.reset_index(drop=True, inplace=True)

# Exclude columns that are not part of the analysis
exclude_cols = ['hh_id_in_wh', 'segment']

# ------------------------------
# 2. Identify Count and Amount Columns
# ------------------------------
# For count columns, we assume they start with "num_" (per your screenshot)
count_cols = [col for col in df.columns 
              if col.startswith("num_") and col not in exclude_cols and col not in ['ip_id', 'attrition_label', 'month_end_date']]

# For amount columns, we assume they contain "amt" (case sensitive; adjust if needed)
amount_cols = [col for col in df.columns 
               if "amt" in col and col not in exclude_cols and col not in ['attrition_label']]

# Create overall totals for broader signal analysis
df['total_count'] = df[count_cols].sum(axis=1)
df['total_amount'] = df[amount_cols].sum(axis=1)

# ------------------------------
# 3. Helper Functions for Feature Engineering
# ------------------------------
def add_lag_and_pct_change(df, group_col, col, lag=1):
    """
    For each user (group_col), create a lag feature of 'col' and calculate the percentage change.
    """
    df[f'lag_{col}'] = df.groupby(group_col)[col].shift(lag)
    df[f'{col}_pct_change'] = (df[col] - df[f'lag_{col}']) / df[f'lag_{col}']
    # Replace infinities and possible div-by-zero issues
    df[f'{col}_pct_change'].replace([np.inf, -np.inf], np.nan, inplace=True)
    return df

def add_rolling_recovery(df, group_col, col, window=3):
    """
    For each user, compute the maximum value in the next 'window' months.
    'Recovery' is defined as reaching or exceeding the previous month's (lag) value.
    """
    def rolling_max_next_n(series):
        return series.shift(-1).rolling(window=window, min_periods=1).max()
    df[f'{col}_max_next_{window}'] = df.groupby(group_col)[col].transform(rolling_max_next_n)
    df[f'{col}_recovered'] = df[f'{col}_max_next_{window}'] >= df[f'lag_{col}']
    return df

# ------------------------------
# 4. Implementing Candidate Attrition Rules
# ------------------------------

# --- Rule 1: ACH Incoming Drop ---
# Based on your screenshot, assume the ACH incoming count column is named "num_ach_incoming"
col_ach_count = "num_ach_incoming"  # adjust if the actual column name differs
if col_ach_count in df.columns:
    df = add_lag_and_pct_change(df, group_col='ip_id', col=col_ach_count)
    df = add_rolling_recovery(df, group_col='ip_id', col=col_ach_count, window=3)
    # Flag when ACH incoming drops by 50% or more and does not recover within 3 months
    df['rule_ach_drop'] = ((df[f'{col_ach_count}_pct_change'] <= -0.50) &
                           (df[f'{col_ach_count}_recovered'] == False)).astype(int)
else:
    print(f"Column {col_ach_count} not found in dataframe.")

# --- Rule 2: Total Transaction Count Drop ---
col_total_count = "total_count"
df = add_lag_and_pct_change(df, group_col='ip_id', col=col_total_count)
df = add_rolling_recovery(df, group_col='ip_id', col=col_total_count, window=3)
# Flag when total transaction count drops by 40% or more with no recovery
df['rule_total_count_drop'] = ((df[f'{col_total_count}_pct_change'] <= -0.40) &
                               (df[f'{col_total_count}_recovered'] == False)).astype(int)

# --- Rule 3: Zelle Credit Drop ---
# Assume the Zelle credit count column is named "num_zelle_credit" based on your screenshot
col_zelle_credit = "num_zelle_credit"  # adjust if needed
if col_zelle_credit in df.columns:
    df = add_lag_and_pct_change(df, group_col='ip_id', col=col_zelle_credit)
    df = add_rolling_recovery(df, group_col='ip_id', col=col_zelle_credit, window=3)
    # Flag when Zelle credit drops by 40% or more with no recovery
    df['rule_zelle_drop'] = ((df[f'{col_zelle_credit}_pct_change'] <= -0.40) &
                             (df[f'{col_zelle_credit}_recovered'] == False)).astype(int)
else:
    print(f"Column {col_zelle_credit} not found in dataframe.")

# ------------------------------
# 5. Combine the Rules into a Final Attrition Alert
# ------------------------------
# For this example, if any one of the rules is triggered in a month for a user, we flag an alert.
df['attrition_alert'] = ((df.get('rule_ach_drop', 0) == 1) | 
                         (df['rule_total_count_drop'] == 1) |
                         (df.get('rule_zelle_drop', 0) == 1)).astype(int)

# ------------------------------
# 6. Evaluation & Exploratory Analysis
# ------------------------------

# Evaluate rule performance against the actual attrition label
cm = confusion_matrix(df['attrition_label'], df['attrition_alert'])
print("Confusion Matrix:\n", cm)
report = classification_report(df['attrition_label'], df['attrition_alert'])
print("\nClassification Report:\n", report)

# Compare average percentage changes for key metrics by attrition label
cols_to_check = []
if col_ach_count in df.columns:
    cols_to_check.append(f'{col_ach_count}_pct_change')
cols_to_check.append(f'{col_total_count}_pct_change')
if col_zelle_credit in df.columns:
    cols_to_check.append(f'{col_zelle_credit}_pct_change')

print("Average Percentage Changes by Attrition Label:")
print(df.groupby('attrition_label')[cols_to_check].mean())

# ------------------------------
# 7. Visualization Example: Sample User Trend
# ------------------------------
if col_ach_count in df.columns:
    sample_user = df['ip_id'].iloc[0]  # pick the first user
    user_df = df[df['ip_id'] == sample_user]
    plt.figure(figsize=(10,5))
    plt.plot(user_df['month_end_date'], user_df[col_ach_count], marker='o', label='ACH Incoming')
    plt.title(f'ACH Incoming Transactions Over Time for User {sample_user}')
    plt.xlabel('Month End Date')
    plt.ylabel('ACH Incoming Transactions')
    plt.xticks(rotation=45)
    plt.legend()
    plt.tight_layout()
    plt.show()













import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report

# --- Load your data ---
# Assuming your dataframe is already loaded into df_master_transaction
df = df_master_transaction.copy()

# Convert 'month_end_date' to datetime (if not already)
df['month_end_date'] = pd.to_datetime(df['month_end_date'], errors='coerce')

# Sort by user and date
df.sort_values(['ip_id', 'month_end_date'], inplace=True)
df.reset_index(drop=True, inplace=True)


# Identify columns for counts and amounts
count_cols = [col for col in df.columns 
              if col.startswith('num_') and col.endswith('_cnt_monthly_sum')]
amount_cols = [col for col in df.columns if col.endswith('_amt_monthly_sum')]

# Create a total transaction count and total transaction amount
df['total_count'] = df[count_cols].sum(axis=1)
df['total_amount'] = df[amount_cols].sum(axis=1)


def add_lag_and_pct_change(df, group_col, date_col, col, lag=1):
    """
    For each user (group_col), create a lag feature of `col` and a percentage change vs. that lag.
    """
    df[f'lag_{col}'] = df.groupby(group_col)[col].shift(lag)
    df[f'{col}_pct_change'] = (
        (df[col] - df[f'lag_{col}']) / df[f'lag_{col}']
    ).replace([np.inf, -np.inf], np.nan)
    return df

def add_rolling_recovery(df, group_col, col, window=3):
    """
    For each user, check if the metric recovers within the next 'window' months.
    'Recovery' means the value returns to or exceeds the prior month's value.
    """
    def rolling_max_next_n(series):
        # Shift by -1 to start looking at the "next" month
        # Then rolling() over 'window' to find max in next 'window' months
        return series.shift(-1).rolling(window=window, min_periods=1).max()
    
    # Compute the rolling max over the next 'window' months
    df[f'{col}_max_next_{window}'] = df.groupby(group_col)[col].transform(rolling_max_next_n)
    
    # We define "recovered" if the max in the next N months >= last month's value
    # But we must compare to the 'lag_{col}' so we know the previous month's value
    df[f'{col}_recovered'] = (
        df[f'{col}_max_next_{window}'] >= df[f'lag_{col}']
    )
    
    return df


# ACH Incoming Count
col_ach_count = 'num_ach_incoming_cnt_monthly_sum'

df = add_lag_and_pct_change(df, group_col='ip_id', date_col='month_end_date', col=col_ach_count)
df = add_rolling_recovery(df, group_col='ip_id', col=col_ach_count, window=3)

# Rule: Drop >= 50% and no recovery
df['rule_ach_drop'] = (
    (df[f'{col_ach_count}_pct_change'] <= -0.50) &
    (df[f'{col_ach_count}_recovered'] == False)
).astype(int)


col_total_count = 'total_count'

df = add_lag_and_pct_change(df, group_col='ip_id', date_col='month_end_date', col=col_total_count)
df = add_rolling_recovery(df, group_col='ip_id', col=col_total_count, window=3)

# Rule: Drop >= 40% and no recovery
df['rule_total_count_drop'] = (
    (df[f'{col_total_count}_pct_change'] <= -0.40) &
    (df[f'{col_total_count}_recovered'] == False)
).astype(int)


# If you have a column for Zelle credit counts
col_zelle_credit = 'num_zelle_credit_monthly_sum'  # adjust if needed

df = add_lag_and_pct_change(df, group_col='ip_id', date_col='month_end_date', col=col_zelle_credit)
df = add_rolling_recovery(df, group_col='ip_id', col=col_zelle_credit, window=3)

df['rule_zelle_drop'] = (
    (df[f'{col_zelle_credit}_pct_change'] <= -0.40) &
    (df[f'{col_zelle_credit}_recovered'] == False)
).astype(int)


df['attrition_alert'] = (
    (df['rule_ach_drop'] == 1) |
    (df['rule_total_count_drop'] == 1) |
    (df['rule_zelle_drop'] == 1)
).astype(int)




# Confusion Matrix
cm = confusion_matrix(df['attrition_label'], df['attrition_alert'])
print("Confusion Matrix:\n", cm)

# Classification Report (precision, recall, F1)
report = classification_report(df['attrition_label'], df['attrition_alert'])
print("\nClassification Report:\n", report)


grouped = df.groupby('attrition_label')[
    [f'{col_ach_count}_pct_change',
     f'{col_total_count}_pct_change',
     f'{col_zelle_credit}_pct_change']
].mean()

print("Average % Change by Attrition Label:\n", grouped)


sample_user = df['ip_id'].iloc[0]  # pick one user ID
user_df = df[df['ip_id'] == sample_user]

plt.figure(figsize=(10,5))
plt.plot(user_df['month_end_date'], user_df[col_ach_count], marker='o', label='ACH Incoming Count')
plt.title(f'ACH Incoming Transactions Over Time for User {sample_user}')
plt.xlabel('Month')
plt.ylabel('ACH Incoming Count')
plt.xticks(rotation=45)
plt.legend()
plt.tight_layout()
plt.show()

