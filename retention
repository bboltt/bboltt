from pyspark.sql import SparkSession
from pyspark.sql.functions import last_day, sum as _sum

# Initialize SparkSession if not already available
spark = SparkSession.builder.appName("AggregateColumns").enableHiveSupport().getOrCreate()

# Read the Hive table into a DataFrame; replace 'your_table' with your actual table name
df = spark.table("your_table")

# Define the columns to exclude from aggregation
exclude_columns = {'involved_party_id', 'business_date', 'system_id', 'data_type'}

# Build aggregation expressions for each remaining column dynamically
agg_expressions = [_sum(c).alias(f"{c}_sum") for c in df.columns if c not in exclude_columns]

# Group by involved_party_id and the last day of business_date (to represent the month)
result_df = df.groupBy("involved_party_id", last_day("business_date").alias("month_end_date")) \
              .agg(*agg_expressions)

# Display the result
result_df.show()

