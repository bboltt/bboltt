import pandas as pd
import numpy as np
from collections import defaultdict, deque  # deque unused but kept to avoid edits elsewhere

# --- inputs (same variables) ---
CORR_CSV = "/development_work/cs_2025/misc_figures/6_newer_sample_custs_with_accts/correlation_all_features_pearson.csv"
IV_CSV   = "/development_work/cs_2025/misc_figures/7_newer_sample_0912_custs_with_accts/information_values_filledna.csv"
THRESH   = 0.9
LABEL_COL = "fraud_label_180"

# ======================
# load correlation matrix
# ======================
corr = pd.read_csv(CORR_CSV)
# if first column is an index col, set it as index
if corr.columns[0] not in corr.columns[1:]:
    corr = corr.set_index(corr.columns[0])

# keep only symmetric intersection (defensive)
common = corr.index.intersection(corr.columns)
corr = corr.loc[common, common]

# drop label row/col if present (safe)
if LABEL_COL in corr.index:
    corr = corr.drop(index=LABEL_COL, errors="ignore")
    corr = corr.drop(columns=LABEL_COL, errors="ignore")

# ensure numeric and zero diagonal
corr = corr.astype(float)
np.fill_diagonal(corr.values, 0.0)

# ======================
# load IVs (keep your column names)
# ======================
iv = pd.read_csv(IV_CSV)
# normalize typical columns "Variable","IV"
iv = iv.rename(columns={c: c.strip() for c in iv.columns})
iv["Variable"] = iv["Variable"].astype(str).str.strip()
iv["IV"] = pd.to_numeric(iv["IV"], errors="coerce")

iv_map = dict(zip(iv["Variable"], iv["IV"]))

def get_iv(f):
    v = iv_map.get(f, np.nan)
    return -1e15 if pd.isna(v) else float(v)

# ======================
# build high-corr pairs (|corr| >= THRESH), upper triangle only
# ======================
pairs = []
cols = corr.columns.tolist()
arr = corr.values
n = arr.shape[0]
for i in range(n):
    # j>i upper triangle
    hi = np.where(np.abs(arr[i, i+1:]) >= THRESH)[0]
    for off in hi:
        j = i + 1 + int(off)
        pairs.append((cols[i], cols[j]))

# short-circuit if nothing to prune
if not pairs:
    print("No pairs with |corr| >=", THRESH)
    pd.DataFrame([], columns=["feature"]).to_csv("features_to_drop.csv", index=False)
    pd.DataFrame([], columns=["feature_to_drop","kept_feature","corr_with_kept",
                              "direct_edge","partner_in_comp","corr_with_partner",
                              "related_features"]) \
      .to_csv("features_to_drop_audit.csv", index=False)
else:
    # ======================
    # adjacency and edge correlation (signed) for DIRECT checks
    # ======================
    adj = defaultdict(set)
    edge_corr = {}
    for a, b in pairs:
        adj[a].add(b)
        adj[b].add(a)
        edge_corr[frozenset((a, b))] = float(corr.loc[a, b])

    # all features seen
    all_feats = list(set(corr.columns) | set(adj.keys()))

    # order by IV desc, tiebreak by name for determinism
    order = sorted(all_feats, key=lambda f: (get_iv(f), f), reverse=True)

    kept = []
    to_drop = set()
    covered = set()          # features already handled (kept or blocked by a kept)
    reason = {}              # feature -> (kept_feature, corr_used)
    audit_rows = []

    for f in order:
        if f in covered:
            continue
        # keep f
        kept.append(f)
        covered.add(f)

        # drop only direct neighbors with |corr| >= THRESH
        for g in adj[f]:
            if g in covered:
                continue
            c_fg = float(corr.loc[f, g])
            if abs(c_fg) >= THRESH:
                covered.add(g)
                # keep strongest direct reason if multiple
                prev = reason.get(g)
                if (prev is None) or (abs(c_fg) > abs(prev[1])):
                    reason[g] = (f, c_fg)
                to_drop.add(g)

    # --- build compact audit: every dropped feature has a direct cause keeper
    for g in sorted(to_drop):
        keeper, cval = reason[g]
        audit_rows.append({
            "feature_to_drop": g,
            "kept_feature": keeper,
            "corr_with_kept": float(cval),
            "direct_edge": 1,
            "partner_in_comp": keeper,              # same as kept_feature; direct cause
            "corr_with_partner": float(cval),       # same correlation value
            "related_features": keeper              # keep audit compact: direct cause only
        })

    # ======================
    # outputs
    # ======================
    pd.DataFrame(sorted(to_drop), columns=["feature"]).to_csv("features_to_drop.csv", index=False)
    pd.DataFrame(audit_rows).to_csv("features_to_drop_audit.csv", index=False)

    print(f"Kept {len(kept)} features; Dropping {len(to_drop)}.")













import pandas as pd
import numpy as np
from pathlib import Path

# ---- CONFIG ----
CORR_CSV = "correlation_all_features_pearson.csv"
FI_CSV   = "feature_importance_summary.csv"
THRESH   = 0.90                  # absolute Pearson threshold
OUT_DROP = "features_to_drop_corr090.csv"

# ---- load correlation matrix (square) ----
corr = pd.read_csv(CORR_CSV)
# assume first column is the index with feature names if not already
if corr.columns[0] != corr.index.name:
    # try to set index from first column if it’s the feature name column
    if corr.columns[0] not in corr.columns[1:]:
        corr = corr.set_index(corr.columns[0])

# sanity: make symmetric and zero self-corr
corr = corr.astype(float)
np.fill_diagonal(corr.values, 0.0)

# melt into pairs (i < j) to avoid duplicates
pairs = (
    corr.stack()
         .rename("corr")
         .reset_index(names=["f1", "f2"])
)
pairs = pairs[pairs["f1"] < pairs["f2"]]              # i<j
pairs = pairs[pairs["corr"].abs() >= THRESH]          # strong correlations only

# ---- load feature importance ----
fi = pd.read_csv(FI_CSV)

# pick an "importance" score:
# 1) average of all columns starting with "gain_pct"
gain_pct_cols = [c for c in fi.columns if c.startswith("gain_pct")]
if gain_pct_cols:
    fi["importance"] = fi[gain_pct_cols].mean(axis=1)
else:
    # 2) else average of all columns starting with "gain"
    gain_cols = [c for c in fi.columns if c.startswith("gain")]
    if gain_cols:
        fi["importance"] = fi[gain_cols].mean(axis=1)
    else:
        # 3) fallback: if only a single 'gain' or 'importance' available, try those
        for cand in ("importance","gain","Gain","gain_mean"):
            if cand in fi.columns:
                fi["importance"] = fi[cand]
                break
        if "importance" not in fi.columns:
            raise ValueError("Could not find importance columns (gain_pct*/gain*/importance) in FI file.")

fi = fi[["feature","importance"]].drop_duplicates()
imp = dict(zip(fi["feature"], fi["importance"]))

# default importance (very small) for any feature missing in FI
def get_imp(f): 
    v = imp.get(f)
    return float(v) if pd.notnull(v) else -1e9

# ---- build neighbor graph for correlation ≥ THRESH ----
from collections import defaultdict
nbrs = defaultdict(set)
for _, r in pairs.iterrows():
    a, b = r["f1"], r["f2"]
    nbrs[a].add(b)
    nbrs[b].add(a)

all_feats = set(corr.index.tolist())
# sort all features by importance (desc), tie-break by name (asc)
ordered = sorted(all_feats, key=lambda f: (-get_imp(f), f))

keep, drop = set(), set()

# greedy: walk in importance order; keep first in each correlated cluster, drop its neighbors
for f in ordered:
    if f in drop or f in keep:
        continue
    keep.add(f)
    for n in nbrs.get(f, []):
        if n not in keep:
            drop.add(n)

# Optionally: restrict to only features present in your model input (i.e., present in FI)
# drop = [f for f in drop if f in imp]

# ---- save & show ----
drop_list = pd.DataFrame({"feature_to_drop": sorted(drop)})
drop_list.to_csv(OUT_DROP, index=False)
print(f"Dropping {len(drop)} features at |corr| ≥ {THRESH}. Saved to {OUT_DROP}.")

# A short summary:
print("\nTop 20 kept features (by importance) for transparency:")
print(pd.DataFrame({"feature": list(keep)[:20],
                    "importance": [get_imp(f) for f in list(keep)[:20]]}).sort_values("importance", ascending=False))

# If you want to see a sample of conflicts that caused drops:
if not pairs.empty:
    # join pairs with importance to show which side won
    tmp = pairs.copy()
    tmp["imp_f1"] = tmp["f1"].map(get_imp)
    tmp["imp_f2"] = tmp["f2"].map(get_imp)
    tmp["kept"]   = np.where(tmp["imp_f1"] >= tmp["imp_f2"], tmp["f1"], tmp["f2"])
    tmp["dropped"]= np.where(tmp["imp_f1"] >= tmp["imp_f2"], tmp["f2"], tmp["f1"])
    # only show those actually dropped by the greedy pass
    sample = tmp[tmp["dropped"].isin(drop)].sort_values("corr", key=np.abs, ascending=False).head(25)
    if not sample.empty:
        print("\nExamples of highly correlated pairs where the lower-importance feature was dropped:")
        print(sample[["f1","f2","corr","imp_f1","imp_f2","kept","dropped"]].to_string(index=False))

