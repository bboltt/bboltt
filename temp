import pandas as pd
import numpy as np
from collections import defaultdict

# ------------ CONFIG ------------
CORR_CSV = "correlation_all_features_pearson.csv"   # square Pearson matrix
IV_CSV   = "information_values_filledna.csv"        # has columns like: Variable, IV
THRESH   = 0.90                                     # absolute corr threshold
OUT_DROP = "features_to_drop_corr090_byIV.csv"
# --------------------------------

# 1) Load correlation matrix
corr = pd.read_csv(CORR_CSV)
if corr.columns[0] not in corr.columns[1:]:
    corr = corr.set_index(corr.columns[0])
corr = corr.astype(float)
np.fill_diagonal(corr.values, 0.0)

pairs = (
    corr.stack()
         .rename("corr")
         .reset_index(names=["f1","f2"])
)
pairs = pairs[pairs["f1"] < pairs["f2"]]
pairs = pairs[pairs["corr"].abs() >= THRESH]

# 2) Load Information Value table
iv_df = pd.read_csv(IV_CSV)
iv_df.columns = [c.strip().lower() for c in iv_df.columns]
feat_col = "variable" if "variable" in iv_df.columns else "feature"
iv_col   = "iv"        if "iv"        in iv_df.columns else iv_df.columns[1]
iv_df = iv_df[[feat_col, iv_col]].rename(columns={feat_col:"feature", iv_col:"iv"})
iv_df["iv"] = pd.to_numeric(iv_df["iv"], errors="coerce")

iv_map = dict(zip(iv_df["feature"], iv_df["iv"]))

def get_iv(f):
    v = iv_map.get(f)
    return float(v) if pd.notnull(v) else -1e9   # very small if missing IV

# 3) Build neighbor graph for |corr| >= THRESH
nbrs = defaultdict(set)
for _, r in pairs.iterrows():
    a, b = r["f1"], r["f2"]
    nbrs[a].add(b); nbrs[b].add(a)

# 4) Greedy selection by IV: keep highest-IV feature in each cluster
all_feats = set(corr.index.tolist())
ordered = sorted(all_feats, key=lambda f: (-get_iv(f), f))  # tie-break by name for determinism

keep, drop = set(), set()
for f in ordered:
    if f in keep or f in drop:
        continue
    keep.add(f)
    for n in nbrs.get(f, ()):
        if n not in keep:
            drop.add(n)

# 5) Save list to drop
pd.DataFrame({"feature_to_drop": sorted(drop)}).to_csv(OUT_DROP, index=False)
print(f"Dropping {len(drop)} features at |corr| >= {THRESH} using IV. Saved -> {OUT_DROP}")

# (Optional) show a few representative conflicts
if not pairs.empty:
    tmp = pairs.copy()
    tmp["iv_f1"] = tmp["f1"].map(get_iv)
    tmp["iv_f2"] = tmp["f2"].map(get_iv)
    tmp["kept"] = np.where(tmp["iv_f1"] >= tmp["iv_f2"], tmp["f1"], tmp["f2"])
    tmp["dropped"] = np.where(tmp["iv_f1"] >= tmp["iv_f2"], tmp["f2"], tmp["f1"])
    demo = tmp[tmp["dropped"].isin(drop)].sort_values("corr", key=np.abs, ascending=False).head(20)
    if not demo.empty:
        print("\nExamples:")
        print(demo[["f1","f2","corr","iv_f1","iv_f2","kept","dropped"]].to_string(index=False))
















import pandas as pd
import numpy as np
from pathlib import Path

# ---- CONFIG ----
CORR_CSV = "correlation_all_features_pearson.csv"
FI_CSV   = "feature_importance_summary.csv"
THRESH   = 0.90                  # absolute Pearson threshold
OUT_DROP = "features_to_drop_corr090.csv"

# ---- load correlation matrix (square) ----
corr = pd.read_csv(CORR_CSV)
# assume first column is the index with feature names if not already
if corr.columns[0] != corr.index.name:
    # try to set index from first column if it’s the feature name column
    if corr.columns[0] not in corr.columns[1:]:
        corr = corr.set_index(corr.columns[0])

# sanity: make symmetric and zero self-corr
corr = corr.astype(float)
np.fill_diagonal(corr.values, 0.0)

# melt into pairs (i < j) to avoid duplicates
pairs = (
    corr.stack()
         .rename("corr")
         .reset_index(names=["f1", "f2"])
)
pairs = pairs[pairs["f1"] < pairs["f2"]]              # i<j
pairs = pairs[pairs["corr"].abs() >= THRESH]          # strong correlations only

# ---- load feature importance ----
fi = pd.read_csv(FI_CSV)

# pick an "importance" score:
# 1) average of all columns starting with "gain_pct"
gain_pct_cols = [c for c in fi.columns if c.startswith("gain_pct")]
if gain_pct_cols:
    fi["importance"] = fi[gain_pct_cols].mean(axis=1)
else:
    # 2) else average of all columns starting with "gain"
    gain_cols = [c for c in fi.columns if c.startswith("gain")]
    if gain_cols:
        fi["importance"] = fi[gain_cols].mean(axis=1)
    else:
        # 3) fallback: if only a single 'gain' or 'importance' available, try those
        for cand in ("importance","gain","Gain","gain_mean"):
            if cand in fi.columns:
                fi["importance"] = fi[cand]
                break
        if "importance" not in fi.columns:
            raise ValueError("Could not find importance columns (gain_pct*/gain*/importance) in FI file.")

fi = fi[["feature","importance"]].drop_duplicates()
imp = dict(zip(fi["feature"], fi["importance"]))

# default importance (very small) for any feature missing in FI
def get_imp(f): 
    v = imp.get(f)
    return float(v) if pd.notnull(v) else -1e9

# ---- build neighbor graph for correlation ≥ THRESH ----
from collections import defaultdict
nbrs = defaultdict(set)
for _, r in pairs.iterrows():
    a, b = r["f1"], r["f2"]
    nbrs[a].add(b)
    nbrs[b].add(a)

all_feats = set(corr.index.tolist())
# sort all features by importance (desc), tie-break by name (asc)
ordered = sorted(all_feats, key=lambda f: (-get_imp(f), f))

keep, drop = set(), set()

# greedy: walk in importance order; keep first in each correlated cluster, drop its neighbors
for f in ordered:
    if f in drop or f in keep:
        continue
    keep.add(f)
    for n in nbrs.get(f, []):
        if n not in keep:
            drop.add(n)

# Optionally: restrict to only features present in your model input (i.e., present in FI)
# drop = [f for f in drop if f in imp]

# ---- save & show ----
drop_list = pd.DataFrame({"feature_to_drop": sorted(drop)})
drop_list.to_csv(OUT_DROP, index=False)
print(f"Dropping {len(drop)} features at |corr| ≥ {THRESH}. Saved to {OUT_DROP}.")

# A short summary:
print("\nTop 20 kept features (by importance) for transparency:")
print(pd.DataFrame({"feature": list(keep)[:20],
                    "importance": [get_imp(f) for f in list(keep)[:20]]}).sort_values("importance", ascending=False))

# If you want to see a sample of conflicts that caused drops:
if not pairs.empty:
    # join pairs with importance to show which side won
    tmp = pairs.copy()
    tmp["imp_f1"] = tmp["f1"].map(get_imp)
    tmp["imp_f2"] = tmp["f2"].map(get_imp)
    tmp["kept"]   = np.where(tmp["imp_f1"] >= tmp["imp_f2"], tmp["f1"], tmp["f2"])
    tmp["dropped"]= np.where(tmp["imp_f1"] >= tmp["imp_f2"], tmp["f2"], tmp["f1"])
    # only show those actually dropped by the greedy pass
    sample = tmp[tmp["dropped"].isin(drop)].sort_values("corr", key=np.abs, ascending=False).head(25)
    if not sample.empty:
        print("\nExamples of highly correlated pairs where the lower-importance feature was dropped:")
        print(sample[["f1","f2","corr","imp_f1","imp_f2","kept","dropped"]].to_string(index=False))

