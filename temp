# rcif/â€¦/application_history_functions.py (top of file or near other helpers)

from datetime import datetime
from dateutil.relativedelta import relativedelta
from pyspark.sql import functions as F

def _oao_partitions_covering(spark, table: str, first_date: str, last_date: str) -> list[str]:
    """
    Returns partition dates (YYYY-MM-DD) from `table` so the union of their
    3-year snapshots covers [first_date, last_date].
    """
    parts = (
        spark.table(table)
             .select(F.col("ods_business_dt").cast("date").alias("dt"))
             .distinct()
             .collect()
    )
    dates = sorted({r["dt"].strftime("%Y-%m-%d") for r in parts})
    if not dates:
        return []

    def first_on_or_after(target: str) -> str:
        for p in dates:
            if p >= target:
                return p
        return dates[-1]

    picked = []
    cursor = last_date
    while True:
        p = first_on_or_after(cursor)
        if not picked or p != picked[-1]:
            picked.append(p)
        p_dt = datetime.strptime(p, "%Y-%m-%d")
        cover_start = (p_dt - relativedelta(years=3)).strftime("%Y-%m-%d")
        if cover_start <= first_date:
            break
        cursor = cover_start

    return picked


oao_dt1_list = _oao_partitions_covering(spark, "sl1_oao.tbl_application_h",     first_date, last_date)
oao_dt2_list = _oao_partitions_covering(spark, "sl1_oao.tbl_customerproducts_h", first_date, last_date)
oao_dt3_list = _oao_partitions_covering(spark, "sl1_oao.tbl_products_h",         first_date, last_date)
oao_dt4_list = _oao_partitions_covering(spark, "sl1_oao.tbl_producttype_h",      first_date, last_date)

# and use .isin(oao_dtX_list) in your filters
