def create_temp_view_of_lookup(
    spark: SparkSession,
    first_date: str,
    last_date: str,
):
    # ----------------------------------------------------------------------
    # [unchanged code above here: your bp_base_pop, rcds_base, rcds_accts,
    #  rcds_appdates, get_closest_partition usages for RFCQ, etc.]
    # ----------------------------------------------------------------------

    # ======================================================================
    # >>> CHANGED: helper to choose DO partitions covering [first_date, last_date]
    # ======================================================================
    from datetime import datetime
    from dateutil.relativedelta import relativedelta

    def _do_parts(spark, table_name: str, first_date: str, last_date: str) -> list[str]:
        """
        Walk DO snapshots backward ~3 years at a time, ensuring we include:
         - the partition AFTER last_date
         - the first partition AFTER first_date
        Returns a de-duplicated, insertion-order list of partition dates (YYYY-MM-DD).
        """
        parts, seen = [], set()
        cur = datetime.strptime(last_date, "%Y-%m-%d")
        first_dt = datetime.strptime(first_date, "%Y-%m-%d")

        while True:
            p = get_closest_partition(spark, table_name, cur.strftime("%Y-%m-%d"), get_after_date=True)
            if p in seen:  # safety against accidental loop
                break
            parts.append(p)
            seen.add(p)

            # each DO snapshot contains about the prior 3 years of history
            cur = datetime.strptime(p, "%Y-%m-%d") - relativedelta(years=3)

            # once weâ€™ve stepped back past first_date, include the first partition AFTER first_date and stop
            if cur <= first_dt:
                p_first = get_closest_partition(spark, table_name, first_date, get_after_date=True)
                if p_first not in seen:
                    parts.append(p_first)
                break

        # de-dupe preserve order
        out, seen2 = [], set()
        for d in parts:
            if d not in seen2:
                out.append(d)
                seen2.add(d)
        return out
    # ======================================================================
    # <<< CHANGED helper ends
    # ======================================================================

    # ----------------------------------------------------------------------
    # [unchanged code here: any RFCQ / RCF tables you assembled before DO]
    # ----------------------------------------------------------------------

    # ======================================================================
    # >>> CHANGED: DO accounts block (replaces the TODO & any hard-coded lists)
    # ======================================================================

    # pick the required partitions for each DO table
    do_app_parts       = _do_parts(spark, "sl1_do_tbl_application_h",         first_date, last_date)
    do_consumer_parts  = _do_parts(spark, "sl1_do_tbl_consumerloan_h",        first_date, last_date)
    do_prodcd_parts    = _do_parts(spark, "sl1_do_tbl_productcd_h",           first_date, last_date)
    do_prodtype_parts  = _do_parts(spark, "sl1_do_tbl_producttype_h",         first_date, last_date)
    do_custprod_parts  = _do_parts(spark, "sl1_do_tbl_customerproducts_h",    first_date, last_date)

    # optional: print to verify which snapshots are used
    # print("DO partitions:",
    #       "\n  application_h :", do_app_parts,
    #       "\n  consumerloan_h:", do_consumer_parts,
    #       "\n  productcd_h   :", do_prodcd_parts,
    #       "\n  producttype_h :", do_prodtype_parts,
    #       "\n  customerproducts_h:", do_custprod_parts)

    do_app = (
        spark.table("sl1_do_tbl_application_h")
             .filter(F.col("ods_business_dt").isin(do_app_parts))
             .select(
                 F.col("ods_business_dt"),
                 F.col("applicationid"),
                 F.col("session_id"),
             )
             .distinct()
    )

    do_acct_nums = (
        spark.table("sl1_do_tbl_customerproducts_h")
             .filter(F.col("ods_business_dt").isin(do_custprod_parts))
             .select(
                 F.col("applicationid"),
                 F.col("productnumber").cast("bigint").alias("accountnumber"),
             )
             .distinct()
    )

    do_products = (
        spark.table("sl1_do_tbl_productcd_h")
             .filter(F.col("ods_business_dt").isin(do_prodcd_parts))
             .select(
                 F.col("productcd"),
                 F.col("productid"),
             )
             .distinct()
    )

    do_prod_types = (
        spark.table("sl1_do_tbl_producttype_h")
             .filter(F.col("ods_business_dt").isin(do_prodtype_parts))
             .select(
                 F.col("productid"),
                 F.col("producttype"),
             )
             .distinct()
    )

    # join to product metadata (unchanged join keys/logic you already had)
    do_accts = (
        do_acct_nums
        .join(do_products, on="productcd", how="left")
        .join(do_prod_types, on="productid", how="left")
        # keep your original filters here (checking/savings/new banking, productid not null, session_id null handling, etc.)
    )

    # union DO with RCDS (or your other sources) and finish your lookup build
    union_accts = rcds_accts.unionByName(do_accts, allowMissingColumns=True)

    final_lookup = (
        union_accts
        .join(bp_base_pop, on=[F.col("session_id") == bp_base_pop.appid], how="left")  # keep your original join(s)
        .select("datacollectionkey", "applicationid")  # your original projected columns
        .distinct()
    )

    final_lookup.createOrReplaceTempView("do_account_lookup")

    # ======================================================================
    # <<< CHANGED DO accounts block ends
    # ======================================================================

    # ----------------------------------------------------------------------
    # [unchanged code below here: any unpersist/cleanup/returns]
    # ----------------------------------------------------------------------

