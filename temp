from datetime import datetime, timedelta
from dateutil.relativedelta import relativedelta
from typing import List

def get_oao_partitions_covering(
    spark, table: str, first_date: str, last_date: str
) -> List[str]:
    """
    Returns partition dates from `table` so the union of their 3-year snapshots
    covers [first_date, last_date], newest->oldest.

    Minimal change: add safeguards to avoid picking the same partition repeatedly.
    """

    def _on_or_after(target: str) -> str:
        return get_closest_partition(spark, table, target, get_after_date=True)

    def _on_or_before(target: str) -> str:
        return get_closest_partition(spark, table, target, get_after_date=False)

    picked: List[str] = []

    # seed with the partition on/after last_date (your original behavior)
    p = _on_or_after(last_date)
    picked.append(p)

    MAX_HOPS = 24  # defensive; with 3y spans you should be << 10

    for _ in range(MAX_HOPS):
        p_dt = datetime.strptime(p, "%Y-%m-%d")
        cover_start = (p_dt - relativedelta(years=3)).strftime("%Y-%m-%d")

        # covered the whole window?
        if cover_start <= first_date:
            break

        # target the next uncovered point
        next_target = cover_start

        q = _on_or_after(next_target)

        # --- FIX: if we got the same partition, nudge back a day and retry ---
        if q == p:
            next_target = (datetime.strptime(next_target, "%Y-%m-%d") - timedelta(days=1)).strftime("%Y-%m-%d")
            q = _on_or_after(next_target)

        # --- FIX: if still the same, fall back to on/before (previous partition) ---
        if q == p:
            q = _on_or_before(next_target)

        # if nothing progresses (extremely unlikely), stop to avoid infinite loop
        if not q or q == p:
            break

        picked.append(q)
        p = q

    # dedupe while preserving order
    seen = set()
    out = []
    for d in picked:
        if d not in seen:
            seen.add(d)
            out.append(d)
    return out



oao_dt1_list = _oao_partitions_covering(spark, "sl1_oao.tbl_application_h",     first_date, last_date)
oao_dt2_list = _oao_partitions_covering(spark, "sl1_oao.tbl_customerproducts_h", first_date, last_date)
oao_dt3_list = _oao_partitions_covering(spark, "sl1_oao.tbl_products_h",         first_date, last_date)
oao_dt4_list = _oao_partitions_covering(spark, "sl1_oao.tbl_producttype_h",      first_date, last_date)

# and use .isin(oao_dtX_list) in your filters
