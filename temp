# ===== configure =====
import pandas as pd
import numpy as np
from collections import defaultdict, deque

# paths
CORR_CSV = "/path/to/correlation_all_features_pearson.csv"
IV_CSV   = "/path/to/information_values.csv"

THRESH = 0.90
LABEL_COL = "fraud_label_180"   # drop this if present

# ===== load & clean correlation matrix =====
corr = pd.read_csv(CORR_CSV)

# If the first column is the row labels (e.g., "Unnamed: 0"), move it to index
if corr.columns[0] not in corr.columns[1:]:
    corr = corr.set_index(corr.columns[0])

# Ensure square & aligned: keep only intersection of rows/cols and same order
common = corr.index.intersection(corr.columns)
corr = corr.loc[common, common]

# Drop the label row/col if it slipped in
if LABEL_COL in corr.index:
    corr = corr.drop(index=LABEL_COL, columns=LABEL_COL, errors="ignore")

# Force numeric and zero-out diagonal
corr = corr.astype(float)
np.fill_diagonal(corr.values, 0.0)

# ===== load IVs =====
iv = pd.read_csv(IV_CSV)
# Your file looks like columns 'Variable' and 'IV'
iv = iv.rename(columns={c: c.strip() for c in iv.columns})
iv["Variable"] = iv["Variable"].astype(str).str.strip()
iv["IV"] = pd.to_numeric(iv["IV"], errors="coerce")
iv_map = dict(zip(iv["Variable"], iv["IV"]))

# Fallback: any feature missing in IV map gets very small IV so it loses tie-breaks
def get_iv(f):
    v = iv_map.get(f)
    return -1e9 if pd.isna(v) else float(v)

# ===== build the graph of highly correlated pairs (upper triangle only) =====
# mask upper triangle (k=1 excludes diagonal)
mask = np.triu(np.ones_like(corr, dtype=bool), k=1)
high = (corr.where(mask).abs() >= THRESH)

pairs = []
idx = high.index.to_list()
cols = high.columns.to_list()
arr = high.values
for i in range(arr.shape[0]):
    for j in range(i+1, arr.shape[1]):
        if arr[i, j]:
            f1, f2 = idx[i], cols[j]
            pairs.append((f1, f2))

# Nothing to drop?
if not pairs:
    pd.DataFrame(columns=["feature"]).to_csv("features_to_drop.csv", index=False)
    print("No pairs with |corr| >=", THRESH, "- drop list is empty.")
else:
    # Build adjacency for connected components
    adj = defaultdict(set)
    for a, b in pairs:
        adj[a].add(b)
        adj[b].add(a)

    visited = set()
    to_drop = set()

    # For each connected component, keep the single best IV, drop the rest
    for start in list(adj.keys()):
        if start in visited:
            continue
        comp = set()
        q = [start]
        while q:
            u = q.pop()
            if u in visited:
                continue
            visited.add(u)
            comp.add(u)
            q.extend(v for v in adj[u] if v not in visited)

        # pick the best by (IV, then name) to break ties deterministically
        keep = max(comp, key=lambda f: (get_iv(f), f))
        to_drop |= (comp - {keep})

    # Save
    pd.DataFrame(sorted(to_drop), columns=["feature"]).to_csv("features_to_drop.csv", index=False)
    print(f"Dropping {len(to_drop)} features. Saved to features_to_drop.csv")














import pandas as pd
import numpy as np
from pathlib import Path

# ---- CONFIG ----
CORR_CSV = "correlation_all_features_pearson.csv"
FI_CSV   = "feature_importance_summary.csv"
THRESH   = 0.90                  # absolute Pearson threshold
OUT_DROP = "features_to_drop_corr090.csv"

# ---- load correlation matrix (square) ----
corr = pd.read_csv(CORR_CSV)
# assume first column is the index with feature names if not already
if corr.columns[0] != corr.index.name:
    # try to set index from first column if it’s the feature name column
    if corr.columns[0] not in corr.columns[1:]:
        corr = corr.set_index(corr.columns[0])

# sanity: make symmetric and zero self-corr
corr = corr.astype(float)
np.fill_diagonal(corr.values, 0.0)

# melt into pairs (i < j) to avoid duplicates
pairs = (
    corr.stack()
         .rename("corr")
         .reset_index(names=["f1", "f2"])
)
pairs = pairs[pairs["f1"] < pairs["f2"]]              # i<j
pairs = pairs[pairs["corr"].abs() >= THRESH]          # strong correlations only

# ---- load feature importance ----
fi = pd.read_csv(FI_CSV)

# pick an "importance" score:
# 1) average of all columns starting with "gain_pct"
gain_pct_cols = [c for c in fi.columns if c.startswith("gain_pct")]
if gain_pct_cols:
    fi["importance"] = fi[gain_pct_cols].mean(axis=1)
else:
    # 2) else average of all columns starting with "gain"
    gain_cols = [c for c in fi.columns if c.startswith("gain")]
    if gain_cols:
        fi["importance"] = fi[gain_cols].mean(axis=1)
    else:
        # 3) fallback: if only a single 'gain' or 'importance' available, try those
        for cand in ("importance","gain","Gain","gain_mean"):
            if cand in fi.columns:
                fi["importance"] = fi[cand]
                break
        if "importance" not in fi.columns:
            raise ValueError("Could not find importance columns (gain_pct*/gain*/importance) in FI file.")

fi = fi[["feature","importance"]].drop_duplicates()
imp = dict(zip(fi["feature"], fi["importance"]))

# default importance (very small) for any feature missing in FI
def get_imp(f): 
    v = imp.get(f)
    return float(v) if pd.notnull(v) else -1e9

# ---- build neighbor graph for correlation ≥ THRESH ----
from collections import defaultdict
nbrs = defaultdict(set)
for _, r in pairs.iterrows():
    a, b = r["f1"], r["f2"]
    nbrs[a].add(b)
    nbrs[b].add(a)

all_feats = set(corr.index.tolist())
# sort all features by importance (desc), tie-break by name (asc)
ordered = sorted(all_feats, key=lambda f: (-get_imp(f), f))

keep, drop = set(), set()

# greedy: walk in importance order; keep first in each correlated cluster, drop its neighbors
for f in ordered:
    if f in drop or f in keep:
        continue
    keep.add(f)
    for n in nbrs.get(f, []):
        if n not in keep:
            drop.add(n)

# Optionally: restrict to only features present in your model input (i.e., present in FI)
# drop = [f for f in drop if f in imp]

# ---- save & show ----
drop_list = pd.DataFrame({"feature_to_drop": sorted(drop)})
drop_list.to_csv(OUT_DROP, index=False)
print(f"Dropping {len(drop)} features at |corr| ≥ {THRESH}. Saved to {OUT_DROP}.")

# A short summary:
print("\nTop 20 kept features (by importance) for transparency:")
print(pd.DataFrame({"feature": list(keep)[:20],
                    "importance": [get_imp(f) for f in list(keep)[:20]]}).sort_values("importance", ascending=False))

# If you want to see a sample of conflicts that caused drops:
if not pairs.empty:
    # join pairs with importance to show which side won
    tmp = pairs.copy()
    tmp["imp_f1"] = tmp["f1"].map(get_imp)
    tmp["imp_f2"] = tmp["f2"].map(get_imp)
    tmp["kept"]   = np.where(tmp["imp_f1"] >= tmp["imp_f2"], tmp["f1"], tmp["f2"])
    tmp["dropped"]= np.where(tmp["imp_f1"] >= tmp["imp_f2"], tmp["f2"], tmp["f1"])
    # only show those actually dropped by the greedy pass
    sample = tmp[tmp["dropped"].isin(drop)].sort_values("corr", key=np.abs, ascending=False).head(25)
    if not sample.empty:
        print("\nExamples of highly correlated pairs where the lower-importance feature was dropped:")
        print(sample[["f1","f2","corr","imp_f1","imp_f2","kept","dropped"]].to_string(index=False))

