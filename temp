# rcif/…/application_history_functions.py (top of file or near other helpers)

# --- REPLACE ONLY the body of get_oao_partitions_covering with this version ---
from datetime import datetime, timedelta
from dateutil.relativedelta import relativedelta

def get_oao_partitions_covering(
    spark, table: str, first_date: str, last_date: str
) -> List[str]:
    """
    Returns the minimal list of partition dates such that the union of their
    3-year snapshots covers [first_date, last_date].

    Implementation note:
    - Avoids scanning the table for all distinct partition dates.
    - Uses get_closest_partition(spark, table, target_dt, get_after_date=True)
      iteratively to hop backward in ~3-year steps.
    """
    picked: List[str] = []

    # seed with the partition on/after last_date
    p = get_closest_partition(spark, table, last_date, get_after_date=True)
    if not p:
        raise ValueError(f"No partition on/after {last_date} for {table}")
    picked.append(p)

    # walk backward until the 3-year coverage reaches first_date
    # cap iterations defensively (should stay well under this)
    MAX_HOPS = 24

    for _ in range(MAX_HOPS):
        p_dt = datetime.strptime(p, "%Y-%m-%d")
        # this snapshot covers ~3 years before p_dt (inclusive).
        # target the first date just beyond that coverage to continue walking back.
        next_target = (p_dt - relativedelta(years=3)).strftime("%Y-%m-%d")

        # if we've now covered the desired start, we're done
        if next_target <= first_date:
            break

        q = get_closest_partition(spark, table, next_target, get_after_date=True)
        if not q:
            # no older partition found—stop to avoid looping
            break
        if q == picked[-1]:
            # guard against accidental repeats
            break

        picked.append(q)
        p = q

    return picked



oao_dt1_list = _oao_partitions_covering(spark, "sl1_oao.tbl_application_h",     first_date, last_date)
oao_dt2_list = _oao_partitions_covering(spark, "sl1_oao.tbl_customerproducts_h", first_date, last_date)
oao_dt3_list = _oao_partitions_covering(spark, "sl1_oao.tbl_products_h",         first_date, last_date)
oao_dt4_list = _oao_partitions_covering(spark, "sl1_oao.tbl_producttype_h",      first_date, last_date)

# and use .isin(oao_dtX_list) in your filters
