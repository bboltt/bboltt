import pandas as pd
import numpy as np
from collections import defaultdict, deque

CORR_CSV = "..."   # your path
IV_CSV   = "..."   # your path
THRESH   = 0.9
LABEL_COL = "fraud_label_180"

# --- read correlation matrix ---
corr = pd.read_csv(CORR_CSV)
if corr.columns[0] not in corr.columns[1:]:
    corr = corr.set_index(corr.columns[0])

common = corr.index.intersection(corr.columns)
corr = corr.loc[common, common]

if LABEL_COL in corr.index:
    corr = corr.drop(index=LABEL_COL, columns=LABEL_COL, errors="ignore")

corr = corr.astype(float)
np.fill_diagonal(corr.values, 0.0)

# --- read IV file and map ---
iv = pd.read_csv(IV_CSV)
iv = iv.rename(columns={c: c.strip() for c in iv.columns})
iv["Variable"] = iv["Variable"].astype(str).str.strip()
iv["IV"] = pd.to_numeric(iv["IV"], errors="coerce")
iv_map = dict(zip(iv["Variable"], iv["IV"]))

def get_iv(f):
    v = iv_map.get(f)
    return -1e9 if pd.isna(v) else float(v)

# --- build high-|corr| pairs (keep your 'pairs' name) ---
arr = corr.values
cols = corr.columns.to_list()
I, J = np.where(np.triu(np.ones(arr.shape, dtype=bool), k=1) & (np.abs(arr) >= THRESH))

pairs = [(cols[i], cols[j]) for i, j in zip(I, J)]  # unchanged shape
# keep a lookup of corr values for audit (new, non-disruptive)
edge_corr = {frozenset((f1, f2)): float(corr.loc[f1, f2]) for f1, f2 in pairs}

if not pairs:
    print("No pairs with |corr| >=", THRESH)
    pd.DataFrame([], columns=["feature"]).to_csv("features_to_drop.csv", index=False)
    pd.DataFrame([], columns=["feature_to_drop","kept_feature","corr_with_kept","related_features"])\
      .to_csv("features_to_drop_audit.csv", index=False)
else:
    # adjacency (same variable names as before)
    adj = defaultdict(set)
    for f1, f2 in pairs:
        adj[f1].add(f2)
        adj[f2].add(f1)

    visited = set()
    to_drop = set()
    audit_rows = []  # new: record per-feature details

    # for each connected component, keep single best IV, drop the rest
    for start in list(adj.keys()):
        if start in visited:
            continue
        comp = set()
        q = deque([start])
        while q:
            u = q.popleft()
            if u in visited:
                continue
            visited.add(u)
            comp.add(u)
            for v in adj[u]:
                if v not in visited:
                    q.append(v)

        keep = max(comp, key=lambda f: (get_iv(f), f))
        drops = sorted(comp - {keep})
        to_drop |= set(drops)

        # record: for each drop, include kept feature, corr value, and related features in the component
        for f in drops:
            # prefer the edge corr if it was one of the thresholded pairs; otherwise use raw corr matrix value
            corr_val = edge_corr.get(frozenset((f, keep)), float(corr.loc[f, keep]))
            related = ", ".join(sorted(comp - {f}))
            audit_rows.append({
                "feature_to_drop": f,
                "kept_feature": keep,
                "corr_with_kept": corr_val,
                "related_features": related,
            })

    # outputs
    pd.DataFrame(sorted(to_drop), columns=["feature"]).to_csv("features_to_drop.csv", index=False)
    pd.DataFrame(audit_rows).to_csv("features_to_drop_audit.csv", index=False)

    print(f"Dropping {len(to_drop)} features. Wrote features_to_drop.csv and features_to_drop_audit.csv")













import pandas as pd
import numpy as np
from pathlib import Path

# ---- CONFIG ----
CORR_CSV = "correlation_all_features_pearson.csv"
FI_CSV   = "feature_importance_summary.csv"
THRESH   = 0.90                  # absolute Pearson threshold
OUT_DROP = "features_to_drop_corr090.csv"

# ---- load correlation matrix (square) ----
corr = pd.read_csv(CORR_CSV)
# assume first column is the index with feature names if not already
if corr.columns[0] != corr.index.name:
    # try to set index from first column if it’s the feature name column
    if corr.columns[0] not in corr.columns[1:]:
        corr = corr.set_index(corr.columns[0])

# sanity: make symmetric and zero self-corr
corr = corr.astype(float)
np.fill_diagonal(corr.values, 0.0)

# melt into pairs (i < j) to avoid duplicates
pairs = (
    corr.stack()
         .rename("corr")
         .reset_index(names=["f1", "f2"])
)
pairs = pairs[pairs["f1"] < pairs["f2"]]              # i<j
pairs = pairs[pairs["corr"].abs() >= THRESH]          # strong correlations only

# ---- load feature importance ----
fi = pd.read_csv(FI_CSV)

# pick an "importance" score:
# 1) average of all columns starting with "gain_pct"
gain_pct_cols = [c for c in fi.columns if c.startswith("gain_pct")]
if gain_pct_cols:
    fi["importance"] = fi[gain_pct_cols].mean(axis=1)
else:
    # 2) else average of all columns starting with "gain"
    gain_cols = [c for c in fi.columns if c.startswith("gain")]
    if gain_cols:
        fi["importance"] = fi[gain_cols].mean(axis=1)
    else:
        # 3) fallback: if only a single 'gain' or 'importance' available, try those
        for cand in ("importance","gain","Gain","gain_mean"):
            if cand in fi.columns:
                fi["importance"] = fi[cand]
                break
        if "importance" not in fi.columns:
            raise ValueError("Could not find importance columns (gain_pct*/gain*/importance) in FI file.")

fi = fi[["feature","importance"]].drop_duplicates()
imp = dict(zip(fi["feature"], fi["importance"]))

# default importance (very small) for any feature missing in FI
def get_imp(f): 
    v = imp.get(f)
    return float(v) if pd.notnull(v) else -1e9

# ---- build neighbor graph for correlation ≥ THRESH ----
from collections import defaultdict
nbrs = defaultdict(set)
for _, r in pairs.iterrows():
    a, b = r["f1"], r["f2"]
    nbrs[a].add(b)
    nbrs[b].add(a)

all_feats = set(corr.index.tolist())
# sort all features by importance (desc), tie-break by name (asc)
ordered = sorted(all_feats, key=lambda f: (-get_imp(f), f))

keep, drop = set(), set()

# greedy: walk in importance order; keep first in each correlated cluster, drop its neighbors
for f in ordered:
    if f in drop or f in keep:
        continue
    keep.add(f)
    for n in nbrs.get(f, []):
        if n not in keep:
            drop.add(n)

# Optionally: restrict to only features present in your model input (i.e., present in FI)
# drop = [f for f in drop if f in imp]

# ---- save & show ----
drop_list = pd.DataFrame({"feature_to_drop": sorted(drop)})
drop_list.to_csv(OUT_DROP, index=False)
print(f"Dropping {len(drop)} features at |corr| ≥ {THRESH}. Saved to {OUT_DROP}.")

# A short summary:
print("\nTop 20 kept features (by importance) for transparency:")
print(pd.DataFrame({"feature": list(keep)[:20],
                    "importance": [get_imp(f) for f in list(keep)[:20]]}).sort_values("importance", ascending=False))

# If you want to see a sample of conflicts that caused drops:
if not pairs.empty:
    # join pairs with importance to show which side won
    tmp = pairs.copy()
    tmp["imp_f1"] = tmp["f1"].map(get_imp)
    tmp["imp_f2"] = tmp["f2"].map(get_imp)
    tmp["kept"]   = np.where(tmp["imp_f1"] >= tmp["imp_f2"], tmp["f1"], tmp["f2"])
    tmp["dropped"]= np.where(tmp["imp_f1"] >= tmp["imp_f2"], tmp["f2"], tmp["f1"])
    # only show those actually dropped by the greedy pass
    sample = tmp[tmp["dropped"].isin(drop)].sort_values("corr", key=np.abs, ascending=False).head(25)
    if not sample.empty:
        print("\nExamples of highly correlated pairs where the lower-importance feature was dropped:")
        print(sample[["f1","f2","corr","imp_f1","imp_f2","kept","dropped"]].to_string(index=False))

