for segment, tbl in prefix_to_table.items():
    c = (
        spark.table(tbl)
        .where(F.col('ods_business_dt').between(F.lit(first_date_cp), F.lit(last_date_cp)))
        .where(F.col('am00_date_close').isNotNull())
        .select(
            F.col('gsam_appl_num'),
            F.to_date(F.col('am00_date_close')).alias('cp_acct_dt_closed'),
            F.col('ods_business_dt').alias('ods_business_dt_cc'),
        ).alias('c')
    )

    p = all_custs_pop.alias('p')

    joined = (
        p.join(
            c,
            (F.col('p.prodcode') == F.lit('CC')) &
            (F.substring(F.col('p.account_num'), 1, 7) == F.lit(segment)) &
            (F.substring(F.col('p.account_num'), -9, 9) == F.col('c.gsam_appl_num')) &
            (F.col('p.ods_business_dt') == F.col('c.ods_business_dt_cc')),
            'left'
        )
        # carry left columns, plus right-side fields ONLY as temps
        .select('p.*',
                F.col('c.cp_acct_dt_closed').alias('__cp_close'),
                F.col('c.ods_business_dt_cc').alias('__ods_cc'))
        .withColumn(
            'acct_dt_closed',
            F.when(F.col('__cp_close').isNotNull(), F.col('__cp_close'))
             .otherwise(F.col('acct_dt_closed'))
        )
        .withColumn(
            'acct_open',
            F.when(F.col('__cp_close').isNotNull(), F.lit(0))
             .otherwise(F.col('acct_open'))
        )
    )

    # create or update the single canonical ods_business_dt_cc
    if 'ods_business_dt_cc' in joined.columns:
        # overwrite with coalesce so previously-set values persist
        joined = joined.withColumn(
            'ods_business_dt_cc',
            F.coalesce(F.col('ods_business_dt_cc'), F.col('__ods_cc'))
        )
    else:
        joined = joined.withColumn('ods_business_dt_cc', F.col('__ods_cc'))

    # drop temps so nothing ambiguous survives to next iteration
    all_custs_pop = joined.drop('__cp_close', '__ods_cc')

