```python
# credit_card_features.py
from pyspark.sql import SparkSession, DataFrame, functions as F
from typing import Optional
from functools import reduce


def cc_balance_history(
    spark: SparkSession,
    first_date: str,
    last_date: str,
    acct_df: Optional[DataFrame] = None
) -> DataFrame:
    """
    Extract and aggregate full balance history (AM02) for credit-card accounts between first_date and last_date.
    Optionally filters to the accounts in acct_df.
    Transforms date fields into days-since (relative to ods_business_dt).
    """
    df = (
        spark.table("s11_cp_cnsum.am02_h")
             .filter(F.col("ods_business_dt").between(first_date, last_date))
             .filter(F.col("prodcode") == "CC")
             .filter(F.substring(F.col("account_num"), 1, 1) == "C")
             .select(
                 F.col("gsam_appl_num").cast("bigint").alias("account_num"),
                 "ods_business_dt",
                 "am02_date_potential_chargeoff",
                 "am02_date_chargeoff_reinstated",
                 "am02_date_first_use",
                 "am02_date_last_nsf_payment",
                 "am02_date_last_paid_balance",
                 "am02_date_last_payment",
                 "am02_balance_prev_statement",
                 "am02_balance_current",
                 "am02_balance_high_ltd",
                 "am02_num_lte_fee_since_lst_cur",
                 "am02_num_ol_fee_lst_w_in_limit",
                 "am02_num_nsf_payments_ltd",
                 "am02_amt_last_nsf_payment",
                 "am02_num_ptp_broken_ltd",
                 "am02_amt_ptp_broken_ltd",
                 "am02_num_ptp_kept_ltd",
                 "am02_amt_ptp_kept_ltd",
                 "am02_num_ptp_partial_ltd",
                 "am02_amt_ptp_partial_ltd",
                 "am02_mths_consecutive_min_pay"
             )
    )
    if acct_df is not None:
        df = df.join(
            acct_df.select(F.col("account_num").cast("bigint")).distinct(),
            on="account_num", how="inner"
        )

    # convert date fields into days-since
    date_cols = [
        "am02_date_potential_chargeoff",
        "am02_date_chargeoff_reinstated",
        "am02_date_first_use",
        "am02_date_last_nsf_payment",
        "am02_date_last_paid_balance",
        "am02_date_last_payment"
    ]
    for col_name in date_cols:
        df = df.withColumn(
            f"ds_{col_name}",
            F.datediff(F.col("ods_business_dt"), F.col(col_name))
        ).drop(col_name)

    # aggregate per account
    return (
        df.groupBy("account_num")
          .agg(
              F.min("ds_am02_date_potential_chargeoff").alias("cc_days_since_potential_chargeoff"),
              F.min("ds_am02_date_chargeoff_reinstated").alias("cc_days_since_chargeoff_reinstated"),
              F.min("ds_am02_date_first_use").alias("cc_days_since_first_use"),
              F.min("ds_am02_date_last_nsf_payment").alias("cc_days_since_last_nsf"),
              F.min("ds_am02_date_last_paid_balance").alias("cc_days_since_last_paid"),
              F.min("ds_am02_date_last_payment").alias("cc_days_since_last_payment"),

              F.max("am02_balance_current").alias("cc_bal_current_max"),
              F.max("am02_balance_prev_statement").alias("cc_bal_prev_max"),
              F.max("am02_balance_high_ltd").alias("cc_bal_high_ltd_max"),

              F.sum("am02_num_lte_fee_since_lst_cur").alias("cc_latefee_count"),
              F.sum("am02_num_ol_fee_lst_w_in_limit").alias("cc_overlimit_event_count"),
              F.sum("am02_num_nsf_payments_ltd").alias("cc_nsf_count"),
              F.max("am02_amt_last_nsf_payment").alias("cc_last_nsf_amt"),

              F.sum("am02_num_ptp_broken_ltd").alias("cc_ptp_broken_count"),
              F.sum("am02_amt_ptp_broken_ltd").alias("cc_ptp_broken_amt"),
              F.sum("am02_num_ptp_kept_ltd").alias("cc_ptp_kept_count"),
              F.sum("am02_amt_ptp_kept_ltd").alias("cc_ptp_kept_amt"),
              F.sum("am02_num_ptp_partial_ltd").alias("cc_ptp_partial_count"),
              F.sum("am02_amt_ptp_partial_ltd").alias("cc_ptp_partial_amt"),

              F.max("am02_mths_consecutive_min_pay").alias("cc_months_min_pay_max")
          )
    )


def cc_pd_history(
    spark: SparkSession,
    first_date: str,
    last_date: str,
    acct_df: Optional[DataFrame] = None
) -> DataFrame:
    """
    Extract and aggregate past-due / overlimit / PTP events (AM04) between first_date and last_date.
    """
    df = (
        spark.table("s11_cp_cnsum.am04_P_h")
             .filter(F.col("ods_business_dt").between(first_date, last_date))
             .filter(F.col("prodcode") == "CC")
             .filter(F.substring(F.col("account_num"), 1, 1) == "C")
             .select(
                 F.col("gsam_appl_num").cast("bigint").alias("account_num"),
                 "ods_business_dt",
                 "am04_date_most_recent_pd",
                 "am04_amt_most_recent_pd",
                 "am04_date_high_balance_pastdue",
                 "am04_balance_high_pastdue",
                 "am04_cons_days_pastdue"
             )
    )
    if acct_df is not None:
        df = df.join(
            acct_df.select(F.col("account_num").cast("bigint")).distinct(),
            on="account_num", how="inner"
        )

    date_cols = ["am04_date_most_recent_pd", "am04_date_high_balance_pastdue"]
    for col_name in date_cols:
        df = df.withColumn(
            f"ds_{col_name}",
            F.datediff(F.col("ods_business_dt"), F.col(col_name))
        ).drop(col_name)

    return (
        df.groupBy("account_num")
          .agg(
              F.min("ds_am04_date_most_recent_pd").alias("cc_days_since_last_pd"),
              F.max("am04_amt_most_recent_pd").alias("cc_amt_most_recent_pd"),
              F.min("ds_am04_date_high_balance_pastdue").alias("cc_days_since_high_bal_pd"),
              F.max("am04_balance_high_pastdue").alias("cc_bal_high_pd_max"),
              F.max("am04_cons_days_pastdue").alias("cc_cons_pd_days_max")
          )
    )


def cc_status_history(
    spark: SparkSession,
    first_date: str,
    last_date: str,
    acct_df: Optional[DataFrame] = None
) -> DataFrame:
    """
    Extract and aggregate account-status and fraud flags (AM00) between first_date and last_date.
    """
    df = (
        spark.table("s11_cp_cnsum.am00_h")
             .filter(F.col("ods_business_dt").between(first_date, last_date))
             .filter(F.col("prodcode") == "CC")
             .filter(F.substring(F.col("account_num"), 1, 1) == "C")
             .select(
                 F.col("gsam_appl_num").cast("bigint").alias("account_num"),
                 "ods_business_dt",
                 "am00_date_fraud_flag",
                 "am00_statf_fraud",
                 "am00_date_security_fraud_stat",
                 "am00_statc_security_fraud",
                 "am00_paid_in_full_flag",
                 "am00_statf_highly_active",
                 "am00_statf_active_since_opened",
                 "am00_inactive_flag",
                 "am00_statf_declined_reissue",
                 "am00_statc_chargeoff",
                 "am00_statf_potential_chargeoff",
                 "am00_typec_vip",
                 "am00_statc_current_overlimit",
                 "am00_statc_current_past_due"
             )
    )
    if acct_df is not None:
        df = df.join(
            acct_df.select(F.col("account_num").cast("bigint")).distinct(),
            on="account_num", how="inner"
        )

    for col_name in ["am00_date_fraud_flag", "am00_date_security_fraud_stat"]:
        df = df.withColumn(
            f"ds_{col_name}",
            F.datediff(F.col("ods_business_dt"), F.col(col_name))
        ).drop(col_name)

    return (
        df.groupBy("account_num")
          .agg(
              F.min("ds_am00_date_fraud_flag").alias("cc_days_since_fraud_flag"),
              F.when(F.max("am00_statf_fraud") == "Y", 1).otherwise(0)
               .alias("cc_fraud_flag"),
              F.min("ds_am00_date_security_fraud_stat")
               .alias("cc_days_since_security_flag"),
              F.when(F.max("am00_statf_potential_chargeoff") == "Y", 1).otherwise(0)
               .alias("cc_potential_chargeoff_flag"),
              F.when(F.max("am00_statc_current_overlimit").isNotNull(), 1).otherwise(0)
               .alias("cc_overlimit_flag"),
              F.when(F.max("am00_statc_current_past_due").isNotNull(), 1).otherwise(0)
               .alias("cc_past_due_flag"),
              F.when(F.max("am00_statf_highly_active") == "Y", 1).otherwise(0)
               .alias("cc_highly_active_flag"),
              F.when(F.max("am00_statf_active_since_opened") == "Y", 1).otherwise(0)
               .alias("cc_ever_active_flag"),
              F.when(F.max("am00_typec_vip") == "Y", 1).otherwise(0)
               .alias("cc_vip_flag")
          )
    )


def get_cc_features(
    spark: SparkSession,
    first_balance_lookup_dt: str,
    last_lookup_dt: str,
    pop_df: DataFrame,
    first_rcif_lookup_dt: Optional[str] = None
) -> DataFrame:
    """
    Combine balance, past-due and status features for all credit-card accounts in pop_df.
    Determines acct_df from pop_df (or via SSN-based RCIF lookup), normalizes account_num for joins,
    then calls cc_balance_history, cc_pd_history, cc_status_history with that acct_df.
    """
    # Determine the set of accounts to use
    if "account_num" not in pop_df.columns:
        acct_df = get_pop_accounts(
            spark,
            first_lookup_dt=first_rcif_lookup_dt,
            last_lookup_dt=last_lookup_dt,
            pop_df=pop_df
        )
    else:
        acct_df = pop_df

    # Filter and normalize acct_df for CC products, extracting last 9 digits as bigint
    acct_df = (
        acct_df
        .filter(F.col("prodcode") == "CC")
        .filter(F.substring(F.col("account_num"), 1, 7) == "0000011")
        .withColumn(
            "account_num",
            F.substring(F.col("account_num"), -9, 9).cast("bigint")
        )
    )

    # Generate feature sets
    bal_feats = cc_balance_history(spark, first_balance_lookup_dt, last_lookup_dt, acct_df)
    pd_feats  = cc_pd_history(spark, first_balance_lookup_dt, last_lookup_dt, acct_df)
    st_feats  = cc_status_history(spark, first_balance_lookup_dt, last_lookup_dt, acct_df)

    # Combine features
    feat_df = reduce(
        lambda left, right: left.join(right, "account_num", "left"),
        [bal_feats, pd_feats, st_feats]
    )
    return feat_df
```
















WITH rf_closed AS (
  SELECT
    LPAD(default.decrypt(rfact_acct), 18, '0')    AS account_num,
    ods_business_dt,
    rfact_dt_closed
  FROM s11_rf.rfsactu_h
  WHERE rfact_dt_closed IS NOT NULL
),
mg_closed AS (
  SELECT
    LPAD(default.decrypt(loan_num), 18, '0')      AS account_num,
    ods_business_dt,
    delq_payment_count
  FROM s11_mg.master_all_h
  WHERE delq_payment_count IS NOT NULL
)

SELECT
  c.account_num,
  c.rfact_dt_closed,
  c.ods_business_dt    AS rf_business_dt,
  m.delq_payment_count,
  m.ods_business_dt    AS mg_business_dt
FROM rf_closed AS c
LEFT JOIN mg_closed AS m
  ON m.account_num     = c.account_num
 AND m.ods_business_dt = c.ods_business_dt
;
