```python
# credit_card_features.py
from pyspark.sql import SparkSession, DataFrame, functions as F
from typing import Optional
from functools import reduce


def cc_balance_history(
    spark: SparkSession,
    first_date: str,
    last_date: str,
    acct_df: Optional[DataFrame] = None
) -> DataFrame:
    """
    Extract and aggregate full balance history (AM02) for credit-card accounts between first_date and last_date.
    Optionally filters to the accounts in acct_df.
    Transforms date fields into days-since (relative to ods_business_dt).
    """
    df = (
        spark.table("s11_cp_cnsum.am02_h")
             .filter(F.col("ods_business_dt").between(first_date, last_date))
             .filter(F.col("prodcode") == "CC")
             .filter(F.substring(F.col("account_num"), 1, 1) == "C")
             .select(
                 F.col("gsam_appl_num").cast("bigint").alias("account_num"),
                 "ods_business_dt",
                 "am02_date_potential_chargeoff",
                 "am02_date_chargeoff_reinstated",
                 "am02_date_first_use",
                 "am02_date_last_nsf_payment",
                 "am02_date_last_paid_balance",
                 "am02_date_last_payment",
                 "am02_balance_prev_statement",
                 "am02_balance_current",
                 "am02_balance_high_ltd",
                 "am02_num_lte_fee_since_lst_cur",
                 "am02_num_ol_fee_lst_w_in_limit",
                 "am02_num_nsf_payments_ltd",
                 "am02_amt_last_nsf_payment",
                 "am02_num_ptp_broken_ltd",
                 "am02_amt_ptp_broken_ltd",
                 "am02_num_ptp_kept_ltd",
                 "am02_amt_ptp_kept_ltd",
                 "am02_num_ptp_partial_ltd",
                 "am02_amt_ptp_partial_ltd",
                 "am02_mths_consecutive_min_pay"
             )
    )
    if acct_df is not None:
        df = df.join(
            acct_df.select(F.col("account_num").cast("bigint")).distinct(),
            on="account_num", how="inner"
        )

    # convert date fields into days-since
    date_cols = [
        "am02_date_potential_chargeoff",
        "am02_date_chargeoff_reinstated",
        "am02_date_first_use",
        "am02_date_last_nsf_payment",
        "am02_date_last_paid_balance",
        "am02_date_last_payment"
    ]
    for col_name in date_cols:
        df = df.withColumn(
            f"ds_{col_name}",
            F.datediff(F.col("ods_business_dt"), F.col(col_name))
        ).drop(col_name)

    # aggregate per account
    return (
        df.groupBy("account_num")
          .agg(
              F.min("ds_am02_date_potential_chargeoff").alias("cc_days_since_potential_chargeoff"),
              F.min("ds_am02_date_chargeoff_reinstated").alias("cc_days_since_chargeoff_reinstated"),
              F.min("ds_am02_date_first_use").alias("cc_days_since_first_use"),
              F.min("ds_am02_date_last_nsf_payment").alias("cc_days_since_last_nsf"),
              F.min("ds_am02_date_last_paid_balance").alias("cc_days_since_last_paid"),
              F.min("ds_am02_date_last_payment").alias("cc_days_since_last_payment"),

              F.max("am02_balance_current").alias("cc_bal_current_max"),
              F.max("am02_balance_prev_statement").alias("cc_bal_prev_max"),
              F.max("am02_balance_high_ltd").alias("cc_bal_high_ltd_max"),

              F.sum("am02_num_lte_fee_since_lst_cur").alias("cc_latefee_count"),
              F.sum("am02_num_ol_fee_lst_w_in_limit").alias("cc_overlimit_event_count"),
              F.sum("am02_num_nsf_payments_ltd").alias("cc_nsf_count"),
              F.max("am02_amt_last_nsf_payment").alias("cc_last_nsf_amt"),

              F.sum("am02_num_ptp_broken_ltd").alias("cc_ptp_broken_count"),
              F.sum("am02_amt_ptp_broken_ltd").alias("cc_ptp_broken_amt"),
              F.sum("am02_num_ptp_kept_ltd").alias("cc_ptp_kept_count"),
              F.sum("am02_amt_ptp_kept_ltd").alias("cc_ptp_kept_amt"),
              F.sum("am02_num_ptp_partial_ltd").alias("cc_ptp_partial_count"),
              F.sum("am02_amt_ptp_partial_ltd").alias("cc_ptp_partial_amt"),

              F.max("am02_mths_consecutive_min_pay").alias("cc_months_min_pay_max")
          )
    )


def cc_pd_history(
    spark: SparkSession,
    first_date: str,
    last_date: str,
    acct_df: Optional[DataFrame] = None
) -> DataFrame:
    """
    Extract and aggregate past-due / overlimit / PTP events (AM04) between first_date and last_date.
    """
    df = (
        spark.table("s11_cp_cnsum.am04_P_h")
             .filter(F.col("ods_business_dt").between(first_date, last_date))
             .filter(F.col("prodcode") == "CC")
             .filter(F.substring(F.col("account_num"), 1, 1) == "C")
             .select(
                 F.col("gsam_appl_num").cast("bigint").alias("account_num"),
                 "ods_business_dt",
                 "am04_date_most_recent_pd",
                 "am04_amt_most_recent_pd",
                 "am04_date_high_balance_pastdue",
                 "am04_balance_high_pastdue",
                 "am04_cons_days_pastdue"
             )
    )
    if acct_df is not None:
        df = df.join(
            acct_df.select(F.col("account_num").cast("bigint")).distinct(),
            on="account_num", how="inner"
        )

    date_cols = ["am04_date_most_recent_pd", "am04_date_high_balance_pastdue"]
    for col_name in date_cols:
        df = df.withColumn(
            f"ds_{col_name}",
            F.datediff(F.col("ods_business_dt"), F.col(col_name))
        ).drop(col_name)

    return (
        df.groupBy("account_num")
          .agg(
              F.min("ds_am04_date_most_recent_pd").alias("cc_days_since_last_pd"),
              F.max("am04_amt_most_recent_pd").alias("cc_amt_most_recent_pd"),
              F.min("ds_am04_date_high_balance_pastdue").alias("cc_days_since_high_bal_pd"),
              F.max("am04_balance_high_pastdue").alias("cc_bal_high_pd_max"),
              F.max("am04_cons_days_pastdue").alias("cc_cons_pd_days_max")
          )
    )


def cc_status_history(
    spark: SparkSession,
    first_date: str,
    last_date: str,
    acct_df: Optional[DataFrame] = None
) -> DataFrame:
    """
    Extract and aggregate account-status and fraud flags (AM00) between first_date and last_date.
    """
    df = (
        spark.table("s11_cp_cnsum.am00_h")
             .filter(F.col("ods_business_dt").between(first_date, last_date))
             .filter(F.col("prodcode") == "CC")
             .filter(F.substring(F.col("account_num"), 1, 1) == "C")
             .select(
                 F.col("gsam_appl_num").cast("bigint").alias("account_num"),
                 "ods_business_dt",
                 "am00_date_fraud_flag",
                 "am00_statf_fraud",
                 "am00_date_security_fraud_stat",
                 "am00_statc_security_fraud",
                 "am00_paid_in_full_flag",
                 "am00_statf_highly_active",
                 "am00_statf_active_since_opened",
                 "am00_inactive_flag",
                 "am00_statf_declined_reissue",
                 "am00_statc_chargeoff",
                 "am00_statf_potential_chargeoff",
                 "am00_typec_vip",
                 "am00_statc_current_overlimit",
                 "am00_statc_current_past_due"
             )
    )
    if acct_df is not None:
        df = df.join(
            acct_df.select(F.col("account_num").cast("bigint")).distinct(),
            on="account_num", how="inner"
        )

    for col_name in ["am00_date_fraud_flag", "am00_date_security_fraud_stat"]:
        df = df.withColumn(
            f"ds_{col_name}",
            F.datediff(F.col("ods_business_dt"), F.col(col_name))
        ).drop(col_name)

    return (
        df.groupBy("account_num")
          .agg(
              F.min("ds_am00_date_fraud_flag").alias("cc_days_since_fraud_flag"),
              F.when(F.max("am00_statf_fraud") == "Y", 1).otherwise(0)
               .alias("cc_fraud_flag"),
              F.min("ds_am00_date_security_fraud_stat")
               .alias("cc_days_since_security_flag"),
              F.when(F.max("am00_statf_potential_chargeoff") == "Y", 1).otherwise(0)
               .alias("cc_potential_chargeoff_flag"),
              F.when(F.max("am00_statc_current_overlimit").isNotNull(), 1).otherwise(0)
               .alias("cc_overlimit_flag"),
              F.when(F.max("am00_statc_current_past_due").isNotNull(), 1).otherwise(0)
               .alias("cc_past_due_flag"),
              F.when(F.max("am00_statf_highly_active") == "Y", 1).otherwise(0)
               .alias("cc_highly_active_flag"),
              F.when(F.max("am00_statf_active_since_opened") == "Y", 1).otherwise(0)
               .alias("cc_ever_active_flag"),
              F.when(F.max("am00_typec_vip") == "Y", 1).otherwise(0)
               .alias("cc_vip_flag")
          )
    )


```python
# credit_card_features.py
from pyspark.sql import SparkSession, DataFrame, functions as F
from typing import Optional, List
from functools import reduce
from fraud_origination.customer_score.cs_v2.data_dev.features.training_data_utils import get_pop_accounts


def cc_balance_history(
    spark: SparkSession,
    first_date: str,
    last_date: str,
    acct_df: Optional[DataFrame] = None
) -> DataFrame:
    # ... same as before: account-level aggregates from AM02
    pass


def cc_pd_history(
    spark: SparkSession,
    first_date: str,
    last_date: str,
    acct_df: Optional[DataFrame] = None
) -> DataFrame:
    # ... same as before: account-level aggregates from AM04
    pass


def cc_status_history(
    spark: SparkSession,
    first_date: str,
    last_date: str,
    acct_df: Optional[DataFrame] = None
) -> DataFrame:
    # ... same as before: account-level aggregates from AM00
    pass


def get_cc_features(
    spark: SparkSession,
    first_balance_lookup_dt: str,
    last_lookup_dt: str,
    pop_df: DataFrame,
    first_rcif_lookup_dt: Optional[str] = None
) -> DataFrame:
    """
    Produce SSN-level credit-card features by:
      1. deriving acct_df from pop_df or RCIF
      2. normalizing account_num for CC products
      3. computing account-level features via history functions
      4. left-joining acct_df to each feature set to retain all accounts
      5. aggregating per SSN with custom logic
    """
    # 1. identify SSN column
    if 'customerssn_obfuscated' in pop_df.columns:
        ssn_col = 'customerssn_obfuscated'
    elif 'customerssn' in pop_df.columns:
        ssn_col = 'customerssn'
    else:
        ssn_col = 'account_num'

    # 2. derive acct_df
    if 'account_num' not in pop_df.columns:
        acct_df = get_pop_accounts(
            spark,
            first_lookup_dt=first_rcif_lookup_dt,
            last_lookup_dt=last_lookup_dt,
            pop_df=pop_df
        )
    else:
        acct_df = pop_df

    # normalize and filter for credit-card accounts
    acct_df = (
        acct_df
        .filter(F.col("prodcode") == "CC")
        .filter(F.substring(F.col("account_num"), 1, 7) == "0000011")
        .withColumn(
            "account_num",
            F.substring(F.col("account_num"), -9, 9).cast("bigint")
        )
        .select(ssn_col, "account_num")
    )

    # 3. compute account-level features
    bal_feats = cc_balance_history(spark, first_balance_lookup_dt, last_lookup_dt, acct_df)
    pd_feats  = cc_pd_history(spark, first_balance_lookup_dt, last_lookup_dt, acct_df)
    st_feats  = cc_status_history(spark, first_balance_lookup_dt, last_lookup_dt, acct_df)

    # 4. merge features to acct_df via left joins to keep all accounts
    acct_feat_df = (
        acct_df
        .join(bal_feats, on="account_num", how="left")
        .join(pd_feats,  on="account_num", how="left")
        .join(st_feats,  on="account_num", how="left")
    )

    # 5. aggregate to SSN level
    sum_features: List[str] = [
        'cc_bal_current_max', 'cc_bal_prev_max', 'cc_bal_high_ltd_max',
        'cc_latefee_count', 'cc_overlimit_event_count', 'cc_nsf_count',
        'cc_ptp_broken_count', 'cc_ptp_kept_count', 'cc_ptp_partial_count'
    ]
    max_features: List[str] = [
        'cc_last_nsf_amt', 'cc_ptp_broken_amt', 'cc_ptp_kept_amt', 'cc_ptp_partial_amt',
        'cc_months_min_pay_max', 'cc_amt_most_recent_pd', 'cc_bal_high_pd_max', 'cc_cons_pd_days_max'
    ]
    recency_features: List[str] = [
        'cc_days_since_potential_chargeoff', 'cc_days_since_chargeoff_reinstated',
        'cc_days_since_first_use', 'cc_days_since_last_nsf', 'cc_days_since_last_paid',
        'cc_days_since_last_payment', 'cc_days_since_last_pd', 'cc_days_since_high_bal_pd',
        'cc_days_since_fraud_flag', 'cc_days_since_security_flag'
    ]
    flag_features = [
        'cc_past_due_flag', 'cc_overlimit_flag', 'cc_fraud_flag',
        'cc_potential_chargeoff_flag', 'cc_highly_active_flag', 'cc_ever_active_flag'
    ]

    agg_exprs = []
    for c in sum_features:
        agg_exprs.append(F.sum(c).alias(c))
    for c in max_features:
        agg_exprs.append(F.max(c).alias(c))
    for c in recency_features:
        agg_exprs.append(F.min(c).alias(c))
    for c in flag_features:
        agg_exprs.append(F.max(c).alias(c))
    agg_exprs.append(F.max('cc_vip_level').alias('cc_vip_level'))

    return (
        acct_feat_df
        .groupBy(ssn_col)
        .agg(*agg_exprs)
    )
```

```
















WITH rf_closed AS (
  SELECT
    LPAD(default.decrypt(rfact_acct), 18, '0')    AS account_num,
    ods_business_dt,
    rfact_dt_closed
  FROM s11_rf.rfsactu_h
  WHERE rfact_dt_closed IS NOT NULL
),
mg_closed AS (
  SELECT
    LPAD(default.decrypt(loan_num), 18, '0')      AS account_num,
    ods_business_dt,
    delq_payment_count
  FROM s11_mg.master_all_h
  WHERE delq_payment_count IS NOT NULL
)

SELECT
  c.account_num,
  c.rfact_dt_closed,
  c.ods_business_dt    AS rf_business_dt,
  m.delq_payment_count,
  m.ods_business_dt    AS mg_business_dt
FROM rf_closed AS c
LEFT JOIN mg_closed AS m
  ON m.account_num     = c.account_num
 AND m.ods_business_dt = c.ods_business_dt
;
