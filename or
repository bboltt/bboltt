from pyspark.sql import functions as F

# assume you still have 'summary_df' and the joined df in scope

# find features with extreme skew
extreme_feats = (
    summary_df
    .filter(F.abs(F.col("skewness")) > 10)
    .select("feature")
    .rdd.flatMap(lambda row: row)
    .collect()
)

# inspect 1st and 99th percentiles
for feat in extreme_feats:
    q_low, q_high = df.approxQuantile(feat, [0.01, 0.99], 0.01)
    print(f"{feat}: 1% → {q_low:.3f}, 99% → {q_high:.3f}")






from pyspark.sql import SparkSession, functions as F
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType

spark = SparkSession.builder.appName("CC_Feature_Profiling_DateRange").getOrCreate()

start_date = "2024-01-01"
end_date   = "2024-06-30"

df_am04 = (
    spark.table("s11_cp_cnsum.am04_P_h")
         .filter((F.col("ods_business_dt") >= start_date) & (F.col("ods_business_dt") <= end_date))
         .select(
             "gsam_appl_num",
             "am04_date_most_recent_pd",
             "am04_amt_most_recent_pd",
             "am04_date_high_balance_pastdue",
             "am04_balance_high_pastdue",
             "am04_cons_days_pastdue"
         )
)

df_am02 = (
    spark.table("s11_cp_cnsum.am02_h")
         .filter((F.col("ods_business_dt") >= start_date) & (F.col("ods_business_dt") <= end_date))
         .select(
             "gsam_appl_num",
             "am02_date_potential_chargeoff",
             "am02_balance_prev_statement",
             "am02_date_chargeoff_reinstated",
             "am02_balance_current",
             "am02_balance_high_ltd",
             "am02_date_first_use",
             "am02_num_lte_fee_since_lst_cur",
             "am02_num_ol_fee_lst_w_in_limit",
             "am02_amt_last_nsf_payment",
             "am02_date_last_nsf_payment",
             "am02_num_nsf_payments_ltd",
             "am02_date_last_paid_balance",
             "am02_date_last_payment",
             "am02_amt_ptp_broken_ltd",
             "am02_num_ptp_broken_ltd",
             "am02_amt_ptp_kept_ltd",
             "am02_num_ptp_kept_ltd",
             "am02_amt_ptp_partial_ltd",
             "am02_num_ptp_partial_ltd",
             "am02_mths_consecutive_min_pay"
         )
)

df_am00 = (
    spark.table("s11_cp_cnsum.am00_h")
         .filter((F.col("ods_business_dt") >= start_date) & (F.col("ods_business_dt") <= end_date))
         .select(
             "gsam_appl_num",
             "am00_date_fraud_flag",
             "am00_statf_fraud",
             "am00_date_security_fraud_stat",
             "am00_statc_security_fraud",
             "am00_paid_in_full_flag",
             "am00_statf_highly_active",
             "am00_statf_active_since_opened",
             "am00_inactive_flag",
             "am00_statf_declined_reissue",
             "am00_statc_chargeoff",
             "am00_statf_potential_chargeoff",
             "am00_typec_vip",
             "am00_statc_current_overlimit",
             "am00_statc_current_past_due"
         )
)

df = df_am04.join(df_am02, "gsam_appl_num", "left") \
           .join(df_am00, "gsam_appl_num", "left")

all_cols     = [c for c,_ in df.dtypes if c != "gsam_appl_num"]
numeric_types= ("int","bigint","double","float","long","decimal")
numeric_cols = [c for c,t in df.dtypes if t in numeric_types]
n            = df.count()

rows = []
for c in all_cols:
    pct_missing = df.filter(F.col(c).isNull()).count() / n
    distinct    = df.select(F.countDistinct(c)).first()[0]
    if c in numeric_cols:
        stats = df.select(
            F.mean(c).alias("mean"),
            F.stddev(c).alias("stddev"),
            F.skewness(c).alias("skewness")
        ).first()
        mean, stddev, skewness = stats["mean"], stats["stddev"], stats["skewness"]
    else:
        mean = stddev = skewness = None
    rows.append((c, pct_missing, distinct, mean, stddev, skewness))

schema = StructType([
    StructField("feature",        StringType(), True),
    StructField("pct_missing",    DoubleType(), True),
    StructField("distinct_count", LongType(),   True),
    StructField("mean",           DoubleType(), True),
    StructField("stddev",         DoubleType(), True),
    StructField("skewness",       DoubleType(), True),
])

summary_df = spark.createDataFrame(rows, schema)
summary_df.orderBy(F.desc("pct_missing")).show(10, False)
summary_df.orderBy("distinct_count").show(10, False)
summary_df.show(truncate=False)

summary_df.coalesce(1) \
    .write.csv(
        "cc_feature_profile_{}_to_{}.csv".format(start_date, end_date),
        header=True,
        mode="overwrite"
    )
























from pyspark.sql import SparkSession, functions as F
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType

# 1. start Spark
spark = SparkSession.builder.appName("CC_Feature_Profiling").getOrCreate()

# 2. load and select just the fields you care about
df_am04 = spark.table("s11_cp_cnsum.am04_P_h").select(
    "gsam_appl_num",
    "am04_date_most_recent_pd",   "am04_amt_most_recent_pd",
    "am04_date_high_balance_pastdue", "am04_balance_high_pastdue",
    "am04_cons_days_pastdue"
)

df_am02 = spark.table("s11_cp_cnsum.am02_h").select(
    "gsam_appl_num",
    "am02_date_potential_chargeoff", "am02_balance_prev_statement",
    "am02_date_chargeoff_reinstated", "am02_balance_current",
    "am02_balance_high_ltd",         "am02_date_first_use",
    "am02_num_lte_fee_since_lst_cur","am02_num_ol_fee_lst_w_in_limit",
    "am02_amt_last_nsf_payment",     "am02_date_last_nsf_payment",
    "am02_num_nsf_payments_ltd",     "am02_date_last_paid_balance",
    "am02_date_last_payment",        "am02_amt_ptp_broken_ltd",
    "am02_num_ptp_broken_ltd",       "am02_amt_ptp_kept_ltd",
    "am02_num_ptp_kept_ltd",         "am02_amt_ptp_partial_ltd",
    "am02_num_ptp_partial_ltd",      "am02_mths_consecutive_min_pay"
)

df_am00 = spark.table("s11_cp_cnsum.am00_h").select(
    "gsam_appl_num",
    "am00_date_fraud_flag",    "am00_statf_fraud",
    "am00_date_security_fraud_stat","am00_statc_security_fraud",
    "am00_paid_in_full_flag",  "am00_statf_highly_active",
    "am00_statf_active_since_opened","am00_inactive_flag",
    "am00_statf_declined_reissue","am00_statc_chargeoff",
    "am00_statf_potential_chargeoff","am00_typec_vip",
    "am00_statc_current_overlimit","am00_statc_current_past_due"
)

# 3. join them all on your key
df = df_am04\
     .join(df_am02, "gsam_appl_num", "left")\
     .join(df_am00, "gsam_appl_num", "left")

# 4. helper lists
all_cols = [c for c,_ in df.dtypes if c != "gsam_appl_num"]
num_types = ("int","bigint","double","float","long","decimal")
numeric_cols = [c for c,t in df.dtypes if t in num_types]

# 5. total row count
n = df.count()

# 6. gather metrics per column
rows = []
for c in all_cols:
    # missing rate
    pct_miss = df.filter(F.col(c).isNull()).count() / n
    # distinct count
    distinct = df.select(F.countDistinct(c)).first()[0]
    # numeric stats?
    if c in numeric_cols:
        stat = df.select(
            F.mean(c).alias("mean"),
            F.stddev(c).alias("stddev"),
            F.skewness(c).alias("skew")
        ).first()
        mean, stddev, skew = stat["mean"], stat["stddev"], stat["skew"]
    else:
        mean = stddev = skew = None
    rows.append((c, pct_miss, distinct, mean, stddev, skew))

# 7. build a DataFrame
schema = StructType([
    StructField("feature",          StringType(), True),
    StructField("pct_missing",      DoubleType(), True),
    StructField("distinct_count",   LongType(),   True),
    StructField("mean",             DoubleType(), True),
    StructField("stddev",           DoubleType(), True),
    StructField("skewness",         DoubleType(), True),
])
summary_df = spark.createDataFrame(rows, schema)

# 8. inspect
print("Most sparse features:")
summary_df.orderBy(F.desc("pct_missing")).show(10, False)

print("Lowest‑cardinality features:")
summary_df.orderBy("distinct_count").show(10, False)

print("Full feature summary:")
summary_df.show(truncate=False)

# 9. (optional) write out for deeper analysis in pandas or Excel
summary_df.coalesce(1) \
    .write.csv("cc_feature_profile.csv", header=True, mode="overwrite")





from pyspark.sql import SparkSession
from pyspark.sql import functions as F

# start Spark
spark = SparkSession.builder.appName("CC_Feature_Analysis").getOrCreate()

# 1. load each table and pick only the CC columns + the join key
df_am04 = spark.table("s11_cp_cnsum.am04_P_h").select(
    "gsam_appl_num",
    "am04_date_most_recent_pd",
    "am04_amt_most_recent_pd",
    "am04_date_high_balance_pastdue",
    "am04_balance_high_pastdue",
    "am04_cons_days_pastdue"
)

df_am02 = spark.table("s11_cp_cnsum.am02_h").select(
    "gsam_appl_num",
    "am02_date_potential_chargeoff",
    "am02_balance_prev_statement",
    "am02_date_chargeoff_reinstated",
    "am02_balance_current",
    "am02_balance_high_ltd",
    "am02_date_first_use",
    "am02_num_lte_fee_since_lst_cur",
    "am02_num_ol_fee_lst_w_in_limit",
    "am02_amt_last_nsf_payment",
    "am02_date_last_nsf_payment",
    "am02_num_nsf_payments_ltd",
    "am02_date_last_paid_balance",
    "am02_date_last_payment",
    "am02_amt_ptp_broken_ltd",
    "am02_num_ptp_broken_ltd",
    "am02_amt_ptp_kept_ltd",
    "am02_num_ptp_kept_ltd",
    "am02_amt_ptp_partial_ltd",
    "am02_num_ptp_partial_ltd",
    "am02_mths_consecutive_min_pay"
)

df_am00 = spark.table("s11_cp_cnsum.am00_h").select(
    "gsam_appl_num",
    "am00_date_fraud_flag",
    "am00_statf_fraud",
    "am00_date_security_fraud_stat",
    "am00_statc_security_fraud",
    "am00_paid_in_full_flag",
    "am00_statf_highly_active",
    "am00_statf_active_since_opened",
    "am00_inactive_flag",
    "am00_statf_declined_reissue",
    "am00_statc_chargeoff",
    "am00_statf_potential_chargeoff",
    "am00_typec_vip",
    "am00_statc_current_overlimit",
    "am00_statc_current_past_due"
)

# 2. join them all
df = (
    df_am04
    .join(df_am02, "gsam_appl_num", "left")
    .join(df_am00, "gsam_appl_num", "left")
)

# 3. specify your target column here (if you have one)
target_col = "target"  # replace with your actual label

# 4. prepare lists of columns
all_cols = [c for c, _ in df.dtypes if c != "gsam_appl_num"]
num_types = ("int", "bigint", "double", "float", "long", "decimal")
numeric_cols = [c for c, t in df.dtypes if t in num_types and c != target_col]
other_cols   = [c for c in all_cols if c not in numeric_cols + [target_col]]

# 5. get total row count
n = df.count()

# 6. build up per-column metrics
rows = []
for c in all_cols:
    missing = df.filter(F.col(c).isNull()).count() / n
    distinct = df.select(F.countDistinct(c)).first()[0]
    mean_v, std_v, skew_v = (None, None, None)
    corr_v = None

    if c in numeric_cols:
        stats = df.select(
            F.mean(c).alias("mean"),
            F.stddev(c).alias("std"),
            F.skewness(c).alias("skew")
        ).first()
        mean_v, std_v, skew_v = stats["mean"], stats["std"], stats["skew"]
        if target_col in df.columns:
            corr_v = df.stat.corr(c, target_col)

    rows.append((c, missing, distinct, mean_v, std_v, skew_v, corr_v))

# 7. convert to a Spark DataFrame and show
schema = ["feature","pct_missing","distinct_count","mean","stddev","skew","corr_with_target"]
summary_df = spark.createDataFrame(rows, schema=schema)

# 8. order by absolute correlation if available, else by percent missing
if target_col in df.columns:
    summary_df = summary_df.orderBy(F.desc(F.abs(F.col("corr_with_target"))))
else:
    summary_df = summary_df.orderBy(F.desc("pct_missing"))

summary_df.show(truncate=False)

# 9. (optional) write out to CSV for deeper analysis
summary_df.coalesce(1).write.csv("cc_feature_summary.csv", header=True, mode="overwrite")














WITH rf_closed AS (
  SELECT
    LPAD(default.decrypt(rfact_acct), 18, '0')    AS account_num,
    ods_business_dt,
    rfact_dt_closed
  FROM s11_rf.rfsactu_h
  WHERE rfact_dt_closed IS NOT NULL
),
mg_closed AS (
  SELECT
    LPAD(default.decrypt(loan_num), 18, '0')      AS account_num,
    ods_business_dt,
    delq_payment_count
  FROM s11_mg.master_all_h
  WHERE delq_payment_count IS NOT NULL
)

SELECT
  c.account_num,
  c.rfact_dt_closed,
  c.ods_business_dt    AS rf_business_dt,
  m.delq_payment_count,
  m.ods_business_dt    AS mg_business_dt
FROM rf_closed AS c
LEFT JOIN mg_closed AS m
  ON m.account_num     = c.account_num
 AND m.ods_business_dt = c.ods_business_dt
;
