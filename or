########## rcif.py changes ##########

# … after you call get_rcif_acct_dts(…) …
data = get_rcif_acct_dts(spark, lookup_dt, ssn_df)

# split prodcode into consumer vs. commercial buckets
data = data.withColumn(
    "prodcode",
    F.when((F.col("prodcode") == "DA") & (F.col("acct_business_flag") == "1"), "DA_comm")
     .when((F.col("prodcode") == "DA") & (F.col("acct_business_flag") != "1"), "DA_cons")
     .when((F.col("prodcode") == "TD") & (F.col("acct_business_flag") == "1"), "TD_comm")
     .when((F.col("prodcode") == "TD") & (F.col("acct_business_flag") != "1"), "TD_cons")
     .when((F.col("prodcode") == "TI") & (F.col("acct_business_flag") == "1"), "TI_comm")
     .when((F.col("prodcode") == "TI") & (F.col("acct_business_flag") != "1"), "TI_cons")
     .when(
         (F.col("prodcode") == "CC") &
         (F.substring(F.col("account_num"), 1, 7) == "0000022"),
         "CC_comm"
     )
     .when(
         (F.col("prodcode") == "CC") &
         (F.substring(F.col("account_num"), 1, 7) == "0000011"),
         "CC_cons"
     )
     .otherwise(F.col("prodcode"))
)

# … then continue building pop_accts from this edited `data` …



########## balance.py changes ##########

# balance.py

from pyspark.sql import SparkSession, DataFrame, functions as F
from datetime import datetime, timedelta
from functools import reduce
from typing import List

def get_balance_history(
    spark: SparkSession,
    first_date: str,
    last_date: str,
    pop_df: DataFrame
) -> DataFrame:
    """
    Single‑scan loader for balance history:
      - Unions the two DA/TD tables (both as deposit)
      - Loads TI balances
      - Joins to pop_df on account_num to get SSN + prodcode
      - Tags each row into one of four families:
          deposit_cons, deposit_comm, ti_cons, ti_comm
    """
    # 1. figure out which column is your SSN
    if 'customerssn_obfuscated' in pop_df.columns:
        ssn_col = 'customerssn_obfuscated'
    elif 'customerssn' in pop_df.columns:
        ssn_col = 'customerssn'
    else:
        raise ValueError("pop_df must include customerssn or customerssn_obfuscated")

    # preload the pop_df key columns
    acct_ref = pop_df.select(ssn_col, 'account_num', 'prodcode').distinct()

    # 2. load DA (dsmastc) + TD (dssavec) in one union
    dep1 = (
        spark.table("sl1_dm.d6s002_dm144d_dsmastc_h")
             .filter(F.col("ods_business_dt").between(first_date, last_date))
             .select(
                 F.col("dsm_account").alias("account_num"),
                 F.col("dsm_balcur"  ).alias("balance"),
                 F.col("ods_business_dt")
             )
    )
    dep2 = (
        spark.table("sl1_dm.d6s002_dm144d_dssavec_h")
             .filter(F.col("ods_business_dt").between(first_date, last_date))
             .select(
                 F.col("dsm_account").alias("account_num"),
                 F.col("dsm_balcur"  ).alias("balance"),
                 F.col("ods_business_dt")
             )
    )
    dep = dep1.union(dep2)

    # 3. load TI history (one‐day snapshot or range as needed)
    ti = (
        spark.table("sl1_tm.iss002_tm190d_ismast_h")
             .filter(F.col("ods_business_dt").between(first_date, last_date))
             .select(
                 F.col("ism_account").alias("account_num"),
                 F.col("ism_balcur"  ).alias("balance"),
                 F.col("ods_business_dt")
             )
    )

    # 4. join both to pop_df to anchor SSN + prodcode
    dep = dep.join(acct_ref, on="account_num", how="inner")
    ti  = ti .join(acct_ref, on="account_num", how="inner")

    # 5. tag each into one of four families
    def tag_family(df):
        return df.withColumn(
            "family",
            F.when(F.col("prodcode").isin("DA_cons","TD_cons"), "deposit_cons")
             .when(F.col("prodcode").isin("DA_comm","TD_comm"), "deposit_comm")
             .when(F.col("prodcode") == "TI_cons",          "ti_cons")
             .when(F.col("prodcode") == "TI_comm",          "ti_comm")
             .otherwise(None)
        ).filter(F.col("family").isNotNull())

    dep = tag_family(dep)
    ti  = tag_family(ti)

    # 6. union deposit + TI
    hist = dep.union(ti)

    # cache for re‑use
    return hist.cache()


from functools import reduce

def get_balance_features(
    spark: SparkSession,
    ssn_to_acct_df: DataFrame,
    lookup_dt: str,
    intervals: List[int]
) -> DataFrame:
    """
    Builds SSN-level balance features over each interval N:
      • For each deposit family (deposit_cons, deposit_comm, deposit_all):
          – sum, avg, max of daily balances
          – days_negative (# days total balance < 0)
          – account_days_negative (sum of # overdrawn accounts each day)
          – max_accounts_overdrawn (worst single‐day # overdrawn accts)
          – coeff_var, skewness, kurtosis of daily total balance
          – bal_change (last_day – first_day) and change_rate
      • For each TI family (ti_cons, ti_comm, ti_all):
          – sum, avg, max of daily balances
          – bal_change and change_rate
    """
    # 1) compute earliest date needed
    max_days = max(intervals)
    first_date = (
        datetime.strptime(lookup_dt, "%Y-%m-%d")
        - timedelta(days=max_days)
    ).strftime("%Y-%m-%d")

    # 2) load tagged history once
    hist = get_balance_history(spark, first_date, lookup_dt, ssn_to_acct_df)

    # 3) pick SSN col
    ssn_col = (
        "customerssn_obfuscated"
        if "customerssn_obfuscated" in ssn_to_acct_df.columns
        else "customerssn"
    )

    deposit_fams = ["deposit_cons", "deposit_comm", "deposit_all"]
    ti_fams      = ["ti_cons",      "ti_comm",      "ti_all"]
    all_fams     = deposit_fams + ti_fams

    feature_frames = []
    for days in sorted(intervals, reverse=True):
        window_start = (
            datetime.strptime(lookup_dt, "%Y-%m-%d")
            - timedelta(days=days)
        ).strftime("%Y-%m-%d")

        # A) pivot daily into fam_bal and fam_od_accts columns
        daily = (
            hist
            .filter(F.col("ods_business_dt").between(window_start, lookup_dt))
            .groupBy(ssn_col, "ods_business_dt", "family")
            .agg(
                F.sum("balance").alias("daily_bal"),
                F.sum(F.when(F.col("balance") < 0, 1).otherwise(0))
                 .alias("daily_od_accts")
            )
        )

        pivoted = (
            daily
            .groupBy(ssn_col, "ods_business_dt")
            .pivot("family", all_fams)
            .agg(
                F.first("daily_bal").alias("bal"),
                F.first("daily_od_accts").alias("od_accts")
            )
            .select(
                ssn_col, "ods_business_dt",
                *[f"{fam}_bal"      for fam in all_fams],
                *[f"{fam}_od_accts" for fam in deposit_fams]
            )
        )

        # B) build aggregations for this window
        exprs = []
        for fam in all_fams:
            # sum/avg/max of daily balances
            exprs += [
                F.sum(F.col(f"{fam}_bal")).alias(f"sum_{fam}_{days}d"),
                F.avg(F.col(f"{fam}_bal")).alias(f"avg_{fam}_{days}d"),
                F.max(F.col(f"{fam}_bal")).alias(f"max_{fam}_{days}d"),
            ]

            # derivative: last_day - first_day, and rate
            exprs += [
                (
                    F.max(F.when(
                        F.col("ods_business_dt") == lookup_dt,
                        F.col(f"{fam}_bal")
                    )) -
                    F.max(F.when(
                        F.col("ods_business_dt") == window_start,
                        F.col(f"{fam}_bal")
                    ))
                ).alias(f"{fam}_bal_change_{days}d"),
                (
                    (
                        F.max(F.when(
                            F.col("ods_business_dt") == lookup_dt,
                            F.col(f"{fam}_bal")
                        )) -
                        F.max(F.when(
                            F.col("ods_business_dt") == window_start,
                            F.col(f"{fam}_bal")
                        ))
                    ) / F.lit(days)
                ).alias(f"{fam}_bal_change_rate_{days}d"),
            ]

        # C) deposit-only behavior signals
        for fam in deposit_fams:
            exprs += [
                # days where total deposit balance < 0
                F.sum(F.when(F.col(f"{fam}_bal") < 0, 1).otherwise(0))
                 .alias(f"days_{fam}_negative_{days}d"),

                # total account-days overdrawn
                F.sum(F.col(f"{fam}_od_accts"))
                 .alias(f"account_days_{fam}_negative_{days}d"),

                # worst-day # accounts overdrawn
                F.max(F.col(f"{fam}_od_accts"))
                 .alias(f"max_{fam}_accounts_overdrawn_{days}d"),

                # distribution of daily balances
                (F.stddev(F.col(f"{fam}_bal"))/F.avg(F.col(f"{fam}_bal")))
                 .alias(f"{fam}_coeff_var_{days}d"),
                F.skewness(F.col(f"{fam}_bal"))
                 .alias(f"{fam}_skewness_{days}d"),
                F.kurtosis(F.col(f"{fam}_bal"))
                 .alias(f"{fam}_kurtosis_{days}d"),
            ]

        feature_frames.append(
            pivoted.groupBy(ssn_col).agg(*exprs)
        )

    # 4) stitch windows side-by-side
    return reduce(
        lambda left, right: left.join(right, on=[ssn_col], how="left"),
        feature_frames
    )





########## transactions.py changes ##########

def acct_transaction_history(
    spark: SparkSession,
    lookup_dt: str,
    intervals: List[int],
    pop_accts: Optional[DataFrame] = None
) -> DataFrame:
    """
    Single scan of dm_transactions, tag DA_cons/TD_cons, join to SSN,
    then two‑family counts & sums per interval.
    """
    max_days = max(intervals)
    start_dt = (datetime.strptime(lookup_dt, "%Y-%m-%d") - timedelta(days=max_days))\
                  .strftime("%Y-%m-%d")

    df = (
        spark.table("s11_dm.dmodst81_dm14dd_ddostran_h")
             .filter(F.col("ods_business_dt").between(start_dt, lookup_dt))
             .join(pop_accts.select("account_num").distinct(), "account_num", "inner")
             .select("account_num", "ods_business_dt", "ods_tramt", "ods_trtype")
    )

    # tag only consumer DA and TD transactions
    df = df.withColumn(
        "trans_type",
        F.when(F.col("ods_trtype").isin(  /* your DA codes list */  ), "DA_cons")
         .when(F.col("ods_trtype").isin(  /* your TD codes list */  ), "TD_cons")
         .otherwise(None)
    ).filter(F.col("trans_type").isNotNull())

    # join back to SSN
    ssn_col = "customerssn_obfuscated" if "customerssn_obfuscated" in pop_accts.columns \
              else "customerssn"
    df = df.join(
        pop_accts.select(ssn_col, "account_num").distinct(),
        on="account_num", how="inner"
    )

    # loop intervals for two-family agg
    from functools import reduce
    results = []
    for days in sorted(intervals, reverse=True):
        start = (datetime.strptime(lookup_dt, "%Y-%m-%d") - timedelta(days=days))\
                    .strftime("%Y-%m-%d")
        w = df.filter(F.col("ods_business_dt").between(start, lookup_dt))

        agg = (
            w.groupBy(ssn_col)
             .agg(
                # DA_cons
                F.count(F.when(F.col("trans_type")=="DA_cons", 1)).alias(f"cnt_DA_cons_{days}d"),
                F.sum  (F.when(F.col("trans_type")=="DA_cons", F.col("ods_tramt"))).alias(f"sum_DA_cons_{days}d"),
                # TD_cons
                F.count(F.when(F.col("trans_type")=="TD_cons", 1)).alias(f"cnt_TD_cons_{days}d"),
                F.sum  (F.when(F.col("trans_type")=="TD_cons", F.col("ods_tramt"))).alias(f"sum_TD_cons_{days}d"),
             )
        )
        results.append(agg)

    return reduce(lambda a, b: a.join(b, ssn_col, "left"), results)







```python
# credit_card_features.py
from pyspark.sql import SparkSession, DataFrame, functions as F
from typing import Optional
from functools import reduce


def cc_balance_history(
    spark: SparkSession,
    first_date: str,
    last_date: str,
    acct_df: Optional[DataFrame] = None
) -> DataFrame:
    """
    Extract and aggregate full balance history (AM02) for credit-card accounts between first_date and last_date.
    Optionally filters to the accounts in acct_df.
    Transforms date fields into days-since (relative to ods_business_dt).
    """
    df = (
        spark.table("s11_cp_cnsum.am02_h")
             .filter(F.col("ods_business_dt").between(first_date, last_date))
             .filter(F.col("prodcode") == "CC")
             .filter(F.substring(F.col("account_num"), 1, 1) == "C")
             .select(
                 F.col("gsam_appl_num").cast("bigint").alias("account_num"),
                 "ods_business_dt",
                 "am02_date_potential_chargeoff",
                 "am02_date_chargeoff_reinstated",
                 "am02_date_first_use",
                 "am02_date_last_nsf_payment",
                 "am02_date_last_paid_balance",
                 "am02_date_last_payment",
                 "am02_balance_prev_statement",
                 "am02_balance_current",
                 "am02_balance_high_ltd",
                 "am02_num_lte_fee_since_lst_cur",
                 "am02_num_ol_fee_lst_w_in_limit",
                 "am02_num_nsf_payments_ltd",
                 "am02_amt_last_nsf_payment",
                 "am02_num_ptp_broken_ltd",
                 "am02_amt_ptp_broken_ltd",
                 "am02_num_ptp_kept_ltd",
                 "am02_amt_ptp_kept_ltd",
                 "am02_num_ptp_partial_ltd",
                 "am02_amt_ptp_partial_ltd",
                 "am02_mths_consecutive_min_pay"
             )
    )
    if acct_df is not None:
        df = df.join(
            acct_df.select(F.col("account_num").cast("bigint")).distinct(),
            on="account_num", how="inner"
        )

    # convert date fields into days-since
    date_cols = [
        "am02_date_potential_chargeoff",
        "am02_date_chargeoff_reinstated",
        "am02_date_first_use",
        "am02_date_last_nsf_payment",
        "am02_date_last_paid_balance",
        "am02_date_last_payment"
    ]
    for col_name in date_cols:
        df = df.withColumn(
            f"ds_{col_name}",
            F.datediff(F.col("ods_business_dt"), F.col(col_name))
        ).drop(col_name)

    # aggregate per account
    return (
        df.groupBy("account_num")
          .agg(
              F.min("ds_am02_date_potential_chargeoff").alias("cc_days_since_potential_chargeoff"),
              F.min("ds_am02_date_chargeoff_reinstated").alias("cc_days_since_chargeoff_reinstated"),
              F.min("ds_am02_date_first_use").alias("cc_days_since_first_use"),
              F.min("ds_am02_date_last_nsf_payment").alias("cc_days_since_last_nsf"),
              F.min("ds_am02_date_last_paid_balance").alias("cc_days_since_last_paid"),
              F.min("ds_am02_date_last_payment").alias("cc_days_since_last_payment"),

              F.max("am02_balance_current").alias("cc_bal_current_max"),
              F.max("am02_balance_prev_statement").alias("cc_bal_prev_max"),
              F.max("am02_balance_high_ltd").alias("cc_bal_high_ltd_max"),

              F.sum("am02_num_lte_fee_since_lst_cur").alias("cc_latefee_count"),
              F.sum("am02_num_ol_fee_lst_w_in_limit").alias("cc_overlimit_event_count"),
              F.sum("am02_num_nsf_payments_ltd").alias("cc_nsf_count"),
              F.max("am02_amt_last_nsf_payment").alias("cc_last_nsf_amt"),

              F.sum("am02_num_ptp_broken_ltd").alias("cc_ptp_broken_count"),
              F.sum("am02_amt_ptp_broken_ltd").alias("cc_ptp_broken_amt"),
              F.sum("am02_num_ptp_kept_ltd").alias("cc_ptp_kept_count"),
              F.sum("am02_amt_ptp_kept_ltd").alias("cc_ptp_kept_amt"),
              F.sum("am02_num_ptp_partial_ltd").alias("cc_ptp_partial_count"),
              F.sum("am02_amt_ptp_partial_ltd").alias("cc_ptp_partial_amt"),

              F.max("am02_mths_consecutive_min_pay").alias("cc_months_min_pay_max")
          )
    )


def cc_pd_history(
    spark: SparkSession,
    first_date: str,
    last_date: str,
    acct_df: Optional[DataFrame] = None
) -> DataFrame:
    """
    Extract and aggregate past-due / overlimit / PTP events (AM04) between first_date and last_date.
    """
    df = (
        spark.table("s11_cp_cnsum.am04_P_h")
             .filter(F.col("ods_business_dt").between(first_date, last_date))
             .filter(F.col("prodcode") == "CC")
             .filter(F.substring(F.col("account_num"), 1, 1) == "C")
             .select(
                 F.col("gsam_appl_num").cast("bigint").alias("account_num"),
                 "ods_business_dt",
                 "am04_date_most_recent_pd",
                 "am04_amt_most_recent_pd",
                 "am04_date_high_balance_pastdue",
                 "am04_balance_high_pastdue",
                 "am04_cons_days_pastdue"
             )
    )
    if acct_df is not None:
        df = df.join(
            acct_df.select(F.col("account_num").cast("bigint")).distinct(),
            on="account_num", how="inner"
        )

    date_cols = ["am04_date_most_recent_pd", "am04_date_high_balance_pastdue"]
    for col_name in date_cols:
        df = df.withColumn(
            f"ds_{col_name}",
            F.datediff(F.col("ods_business_dt"), F.col(col_name))
        ).drop(col_name)

    return (
        df.groupBy("account_num")
          .agg(
              F.min("ds_am04_date_most_recent_pd").alias("cc_days_since_last_pd"),
              F.max("am04_amt_most_recent_pd").alias("cc_amt_most_recent_pd"),
              F.min("ds_am04_date_high_balance_pastdue").alias("cc_days_since_high_bal_pd"),
              F.max("am04_balance_high_pastdue").alias("cc_bal_high_pd_max"),
              F.max("am04_cons_days_pastdue").alias("cc_cons_pd_days_max")
          )
    )


def cc_status_history(
    spark: SparkSession,
    first_date: str,
    last_date: str,
    acct_df: Optional[DataFrame] = None
) -> DataFrame:
    """
    Extract and aggregate account-status and fraud flags (AM00) between first_date and last_date.
    """
    df = (
        spark.table("s11_cp_cnsum.am00_h")
             .filter(F.col("ods_business_dt").between(first_date, last_date))
             .filter(F.col("prodcode") == "CC")
             .filter(F.substring(F.col("account_num"), 1, 1) == "C")
             .select(
                 F.col("gsam_appl_num").cast("bigint").alias("account_num"),
                 "ods_business_dt",
                 "am00_date_fraud_flag",
                 "am00_statf_fraud",
                 "am00_date_security_fraud_stat",
                 "am00_statc_security_fraud",
                 "am00_paid_in_full_flag",
                 "am00_statf_highly_active",
                 "am00_statf_active_since_opened",
                 "am00_inactive_flag",
                 "am00_statf_declined_reissue",
                 "am00_statc_chargeoff",
                 "am00_statf_potential_chargeoff",
                 "am00_typec_vip",
                 "am00_statc_current_overlimit",
                 "am00_statc_current_past_due"
             )
    )
    if acct_df is not None:
        df = df.join(
            acct_df.select(F.col("account_num").cast("bigint")).distinct(),
            on="account_num", how="inner"
        )

    for col_name in ["am00_date_fraud_flag", "am00_date_security_fraud_stat"]:
        df = df.withColumn(
            f"ds_{col_name}",
            F.datediff(F.col("ods_business_dt"), F.col(col_name))
        ).drop(col_name)

    return (
        df.groupBy("account_num")
          .agg(
              F.min("ds_am00_date_fraud_flag").alias("cc_days_since_fraud_flag"),
              F.when(F.max("am00_statf_fraud") == "Y", 1).otherwise(0)
               .alias("cc_fraud_flag"),
              F.min("ds_am00_date_security_fraud_stat")
               .alias("cc_days_since_security_flag"),
              F.when(F.max("am00_statf_potential_chargeoff") == "Y", 1).otherwise(0)
               .alias("cc_potential_chargeoff_flag"),
              F.when(F.max("am00_statc_current_overlimit").isNotNull(), 1).otherwise(0)
               .alias("cc_overlimit_flag"),
              F.when(F.max("am00_statc_current_past_due").isNotNull(), 1).otherwise(0)
               .alias("cc_past_due_flag"),
              F.when(F.max("am00_statf_highly_active") == "Y", 1).otherwise(0)
               .alias("cc_highly_active_flag"),
              F.when(F.max("am00_statf_active_since_opened") == "Y", 1).otherwise(0)
               .alias("cc_ever_active_flag"),
              F.when(F.max("am00_typec_vip") == "Y", 1).otherwise(0)
               .alias("cc_vip_flag")
          )
    )


```python
# credit_card_features.py
from pyspark.sql import SparkSession, DataFrame, functions as F
from typing import Optional, List
from functools import reduce
from fraud_origination.customer_score.cs_v2.data_dev.features.training_data_utils import get_pop_accounts


def cc_balance_history(
    spark: SparkSession,
    first_date: str,
    last_date: str,
    acct_df: Optional[DataFrame] = None
) -> DataFrame:
    # ... same as before: account-level aggregates from AM02
    pass


def cc_pd_history(
    spark: SparkSession,
    first_date: str,
    last_date: str,
    acct_df: Optional[DataFrame] = None
) -> DataFrame:
    # ... same as before: account-level aggregates from AM04
    pass


def cc_status_history(
    spark: SparkSession,
    first_date: str,
    last_date: str,
    acct_df: Optional[DataFrame] = None
) -> DataFrame:
    # ... same as before: account-level aggregates from AM00
    pass


def get_cc_features(
    spark: SparkSession,
    first_balance_lookup_dt: str,
    last_lookup_dt: str,
    pop_df: DataFrame,
    first_rcif_lookup_dt: Optional[str] = None
) -> DataFrame:
    """
    Produce SSN-level credit-card features by:
      1. deriving acct_df from pop_df or RCIF
      2. normalizing account_num for CC products
      3. computing account-level features via history functions
      4. left-joining acct_df to each feature set to retain all accounts
      5. aggregating per SSN with custom logic
    """
    # 1. identify SSN column
    if 'customerssn_obfuscated' in pop_df.columns:
        ssn_col = 'customerssn_obfuscated'
    elif 'customerssn' in pop_df.columns:
        ssn_col = 'customerssn'
    else:
        ssn_col = 'account_num'

    # 2. derive acct_df
    if 'account_num' not in pop_df.columns:
        acct_df = get_pop_accounts(
            spark,
            first_lookup_dt=first_rcif_lookup_dt,
            last_lookup_dt=last_lookup_dt,
            pop_df=pop_df
        )
    else:
        acct_df = pop_df

    # normalize and filter for credit-card accounts
    acct_df = (
        acct_df
        .filter(F.col("prodcode") == "CC")
        .filter(F.substring(F.col("account_num"), 1, 7) == "0000011")
        .withColumn(
            "account_num",
            F.substring(F.col("account_num"), -9, 9).cast("bigint")
        )
        .select(ssn_col, "account_num")
    )

    # 3. compute account-level features
    bal_feats = cc_balance_history(spark, first_balance_lookup_dt, last_lookup_dt, acct_df)
    pd_feats  = cc_pd_history(spark, first_balance_lookup_dt, last_lookup_dt, acct_df)
    st_feats  = cc_status_history(spark, first_balance_lookup_dt, last_lookup_dt, acct_df)

    # 4. merge features to acct_df via left joins to keep all accounts
    acct_feat_df = (
        acct_df
        .join(bal_feats, on="account_num", how="left")
        .join(pd_feats,  on="account_num", how="left")
        .join(st_feats,  on="account_num", how="left")
    )

    # 5. aggregate to SSN level
    sum_features: List[str] = [
        'cc_bal_current_max', 'cc_bal_prev_max', 'cc_bal_high_ltd_max',
        'cc_latefee_count', 'cc_overlimit_event_count', 'cc_nsf_count',
        'cc_ptp_broken_count', 'cc_ptp_kept_count', 'cc_ptp_partial_count'
    ]
    max_features: List[str] = [
        'cc_last_nsf_amt', 'cc_ptp_broken_amt', 'cc_ptp_kept_amt', 'cc_ptp_partial_amt',
        'cc_months_min_pay_max', 'cc_amt_most_recent_pd', 'cc_bal_high_pd_max', 'cc_cons_pd_days_max'
    ]
    recency_features: List[str] = [
        'cc_days_since_potential_chargeoff', 'cc_days_since_chargeoff_reinstated',
        'cc_days_since_first_use', 'cc_days_since_last_nsf', 'cc_days_since_last_paid',
        'cc_days_since_last_payment', 'cc_days_since_last_pd', 'cc_days_since_high_bal_pd',
        'cc_days_since_fraud_flag', 'cc_days_since_security_flag'
    ]
    flag_features = [
        'cc_past_due_flag', 'cc_overlimit_flag', 'cc_fraud_flag',
        'cc_potential_chargeoff_flag', 'cc_highly_active_flag', 'cc_ever_active_flag'
    ]

    agg_exprs = []
    for c in sum_features:
        agg_exprs.append(F.sum(c).alias(c))
    for c in max_features:
        agg_exprs.append(F.max(c).alias(c))
    for c in recency_features:
        agg_exprs.append(F.min(c).alias(c))
    for c in flag_features:
        agg_exprs.append(F.max(c).alias(c))
    agg_exprs.append(F.max('cc_vip_level').alias('cc_vip_level'))

    return (
        acct_feat_df
        .groupBy(ssn_col)
        .agg(*agg_exprs)
    )
```

```
















WITH rf_closed AS (
  SELECT
    LPAD(default.decrypt(rfact_acct), 18, '0')    AS account_num,
    ods_business_dt,
    rfact_dt_closed
  FROM s11_rf.rfsactu_h
  WHERE rfact_dt_closed IS NOT NULL
),
mg_closed AS (
  SELECT
    LPAD(default.decrypt(loan_num), 18, '0')      AS account_num,
    ods_business_dt,
    delq_payment_count
  FROM s11_mg.master_all_h
  WHERE delq_payment_count IS NOT NULL
)

SELECT
  c.account_num,
  c.rfact_dt_closed,
  c.ods_business_dt    AS rf_business_dt,
  m.delq_payment_count,
  m.ods_business_dt    AS mg_business_dt
FROM rf_closed AS c
LEFT JOIN mg_closed AS m
  ON m.account_num     = c.account_num
 AND m.ods_business_dt = c.ods_business_dt
;
