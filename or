########## rcif.py changes ##########

# … after you call get_rcif_acct_dts(…) …
data = get_rcif_acct_dts(spark, lookup_dt, ssn_df)

# split prodcode into consumer vs. commercial buckets
data = data.withColumn(
    "prodcode",
    F.when((F.col("prodcode") == "DA") & (F.col("acct_business_flag") == "1"), "DA_comm")
     .when((F.col("prodcode") == "DA") & (F.col("acct_business_flag") != "1"), "DA_cons")
     .when((F.col("prodcode") == "TD") & (F.col("acct_business_flag") == "1"), "TD_comm")
     .when((F.col("prodcode") == "TD") & (F.col("acct_business_flag") != "1"), "TD_cons")
     .when((F.col("prodcode") == "TI") & (F.col("acct_business_flag") == "1"), "TI_comm")
     .when((F.col("prodcode") == "TI") & (F.col("acct_business_flag") != "1"), "TI_cons")
     .when(
         (F.col("prodcode") == "CC") &
         (F.substring(F.col("account_num"), 1, 7) == "0000022"),
         "CC_comm"
     )
     .when(
         (F.col("prodcode") == "CC") &
         (F.substring(F.col("account_num"), 1, 7) == "0000011"),
         "CC_cons"
     )
     .otherwise(F.col("prodcode"))
)

# … then continue building pop_accts from this edited `data` …



########## balance.py changes ##########

from pyspark.sql import SparkSession, DataFrame, functions as F
from datetime import datetime, timedelta
from functools import reduce

def get_balance_df(
    spark: SparkSession,
    ssn_to_acct_df: DataFrame,
    lookup_dt: str,
    intervals: List[int]
) -> DataFrame:
    """
    One-pass into dfs002_dm14dd_ddssav_h to get both DA (deposit) and TD (savings),
    plus your TI table, then emits sum/avg/max per family for each interval.
    """
    # determine SSN column
    ssn_col = ("customerssn_obfuscated" if "customerssn_obfuscated" in ssn_to_acct_df.columns
               else "customerssn" if "customerssn" in ssn_to_acct_df.columns
               else "account_num")

    # 1) load DA/TD history once
    max_days = max(intervals)
    start_dt = (datetime.strptime(lookup_dt, "%Y-%m-%d") - timedelta(days=max_days))\
                  .strftime("%Y-%m-%d")

    df_ds = (
        spark.table("s11_dm.dfs002_dm14dd_ddssav_h")
             .filter(F.col("ods_business_dt").between(start_dt, lookup_dt))
             .filter(F.col("prodcode").isin("DA", "TD"))
             .select(
                 F.col("dsm_account").alias("account_num"),
                 F.col("dsm_balcur"  ).alias("balance"),
                 F.col("ods_business_dt"),
                 F.col("prodcode")
             )
             .join(
                 ssn_to_acct_df.select(ssn_col, "account_num").distinct(),
                 on="account_num", how="inner"
             )
    )

    # 2) split into deposit vs savings
    dep = df_ds.filter(F.col("prodcode") == "DA") \
               .withColumn("account_type", F.lit("deposit"))
    sav = df_ds.filter(F.col("prodcode") == "TD") \
               .withColumn("account_type", F.lit("savings"))

    # 3) load time-investment history (replace this with your real TI loader)
    ti = (
        spark.table("s11_tm.tms002_tm14dd_tmassmat_h")
             .filter(F.col("ods_business_dt").between(start_dt, lookup_dt))
             .select(
                 F.col("tma_account").alias("account_num"),
                 F.col("tma_balamt"  ).alias("balance"),
                 F.col("ods_business_dt")
             )
             .join(
                 ssn_to_acct_df.select(ssn_col, "account_num").distinct(),
                 on="account_num", how="inner"
             )
             .withColumn("account_type", F.lit("time_investment"))
    )

    # 4) union all three
    uni = dep.union(sav).union(ti)

    # 5) for each interval compute sum/avg/max per family
    aggs = []
    for days in sorted(intervals, reverse=True):
        window_start = (datetime.strptime(lookup_dt, "%Y-%m-%d") - timedelta(days=days))\
                           .strftime("%Y-%m-%d")
        w = uni.filter(F.col("ods_business_dt").between(window_start, lookup_dt))

        aggs.append(
            w.groupBy(ssn_col).agg(
                # deposit family
                F.sum(F.when(F.col("account_type")=="deposit", F.col("balance"))).alias(f"sum_dep_{days}d"),
                F.avg(F.when(F.col("account_type")=="deposit", F.col("balance"))).alias(f"avg_dep_{days}d"),
                F.max(F.when(F.col("account_type")=="deposit", F.col("balance"))).alias(f"max_dep_{days}d"),
                # savings family
                F.sum(F.when(F.col("account_type")=="savings", F.col("balance"))).alias(f"sum_sav_{days}d"),
                F.avg(F.when(F.col("account_type")=="savings", F.col("balance"))).alias(f"avg_sav_{days}d"),
                F.max(F.when(F.col("account_type")=="savings", F.col("balance"))).alias(f"max_sav_{days}d"),
                # time-investment family
                F.sum(F.when(F.col("account_type")=="time_investment", F.col("balance"))).alias(f"sum_ti_{days}d"),
                F.avg(F.when(F.col("account_type")=="time_investment", F.col("balance"))).alias(f"avg_ti_{days}d"),
                F.max(F.when(F.col("account_type")=="time_investment", F.col("balance"))).alias(f"max_ti_{days}d"),
            )
        )

    # 6) stitch all interval DataFrames back together
    return reduce(lambda a, b: a.join(b, ssn_col, "left"), aggs)















def _load_hist(
    spark: SparkSession,
    table_name: str,
    first_date: str,
    last_date: str,
    acct_df: Optional[DataFrame]
) -> DataFrame:
    """
    Reads table_name between the two dates, selects account_num, balance, ods_business_dt,
    and (if acct_df is given) inner-joins to that list of accounts.
    """
    df = (
      spark.table(table_name)
           .filter(F.col("ods_business_dt").between(first_date, last_date))
           .select(
               F.col("dsm_account").alias("account_num"),
               # tweak this to the right balance column for each table
               F.col("dsm_balcur").alias("balance"),
               F.col("ods_business_dt")
           )
    )
    if acct_df is not None and "account_num" in acct_df.columns:
        df = df.join(
            acct_df.select("account_num").distinct(),
            on="account_num",
            how="inner"
        )
    return df





def dm_acct_history(
    spark: SparkSession,
    first_date: str,
    last_date: str,
    acct_df: Optional[DataFrame] = None
) -> DataFrame:
    """
    One‑pass union of deposit, savings & time‑investment histories,
    tag each with account_type, join to SSN, then aggregate three families.
    """
    # load & tag each history exactly once
    dep = _load_hist("s11_dm.dfs002_dm14dd_ddssav_h", first_date, last_date, acct_df) \
             .withColumn("account_type", F.lit("deposit"))
    sav = _load_hist("s11_dm.dfs002_dm14dd_ddssav_h", first_date, last_date, acct_df) \
             .withColumn("account_type", F.lit("savings"))
    ti  = _load_hist("s11_tm.tms002_tm14dd_tmassmat_h", first_date, last_date, acct_df) \
             .withColumn("account_type", F.lit("time_investment"))

    uni = dep.union(sav).union(ti)

    # join back to SSN
    ssn_col = "customerssn_obfuscated" if "customerssn_obfuscated" in acct_df.columns \
              else "customerssn" if "customerssn" in acct_df.columns \
              else "account_num"
    uni = uni.join(
        acct_df.select(ssn_col, "account_num").distinct(),
        on="account_num", how="inner"
    )

    # one loop to produce sum/avg/max per family per interval
    from functools import reduce
    aggregates = []
    for days in sorted(intervals, reverse=True):
        start = (datetime.strptime(last_date, "%Y-%m-%d") - timedelta(days=days))\
                    .strftime("%Y-%m-%d")
        window_df = uni.filter(F.col("ods_business_dt").between(start, last_date))

        agg = (
            window_df.groupBy(ssn_col)
                     .agg(
                        # deposit family
                        F.sum(F.when(F.col("account_type")=="deposit", F.col("balance"))).alias(f"sum_dep_{days}d"),
                        F.avg(F.when(F.col("account_type")=="deposit", F.col("balance"))).alias(f"avg_dep_{days}d"),
                        F.max(F.when(F.col("account_type")=="deposit", F.col("balance"))).alias(f"max_dep_{days}d"),

                        # savings family
                        F.sum(F.when(F.col("account_type")=="savings", F.col("balance"))).alias(f"sum_sav_{days}d"),
                        F.avg(F.when(F.col("account_type")=="savings", F.col("balance"))).alias(f"avg_sav_{days}d"),
                        F.max(F.when(F.col("account_type")=="savings", F.col("balance"))).alias(f"max_sav_{days}d"),

                        # time‑investment family
                        F.sum(F.when(F.col("account_type")=="time_investment", F.col("balance"))).alias(f"sum_ti_{days}d"),
                        F.avg(F.when(F.col("account_type")=="time_investment", F.col("balance"))).alias(f"avg_ti_{days}d"),
                        F.max(F.when(F.col("account_type")=="time_investment", F.col("balance"))).alias(f"max_ti_{days}d"),
                     )
        )
        aggregates.append(agg)

    # stitch all intervals together
    return reduce(lambda a, b: a.join(b, ssn_col, "left"), aggregates)


########## transactions.py changes ##########

def acct_transaction_history(
    spark: SparkSession,
    lookup_dt: str,
    intervals: List[int],
    pop_accts: Optional[DataFrame] = None
) -> DataFrame:
    """
    Single scan of dm_transactions, tag DA_cons/TD_cons, join to SSN,
    then two‑family counts & sums per interval.
    """
    max_days = max(intervals)
    start_dt = (datetime.strptime(lookup_dt, "%Y-%m-%d") - timedelta(days=max_days))\
                  .strftime("%Y-%m-%d")

    df = (
        spark.table("s11_dm.dmodst81_dm14dd_ddostran_h")
             .filter(F.col("ods_business_dt").between(start_dt, lookup_dt))
             .join(pop_accts.select("account_num").distinct(), "account_num", "inner")
             .select("account_num", "ods_business_dt", "ods_tramt", "ods_trtype")
    )

    # tag only consumer DA and TD transactions
    df = df.withColumn(
        "trans_type",
        F.when(F.col("ods_trtype").isin(  /* your DA codes list */  ), "DA_cons")
         .when(F.col("ods_trtype").isin(  /* your TD codes list */  ), "TD_cons")
         .otherwise(None)
    ).filter(F.col("trans_type").isNotNull())

    # join back to SSN
    ssn_col = "customerssn_obfuscated" if "customerssn_obfuscated" in pop_accts.columns \
              else "customerssn"
    df = df.join(
        pop_accts.select(ssn_col, "account_num").distinct(),
        on="account_num", how="inner"
    )

    # loop intervals for two-family agg
    from functools import reduce
    results = []
    for days in sorted(intervals, reverse=True):
        start = (datetime.strptime(lookup_dt, "%Y-%m-%d") - timedelta(days=days))\
                    .strftime("%Y-%m-%d")
        w = df.filter(F.col("ods_business_dt").between(start, lookup_dt))

        agg = (
            w.groupBy(ssn_col)
             .agg(
                # DA_cons
                F.count(F.when(F.col("trans_type")=="DA_cons", 1)).alias(f"cnt_DA_cons_{days}d"),
                F.sum  (F.when(F.col("trans_type")=="DA_cons", F.col("ods_tramt"))).alias(f"sum_DA_cons_{days}d"),
                # TD_cons
                F.count(F.when(F.col("trans_type")=="TD_cons", 1)).alias(f"cnt_TD_cons_{days}d"),
                F.sum  (F.when(F.col("trans_type")=="TD_cons", F.col("ods_tramt"))).alias(f"sum_TD_cons_{days}d"),
             )
        )
        results.append(agg)

    return reduce(lambda a, b: a.join(b, ssn_col, "left"), results)







```python
# credit_card_features.py
from pyspark.sql import SparkSession, DataFrame, functions as F
from typing import Optional
from functools import reduce


def cc_balance_history(
    spark: SparkSession,
    first_date: str,
    last_date: str,
    acct_df: Optional[DataFrame] = None
) -> DataFrame:
    """
    Extract and aggregate full balance history (AM02) for credit-card accounts between first_date and last_date.
    Optionally filters to the accounts in acct_df.
    Transforms date fields into days-since (relative to ods_business_dt).
    """
    df = (
        spark.table("s11_cp_cnsum.am02_h")
             .filter(F.col("ods_business_dt").between(first_date, last_date))
             .filter(F.col("prodcode") == "CC")
             .filter(F.substring(F.col("account_num"), 1, 1) == "C")
             .select(
                 F.col("gsam_appl_num").cast("bigint").alias("account_num"),
                 "ods_business_dt",
                 "am02_date_potential_chargeoff",
                 "am02_date_chargeoff_reinstated",
                 "am02_date_first_use",
                 "am02_date_last_nsf_payment",
                 "am02_date_last_paid_balance",
                 "am02_date_last_payment",
                 "am02_balance_prev_statement",
                 "am02_balance_current",
                 "am02_balance_high_ltd",
                 "am02_num_lte_fee_since_lst_cur",
                 "am02_num_ol_fee_lst_w_in_limit",
                 "am02_num_nsf_payments_ltd",
                 "am02_amt_last_nsf_payment",
                 "am02_num_ptp_broken_ltd",
                 "am02_amt_ptp_broken_ltd",
                 "am02_num_ptp_kept_ltd",
                 "am02_amt_ptp_kept_ltd",
                 "am02_num_ptp_partial_ltd",
                 "am02_amt_ptp_partial_ltd",
                 "am02_mths_consecutive_min_pay"
             )
    )
    if acct_df is not None:
        df = df.join(
            acct_df.select(F.col("account_num").cast("bigint")).distinct(),
            on="account_num", how="inner"
        )

    # convert date fields into days-since
    date_cols = [
        "am02_date_potential_chargeoff",
        "am02_date_chargeoff_reinstated",
        "am02_date_first_use",
        "am02_date_last_nsf_payment",
        "am02_date_last_paid_balance",
        "am02_date_last_payment"
    ]
    for col_name in date_cols:
        df = df.withColumn(
            f"ds_{col_name}",
            F.datediff(F.col("ods_business_dt"), F.col(col_name))
        ).drop(col_name)

    # aggregate per account
    return (
        df.groupBy("account_num")
          .agg(
              F.min("ds_am02_date_potential_chargeoff").alias("cc_days_since_potential_chargeoff"),
              F.min("ds_am02_date_chargeoff_reinstated").alias("cc_days_since_chargeoff_reinstated"),
              F.min("ds_am02_date_first_use").alias("cc_days_since_first_use"),
              F.min("ds_am02_date_last_nsf_payment").alias("cc_days_since_last_nsf"),
              F.min("ds_am02_date_last_paid_balance").alias("cc_days_since_last_paid"),
              F.min("ds_am02_date_last_payment").alias("cc_days_since_last_payment"),

              F.max("am02_balance_current").alias("cc_bal_current_max"),
              F.max("am02_balance_prev_statement").alias("cc_bal_prev_max"),
              F.max("am02_balance_high_ltd").alias("cc_bal_high_ltd_max"),

              F.sum("am02_num_lte_fee_since_lst_cur").alias("cc_latefee_count"),
              F.sum("am02_num_ol_fee_lst_w_in_limit").alias("cc_overlimit_event_count"),
              F.sum("am02_num_nsf_payments_ltd").alias("cc_nsf_count"),
              F.max("am02_amt_last_nsf_payment").alias("cc_last_nsf_amt"),

              F.sum("am02_num_ptp_broken_ltd").alias("cc_ptp_broken_count"),
              F.sum("am02_amt_ptp_broken_ltd").alias("cc_ptp_broken_amt"),
              F.sum("am02_num_ptp_kept_ltd").alias("cc_ptp_kept_count"),
              F.sum("am02_amt_ptp_kept_ltd").alias("cc_ptp_kept_amt"),
              F.sum("am02_num_ptp_partial_ltd").alias("cc_ptp_partial_count"),
              F.sum("am02_amt_ptp_partial_ltd").alias("cc_ptp_partial_amt"),

              F.max("am02_mths_consecutive_min_pay").alias("cc_months_min_pay_max")
          )
    )


def cc_pd_history(
    spark: SparkSession,
    first_date: str,
    last_date: str,
    acct_df: Optional[DataFrame] = None
) -> DataFrame:
    """
    Extract and aggregate past-due / overlimit / PTP events (AM04) between first_date and last_date.
    """
    df = (
        spark.table("s11_cp_cnsum.am04_P_h")
             .filter(F.col("ods_business_dt").between(first_date, last_date))
             .filter(F.col("prodcode") == "CC")
             .filter(F.substring(F.col("account_num"), 1, 1) == "C")
             .select(
                 F.col("gsam_appl_num").cast("bigint").alias("account_num"),
                 "ods_business_dt",
                 "am04_date_most_recent_pd",
                 "am04_amt_most_recent_pd",
                 "am04_date_high_balance_pastdue",
                 "am04_balance_high_pastdue",
                 "am04_cons_days_pastdue"
             )
    )
    if acct_df is not None:
        df = df.join(
            acct_df.select(F.col("account_num").cast("bigint")).distinct(),
            on="account_num", how="inner"
        )

    date_cols = ["am04_date_most_recent_pd", "am04_date_high_balance_pastdue"]
    for col_name in date_cols:
        df = df.withColumn(
            f"ds_{col_name}",
            F.datediff(F.col("ods_business_dt"), F.col(col_name))
        ).drop(col_name)

    return (
        df.groupBy("account_num")
          .agg(
              F.min("ds_am04_date_most_recent_pd").alias("cc_days_since_last_pd"),
              F.max("am04_amt_most_recent_pd").alias("cc_amt_most_recent_pd"),
              F.min("ds_am04_date_high_balance_pastdue").alias("cc_days_since_high_bal_pd"),
              F.max("am04_balance_high_pastdue").alias("cc_bal_high_pd_max"),
              F.max("am04_cons_days_pastdue").alias("cc_cons_pd_days_max")
          )
    )


def cc_status_history(
    spark: SparkSession,
    first_date: str,
    last_date: str,
    acct_df: Optional[DataFrame] = None
) -> DataFrame:
    """
    Extract and aggregate account-status and fraud flags (AM00) between first_date and last_date.
    """
    df = (
        spark.table("s11_cp_cnsum.am00_h")
             .filter(F.col("ods_business_dt").between(first_date, last_date))
             .filter(F.col("prodcode") == "CC")
             .filter(F.substring(F.col("account_num"), 1, 1) == "C")
             .select(
                 F.col("gsam_appl_num").cast("bigint").alias("account_num"),
                 "ods_business_dt",
                 "am00_date_fraud_flag",
                 "am00_statf_fraud",
                 "am00_date_security_fraud_stat",
                 "am00_statc_security_fraud",
                 "am00_paid_in_full_flag",
                 "am00_statf_highly_active",
                 "am00_statf_active_since_opened",
                 "am00_inactive_flag",
                 "am00_statf_declined_reissue",
                 "am00_statc_chargeoff",
                 "am00_statf_potential_chargeoff",
                 "am00_typec_vip",
                 "am00_statc_current_overlimit",
                 "am00_statc_current_past_due"
             )
    )
    if acct_df is not None:
        df = df.join(
            acct_df.select(F.col("account_num").cast("bigint")).distinct(),
            on="account_num", how="inner"
        )

    for col_name in ["am00_date_fraud_flag", "am00_date_security_fraud_stat"]:
        df = df.withColumn(
            f"ds_{col_name}",
            F.datediff(F.col("ods_business_dt"), F.col(col_name))
        ).drop(col_name)

    return (
        df.groupBy("account_num")
          .agg(
              F.min("ds_am00_date_fraud_flag").alias("cc_days_since_fraud_flag"),
              F.when(F.max("am00_statf_fraud") == "Y", 1).otherwise(0)
               .alias("cc_fraud_flag"),
              F.min("ds_am00_date_security_fraud_stat")
               .alias("cc_days_since_security_flag"),
              F.when(F.max("am00_statf_potential_chargeoff") == "Y", 1).otherwise(0)
               .alias("cc_potential_chargeoff_flag"),
              F.when(F.max("am00_statc_current_overlimit").isNotNull(), 1).otherwise(0)
               .alias("cc_overlimit_flag"),
              F.when(F.max("am00_statc_current_past_due").isNotNull(), 1).otherwise(0)
               .alias("cc_past_due_flag"),
              F.when(F.max("am00_statf_highly_active") == "Y", 1).otherwise(0)
               .alias("cc_highly_active_flag"),
              F.when(F.max("am00_statf_active_since_opened") == "Y", 1).otherwise(0)
               .alias("cc_ever_active_flag"),
              F.when(F.max("am00_typec_vip") == "Y", 1).otherwise(0)
               .alias("cc_vip_flag")
          )
    )


```python
# credit_card_features.py
from pyspark.sql import SparkSession, DataFrame, functions as F
from typing import Optional, List
from functools import reduce
from fraud_origination.customer_score.cs_v2.data_dev.features.training_data_utils import get_pop_accounts


def cc_balance_history(
    spark: SparkSession,
    first_date: str,
    last_date: str,
    acct_df: Optional[DataFrame] = None
) -> DataFrame:
    # ... same as before: account-level aggregates from AM02
    pass


def cc_pd_history(
    spark: SparkSession,
    first_date: str,
    last_date: str,
    acct_df: Optional[DataFrame] = None
) -> DataFrame:
    # ... same as before: account-level aggregates from AM04
    pass


def cc_status_history(
    spark: SparkSession,
    first_date: str,
    last_date: str,
    acct_df: Optional[DataFrame] = None
) -> DataFrame:
    # ... same as before: account-level aggregates from AM00
    pass


def get_cc_features(
    spark: SparkSession,
    first_balance_lookup_dt: str,
    last_lookup_dt: str,
    pop_df: DataFrame,
    first_rcif_lookup_dt: Optional[str] = None
) -> DataFrame:
    """
    Produce SSN-level credit-card features by:
      1. deriving acct_df from pop_df or RCIF
      2. normalizing account_num for CC products
      3. computing account-level features via history functions
      4. left-joining acct_df to each feature set to retain all accounts
      5. aggregating per SSN with custom logic
    """
    # 1. identify SSN column
    if 'customerssn_obfuscated' in pop_df.columns:
        ssn_col = 'customerssn_obfuscated'
    elif 'customerssn' in pop_df.columns:
        ssn_col = 'customerssn'
    else:
        ssn_col = 'account_num'

    # 2. derive acct_df
    if 'account_num' not in pop_df.columns:
        acct_df = get_pop_accounts(
            spark,
            first_lookup_dt=first_rcif_lookup_dt,
            last_lookup_dt=last_lookup_dt,
            pop_df=pop_df
        )
    else:
        acct_df = pop_df

    # normalize and filter for credit-card accounts
    acct_df = (
        acct_df
        .filter(F.col("prodcode") == "CC")
        .filter(F.substring(F.col("account_num"), 1, 7) == "0000011")
        .withColumn(
            "account_num",
            F.substring(F.col("account_num"), -9, 9).cast("bigint")
        )
        .select(ssn_col, "account_num")
    )

    # 3. compute account-level features
    bal_feats = cc_balance_history(spark, first_balance_lookup_dt, last_lookup_dt, acct_df)
    pd_feats  = cc_pd_history(spark, first_balance_lookup_dt, last_lookup_dt, acct_df)
    st_feats  = cc_status_history(spark, first_balance_lookup_dt, last_lookup_dt, acct_df)

    # 4. merge features to acct_df via left joins to keep all accounts
    acct_feat_df = (
        acct_df
        .join(bal_feats, on="account_num", how="left")
        .join(pd_feats,  on="account_num", how="left")
        .join(st_feats,  on="account_num", how="left")
    )

    # 5. aggregate to SSN level
    sum_features: List[str] = [
        'cc_bal_current_max', 'cc_bal_prev_max', 'cc_bal_high_ltd_max',
        'cc_latefee_count', 'cc_overlimit_event_count', 'cc_nsf_count',
        'cc_ptp_broken_count', 'cc_ptp_kept_count', 'cc_ptp_partial_count'
    ]
    max_features: List[str] = [
        'cc_last_nsf_amt', 'cc_ptp_broken_amt', 'cc_ptp_kept_amt', 'cc_ptp_partial_amt',
        'cc_months_min_pay_max', 'cc_amt_most_recent_pd', 'cc_bal_high_pd_max', 'cc_cons_pd_days_max'
    ]
    recency_features: List[str] = [
        'cc_days_since_potential_chargeoff', 'cc_days_since_chargeoff_reinstated',
        'cc_days_since_first_use', 'cc_days_since_last_nsf', 'cc_days_since_last_paid',
        'cc_days_since_last_payment', 'cc_days_since_last_pd', 'cc_days_since_high_bal_pd',
        'cc_days_since_fraud_flag', 'cc_days_since_security_flag'
    ]
    flag_features = [
        'cc_past_due_flag', 'cc_overlimit_flag', 'cc_fraud_flag',
        'cc_potential_chargeoff_flag', 'cc_highly_active_flag', 'cc_ever_active_flag'
    ]

    agg_exprs = []
    for c in sum_features:
        agg_exprs.append(F.sum(c).alias(c))
    for c in max_features:
        agg_exprs.append(F.max(c).alias(c))
    for c in recency_features:
        agg_exprs.append(F.min(c).alias(c))
    for c in flag_features:
        agg_exprs.append(F.max(c).alias(c))
    agg_exprs.append(F.max('cc_vip_level').alias('cc_vip_level'))

    return (
        acct_feat_df
        .groupBy(ssn_col)
        .agg(*agg_exprs)
    )
```

```
















WITH rf_closed AS (
  SELECT
    LPAD(default.decrypt(rfact_acct), 18, '0')    AS account_num,
    ods_business_dt,
    rfact_dt_closed
  FROM s11_rf.rfsactu_h
  WHERE rfact_dt_closed IS NOT NULL
),
mg_closed AS (
  SELECT
    LPAD(default.decrypt(loan_num), 18, '0')      AS account_num,
    ods_business_dt,
    delq_payment_count
  FROM s11_mg.master_all_h
  WHERE delq_payment_count IS NOT NULL
)

SELECT
  c.account_num,
  c.rfact_dt_closed,
  c.ods_business_dt    AS rf_business_dt,
  m.delq_payment_count,
  m.ods_business_dt    AS mg_business_dt
FROM rf_closed AS c
LEFT JOIN mg_closed AS m
  ON m.account_num     = c.account_num
 AND m.ods_business_dt = c.ods_business_dt
;
